{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/tutorials/regression-part2-automated-ml.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Use automated machine learning to predict taxi fares"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you use automated machine learning in Azure Machine Learning service to create a regression model to predict NYC taxi fare prices. This process accepts training data and configuration settings, and automatically iterates through combinations of different feature normalization/standardization methods, models, and hyperparameter settings to arrive at the best model.\n",
    "\n",
    "In this tutorial you learn the following tasks:\n",
    "\n",
    "* Download, transform, and clean data using Azure Open Datasets\n",
    "* Train an automated machine learning regression model\n",
    "* Calculate model accuracy\n",
    "\n",
    "If you don\u00e2\u20ac\u2122t have an Azure subscription, create a free account before you begin. Try the [free or paid version](https://aka.ms/AMLFree) of Azure Machine Learning service today."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Complete the [setup tutorial](https://docs.microsoft.com/azure/machine-learning/service/tutorial-1st-experiment-sdk-setup) if you don't already have an Azure Machine Learning service workspace or notebook virtual machine.\n",
    "* After you complete the setup tutorial, open the **tutorials/regression-automated-ml.ipynb** notebook using the same notebook server.\n",
    "\n",
    "This tutorial is also available on [GitHub](https://github.com/Azure/MachineLearningNotebooks/tree/master/tutorials) if you wish to run it in your own [local environment](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/automated-machine-learning/README.md#setup-using-a-local-conda-environment)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and prepare data"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary packages. The Open Datasets package contains a class representing each data source (`NycTlcGreen` for example) to easily filter date parameters before downloading."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from azureml.core import Dataset\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by creating a dataframe to hold the taxi data. Then preview the data."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_taxi_dataset = Dataset.Tabular.from_parquet_files(path=\"https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/green_taxi_data.parquet\")\n",
    "green_taxi_df = green_taxi_dataset.to_pandas_dataframe()\n",
    "green_taxi_df.head(10)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the initial data is loaded, define a function to create various time-based features from the pickup datetime field. This will create new fields for the month number, day of month, day of week, and hour of day, and will allow the model to factor in time-based seasonality. \n",
    "\n",
    "Use the `apply()` function on the dataframe to iteratively apply the `build_time_features()` function to each row in the taxi data."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_time_features(vector):\n",
    "    pickup_datetime = vector[0]\n",
    "    month_num = pickup_datetime.month\n",
    "    day_of_month = pickup_datetime.day\n",
    "    day_of_week = pickup_datetime.weekday()\n",
    "    hour_of_day = pickup_datetime.hour\n",
    "    \n",
    "    return pd.Series((month_num, day_of_month, day_of_week, hour_of_day))\n",
    "\n",
    "green_taxi_df[[\"month_num\", \"day_of_month\",\"day_of_week\", \"hour_of_day\"]] = green_taxi_df[[\"lpepPickupDatetime\"]].apply(build_time_features, axis=1)\n",
    "green_taxi_df.head(10)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove some of the columns that you won't need for training or additional feature building."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = [\"lpepPickupDatetime\", \"lpepDropoffDatetime\", \"puLocationId\", \"doLocationId\", \"extra\", \"mtaTax\",\n",
    "                     \"improvementSurcharge\", \"tollsAmount\", \"ehailFee\", \"tripType\", \"rateCodeID\", \n",
    "                     \"storeAndFwdFlag\", \"paymentType\", \"fareAmount\", \"tipAmount\"\n",
    "                    ]\n",
    "for col in columns_to_remove:\n",
    "    green_taxi_df.pop(col)\n",
    "    \n",
    "green_taxi_df.head(5)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanse data "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the `describe()` function on the new dataframe to see summary statistics for each field."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_taxi_df.describe()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary statistics, you see that there are several fields that have outliers or values that will reduce model accuracy. First filter the lat/long fields to be within the bounds of the Manhattan area. This will filter out longer taxi trips or trips that are outliers in respect to their relationship with other features. \n",
    "\n",
    "Additionally filter the `tripDistance` field to be greater than zero but less than 31 miles (the haversine distance between the two lat/long pairs). This eliminates long outlier trips that have inconsistent trip cost.\n",
    "\n",
    "Lastly, the `totalAmount` field has negative values for the taxi fares, which don't make sense in the context of our model, and the `passengerCount` field has bad data with the minimum values being zero.\n",
    "\n",
    "Filter out these anomalies using query functions, and then remove the last few columns unnecessary for training."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = green_taxi_df.query(\"pickupLatitude>=40.53 and pickupLatitude<=40.88\")\n",
    "final_df = final_df.query(\"pickupLongitude>=-74.09 and pickupLongitude<=-73.72\")\n",
    "final_df = final_df.query(\"tripDistance>=0.25 and tripDistance<31\")\n",
    "final_df = final_df.query(\"passengerCount>0 and totalAmount>0\")\n",
    "\n",
    "columns_to_remove_for_training = [\"pickupLongitude\", \"pickupLatitude\", \"dropoffLongitude\", \"dropoffLatitude\"]\n",
    "for col in columns_to_remove_for_training:\n",
    "    final_df.pop(col)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `describe()` again on the data to ensure cleansing worked as expected. You now have a prepared and cleansed set of taxi, holiday, and weather data to use for machine learning model training."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.describe()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure workspace\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a workspace object from the existing workspace. A [Workspace](https://docs.microsoft.com/python/api/azureml-core/azureml.core.workspace.workspace?view=azure-ml-py) is a class that accepts your Azure subscription and resource information. It also creates a cloud resource to monitor and track your model runs. `Workspace.from_config()` reads the file **config.json** and loads the authentication details into an object named `ws`. `ws` is used throughout the rest of the code in this tutorial."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "ws = Workspace.from_config()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into train and test sets"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and test sets by using the `train_test_split` function in the `scikit-learn` library. This function segregates the data into the x (**features**) data set for model training and the y (**values to predict**) data set for testing. The `test_size` parameter determines the percentage of data to allocate to testing. The `random_state` parameter sets a seed to the random generator, so that your train-test splits are deterministic."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test = train_test_split(final_df, test_size=0.2, random_state=223)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this step is to have data points to test the finished model that haven't been used to train the model, in order to measure true accuracy. \n",
    "\n",
    "In other words, a well-trained model should be able to accurately make predictions from data it hasn't already seen. You now have data prepared for auto-training a machine learning model."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically train a model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To automatically train a model, take the following steps:\n",
    "1. Define settings for the experiment run. Attach your training data to the configuration, and modify settings that control the training process.\n",
    "1. Submit the experiment for model tuning. After submitting the experiment, the process iterates through different machine learning algorithms and hyperparameter settings, adhering to your defined constraints. It chooses the best-fit model by optimizing an accuracy metric."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training settings"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the experiment parameter and model settings for training. View the full list of [settings](https://docs.microsoft.com/azure/machine-learning/service/how-to-configure-auto-train). Submitting the experiment with these default settings will take approximately 20 minutes, but if you want a shorter run time, reduce the `experiment_timeout_hours` parameter.\n",
    "\n",
    "\n",
    "|Property| Value in this tutorial |Description|\n",
    "|----|----|---|\n",
    "|**iteration_timeout_minutes**|10|Time limit in minutes for each iteration. Increase this value for larger datasets that need more time for each iteration.|\n",
    "|**experiment_timeout_hours**|0.3|Maximum amount of time in hours that all iterations combined can take before the experiment terminates.|\n",
    "|**enable_early_stopping**|True|Flag to enable early termination if the score is not improving in the short term.|\n",
    "|**primary_metric**| spearman_correlation | Metric that you want to optimize. The best-fit model will be chosen based on this metric.|\n",
    "|**featurization**| auto | By using auto, the experiment can preprocess the input data (handling missing data, converting text to numeric, etc.)|\n",
    "|**verbosity**| logging.INFO | Controls the level of logging.|\n",
    "|**n_cross_validations**|5|Number of cross-validation splits to perform when validation data is not specified.|"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "automl_settings = {\n",
    "    \"iteration_timeout_minutes\": 10,\n",
    "    \"experiment_timeout_hours\": 0.3,\n",
    "    \"enable_early_stopping\": True,\n",
    "    \"primary_metric\": 'spearman_correlation',\n",
    "    \"featurization\": 'auto',\n",
    "    \"verbosity\": logging.INFO,\n",
    "    \"n_cross_validations\": 5\n",
    "}"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your defined training settings as a `**kwargs` parameter to an `AutoMLConfig` object. Additionally, specify your training data and the type of model, which is `regression` in this case."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "automl_config = AutoMLConfig(task='regression',\n",
    "                             debug_log='automated_ml_errors.log',\n",
    "                             training_data=x_train,\n",
    "                             label_column_name=\"totalAmount\",\n",
    "                             **automl_settings)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automated machine learning pre-processing steps (feature normalization, handling missing data, converting text to numeric, etc.) become part of the underlying model. When using the model for predictions, the same pre-processing steps applied during training are applied to your input data automatically."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the automatic regression model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an experiment object in your workspace. An experiment acts as a container for your individual runs. Pass the defined `automl_config` object to the experiment, and set the output to `True` to view progress during the run. \n",
    "\n",
    "After starting the experiment, the output shown updates live as the experiment runs. For each iteration, you see the model type, the run duration, and the training accuracy. The field `BEST` tracks the best running training score based on your metric type."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.experiment import Experiment\n",
    "experiment = Experiment(ws, \"Tutorial-NYCTaxi\")\n",
    "local_run = experiment.submit(automl_config, show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the results"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the results of automatic training with a [Jupyter widget](https://docs.microsoft.com/python/api/azureml-widgets/azureml.widgets?view=azure-ml-py). The widget allows you to see a graph and table of all individual run iterations, along with training accuracy metrics and metadata. Additionally, you can filter on different accuracy metrics than your primary metric with the dropdown selector."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(local_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the best model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the best model from your iterations. The `get_output` function returns the best run and the fitted model for the last fit invocation. By using the overloads on `get_output`, you can retrieve the best run and fitted model for any logged metric or a particular iteration."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = local_run.get_output()\n",
    "print(best_run)\n",
    "print(fitted_model)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the best model accuracy"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the best model to run predictions on the test data set to predict taxi fares. The function `predict` uses the best model and predicts the values of y, **trip cost**, from the `x_test` data set. Print the first 10 predicted cost values from `y_predict`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = x_test.pop(\"totalAmount\")\n",
    "\n",
    "y_predict = fitted_model.predict(x_test)\n",
    "print(y_predict[:10])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the `root mean squared error` of the results. Convert the `y_test` dataframe to a list to compare to the predicted values. The function `mean_squared_error` takes two arrays of values and calculates the average squared error between them. Taking the square root of the result gives an error in the same units as the y variable, **cost**. It indicates roughly how far the taxi fare predictions are from the actual fares."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "y_actual = y_test.values.flatten().tolist()\n",
    "rmse = sqrt(mean_squared_error(y_actual, y_predict))\n",
    "rmse"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to calculate mean absolute percent error (MAPE) by using the full `y_actual` and `y_predict` data sets. This metric calculates an absolute difference between each predicted and actual value and sums all the differences. Then it expresses that sum as a percent of the total of the actual values."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_actuals = sum_errors = 0\n",
    "\n",
    "for actual_val, predict_val in zip(y_actual, y_predict):\n",
    "    abs_error = actual_val - predict_val\n",
    "    if abs_error < 0:\n",
    "        abs_error = abs_error * -1\n",
    "\n",
    "    sum_errors = sum_errors + abs_error\n",
    "    sum_actuals = sum_actuals + actual_val\n",
    "\n",
    "mean_abs_percent_error = sum_errors / sum_actuals\n",
    "print(\"Model MAPE:\")\n",
    "print(mean_abs_percent_error)\n",
    "print()\n",
    "print(\"Model Accuracy:\")\n",
    "print(1 - mean_abs_percent_error)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the two prediction accuracy metrics, you see that the model is fairly good at predicting taxi fares from the data set's features, typically within +- $4.00, and approximately 15% error. \n",
    "\n",
    "The traditional machine learning model development process is highly resource-intensive, and requires significant domain knowledge and time investment to run and compare the results of dozens of models. Using automated machine learning is a great way to rapidly test many different models for your scenario."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up resources"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not complete this section if you plan on running other Azure Machine Learning service tutorials."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop the notebook VM"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you used a cloud notebook server, stop the VM when you are not using it to reduce cost."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In your workspace, select **Compute**.\n",
    "1. Select the **Notebook VMs** tab in the compute page.\n",
    "1. From the list, select the VM.\n",
    "1. Select **Stop**.\n",
    "1. When you're ready to use the server again, select **Start**."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete everything"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't plan to use the resources you created, delete them, so you don't incur any charges."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In the Azure portal, select **Resource groups** on the far left.\n",
    "1. From the list, select the resource group you created.\n",
    "1. Select **Delete resource group**.\n",
    "1. Enter the resource group name. Then select **Delete**.\n",
    "\n",
    "You can also keep the resource group but delete a single workspace. Display the workspace properties and select **Delete**."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this automated machine learning tutorial, you did the following tasks:\n",
    "\n",
    "> * Configured a workspace and prepared data for an experiment.\n",
    "> * Trained by using an automated regression model locally with custom parameters.\n",
    "> * Explored and reviewed training results.\n",
    "\n",
    "[Deploy your model](https://docs.microsoft.com/azure/machine-learning/service/tutorial-deploy-models-with-aml) with Azure Machine Learning service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/regression-automl-nyc-taxi-data/regression-automated-ml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial #2:  Deploy an image classification model in Azure Container Instance (ACI)\n",
    "\n",
    "This tutorial is **part two of a two-part tutorial series**. In the [previous tutorial](img-classification-part1-training.ipynb), you trained machine learning models and then registered a model in your workspace on the cloud.  \n",
    "\n",
    "Now, you're ready to deploy the model as a web service in [Azure Container Instances](https://docs.microsoft.com/azure/container-instances/) (ACI). A web service is an image, in this case a Docker image, that encapsulates the scoring logic and the model itself. \n",
    "\n",
    "In this part of the tutorial, you use Azure Machine Learning service (Preview) to:\n",
    "\n",
    "> * Set up your testing environment\n",
    "> * Retrieve the model from your workspace\n",
    "> * Test the model locally\n",
    "> * Deploy the model to ACI\n",
    "> * Test the deployed model\n",
    "\n",
    "ACI is a great solution for testing and understanding the workflow. For scalable production deployments, consider using Azure Kubernetes Service. For more information, see [how to deploy and where](https://docs.microsoft.com/azure/machine-learning/service/how-to-deploy-and-where).\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Complete the model training in the [Tutorial #1: Train an image classification model with Azure Machine Learning](train-models.ipynb) notebook.  \n",
    "\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you did NOT complete the tutorial, you can instead run this cell \n",
    "# This will register a model and download the data needed for this tutorial\n",
    "# These prerequisites are created in the training tutorial\n",
    "# Feel free to skip this cell if you completed the training tutorial \n",
    "\n",
    "# register a model\n",
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "from azureml.core.model import Model\n",
    "\n",
    "model_name = \"sklearn_mnist\"\n",
    "model = Model.register(model_path=\"sklearn_mnist_model.pkl\",\n",
    "                        model_name=model_name,\n",
    "                        tags={\"data\": \"mnist\", \"model\": \"classification\"},\n",
    "                        description=\"Mnist handwriting recognition\",\n",
    "                        workspace=ws)\n",
    "\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# to install required packages\n",
    "env = Environment('tutorial-env')\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-dataset-runtime[pandas,fuse]', 'azureml-defaults'], conda_packages = ['scikit-learn==0.22.1'])\n",
    "\n",
    "env.python.conda_dependencies = cd\n",
    "\n",
    "# Register environment to re-use later\n",
    "env.register(workspace = ws)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "\n",
    "Start by setting up a testing environment.\n",
    "\n",
    "### Import packages\n",
    "\n",
    "Import the Python packages needed for this tutorial."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "check version"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import azureml.core\n",
    "\n",
    "# display the core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy as web service\n",
    "\n",
    "Deploy the model as a web service hosted in ACI. \n",
    "\n",
    "To build the correct environment for ACI, provide the following:\n",
    "* A scoring script to show how to use the model\n",
    "* A configuration file to build the ACI\n",
    "* The model you trained before\n",
    "\n",
    "### Create scoring script\n",
    "\n",
    "Create the scoring script, called score.py, used by the web service call to show how to use the model.\n",
    "\n",
    "You must include two required functions into the scoring script:\n",
    "* The `init()` function, which typically loads the model into a global object. This function is run only once when the Docker container is started. \n",
    "\n",
    "* The `run(input_data)` function uses the model to predict a value based on the input data. Inputs and outputs to the run typically use JSON for serialization and de-serialization, but other formats are supported.\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
    "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'sklearn_mnist_model.pkl')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "def run(raw_data):\n",
    "    data = np.array(json.loads(raw_data)['data'])\n",
    "    # make prediction\n",
    "    y_hat = model.predict(data)\n",
    "    # you can return any data type as long as it is JSON-serializable\n",
    "    return y_hat.tolist()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create configuration file\n",
    "\n",
    "Create a deployment configuration file and specify the number of CPUs and gigabyte of RAM needed for your ACI container. While it depends on your model, the default of 1 core and 1 gigabyte of RAM is usually sufficient for many models. If you feel you need more later, you would have to recreate the image and redeploy the service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "configure web service",
     "aci"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags={\"data\": \"MNIST\",  \"method\" : \"sklearn\"}, \n",
    "                                               description='Predict MNIST with sklearn')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy in ACI\n",
    "Estimated time to complete: **about 2-5 minutes**\n",
    "\n",
    "Configure the image and deploy. The following code goes through these steps:\n",
    "\n",
    "1. Create environment object containing dependencies needed by the model using the environment file (`myenv.yml`)\n",
    "1. Create inference configuration necessary to deploy the model as a web service using:\n",
    "   * The scoring file (`score.py`)\n",
    "   * envrionment object created in previous step\n",
    "1. Deploy the model to the ACI container.\n",
    "1. Get the web service HTTP endpoint."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "configure image",
     "create image",
     "deploy web service",
     "aci"
    ]
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import uuid\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "model = Model(ws, 'sklearn_mnist')\n",
    "\n",
    "\n",
    "myenv = Environment.get(workspace=ws, name=\"tutorial-env\", version=\"1\")\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\n",
    "\n",
    "service_name = 'sklearn-mnist-svc-' + str(uuid.uuid4())[:4]\n",
    "service = Model.deploy(workspace=ws, \n",
    "                       name=service_name, \n",
    "                       models=[model], \n",
    "                       inference_config=inference_config, \n",
    "                       deployment_config=aciconfig)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the scoring web service's HTTP endpoint, which accepts REST client calls. This endpoint can be shared with anyone who wants to test the web service or integrate it into an application."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "get scoring uri"
    ]
   },
   "outputs": [],
   "source": [
    "print(service.scoring_uri)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download test data\n",
    "Download the test data to the **./data/** directory"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core import Dataset\n",
    "from azureml.opendatasets import MNIST\n",
    "\n",
    "data_folder = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "mnist_file_dataset = MNIST.get_file_dataset()\n",
    "mnist_file_dataset.download(data_folder, overwrite=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test data\n",
    "\n",
    "Load the test data from the **./data/** directory created during the training tutorial."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data\n",
    "import os\n",
    "import glob\n",
    "\n",
    "data_folder = os.path.join(os.getcwd(), 'data')\n",
    "# note we also shrink the intensity values (X) from 0-255 to 0-1. This helps the neural network converge faster\n",
    "X_test = load_data(glob.glob(os.path.join(data_folder,\"**/t10k-images-idx3-ubyte.gz\"), recursive=True)[0], False) / 255.0\n",
    "y_test = load_data(glob.glob(os.path.join(data_folder,\"**/t10k-labels-idx1-ubyte.gz\"), recursive=True)[0], True).reshape(-1)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict test data\n",
    "\n",
    "Feed the test dataset to the model to get predictions.\n",
    "\n",
    "\n",
    "The following code goes through these steps:\n",
    "1. Send the data as a JSON array to the web service hosted in ACI. \n",
    "\n",
    "1. Use the SDK's `run` API to invoke the service. You can also make raw calls using any HTTP tool such as curl."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "test = json.dumps({\"data\": X_test.tolist()})\n",
    "test = bytes(test, encoding='utf8')\n",
    "y_hat = service.run(input_data=test)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Examine the confusion matrix\n",
    "\n",
    "Generate a confusion matrix to see how many samples from the test set are classified correctly. Notice the mis-classified value for the incorrect predictions."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mx = confusion_matrix(y_test, y_hat)\n",
    "print(conf_mx)\n",
    "print('Overall accuracy:', np.average(y_hat == y_test))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `matplotlib` to display the confusion matrix as a graph. In this graph, the X axis represents the actual values, and the Y axis represents the predicted values. The color in each grid represents the error rate. The lighter the color, the higher the error rate is. For example, many 5's are mis-classified as 3's. Hence you see a bright grid at (5,3)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the diagonal cells so that they don't overpower the rest of the cells when visualized\n",
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(norm_conf_mx, cmap=plt.cm.bone)\n",
    "ticks = np.arange(0, 10, 1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(ticks)\n",
    "ax.set_yticklabels(ticks)\n",
    "fig.colorbar(cax)\n",
    "plt.ylabel('true labels', fontsize=14)\n",
    "plt.xlabel('predicted values', fontsize=14)\n",
    "plt.savefig('conf.png')\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show predictions\n",
    "\n",
    "Test the deployed model with a random sample of 30 images from the test data.  \n",
    "\n",
    "\n",
    "1. Print the returned predictions and plot them along with the input images. Red font and inverse image (white on black) is used to highlight the misclassified samples. \n",
    "\n",
    " Since the model accuracy is high, you might have to run the following code a few times before you can see a misclassified sample."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "score web service"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# find 30 random samples from test set\n",
    "n = 30\n",
    "sample_indices = np.random.permutation(X_test.shape[0])[0:n]\n",
    "\n",
    "test_samples = json.dumps({\"data\": X_test[sample_indices].tolist()})\n",
    "test_samples = bytes(test_samples, encoding='utf8')\n",
    "\n",
    "# predict using the deployed model\n",
    "result = service.run(input_data=test_samples)\n",
    "\n",
    "# compare actual value vs. the predicted values:\n",
    "i = 0\n",
    "plt.figure(figsize = (20, 1))\n",
    "\n",
    "for s in sample_indices:\n",
    "    plt.subplot(1, n, i + 1)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    \n",
    "    # use different color for misclassified sample\n",
    "    font_color = 'red' if y_test[s] != result[i] else 'black'\n",
    "    clr_map = plt.cm.gray if y_test[s] != result[i] else plt.cm.Greys\n",
    "    \n",
    "    plt.text(x=10, y =-10, s=result[i], fontsize=18, color=font_color)\n",
    "    plt.imshow(X_test[s].reshape(28, 28), cmap=clr_map)\n",
    "    \n",
    "    i = i + 1\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also send raw HTTP request to test the web service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "score web service"
    ]
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# send a random row from the test set to score\n",
    "random_index = np.random.randint(0, len(X_test)-1)\n",
    "input_data = \"{\\\"data\\\": [\" + str(list(X_test[random_index])) + \"]}\"\n",
    "\n",
    "headers = {'Content-Type':'application/json'}\n",
    "\n",
    "# for AKS deployment you'd need to the service key in the header as well\n",
    "# api_key = service.get_key()\n",
    "# headers = {'Content-Type':'application/json',  'Authorization':('Bearer '+ api_key)} \n",
    "\n",
    "resp = requests.post(service.scoring_uri, input_data, headers=headers)\n",
    "\n",
    "print(\"POST to url\", service.scoring_uri)\n",
    "#print(\"input data:\", input_data)\n",
    "print(\"label:\", y_test[random_index])\n",
    "print(\"prediction:\", resp.text)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up resources\n",
    "\n",
    "To keep the resource group and workspace for other tutorials and exploration, you can delete only the ACI deployment using this API call:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete web service"
    ]
   },
   "outputs": [],
   "source": [
    "service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If you're not going to use what you've created here, delete the resources you just created with this quickstart so you don't incur any charges. In the Azure portal, select and delete your resource group. You can also keep the resource group, but delete a single workspace by displaying the workspace properties and selecting the Delete button.\n",
    "\n",
    "\n",
    "## Next steps\n",
    "\n",
    "In this Azure Machine Learning tutorial, you used Python to:\n",
    "\n",
    "> * Set up your testing environment\n",
    "> * Retrieve the model from your workspace\n",
    "> * Test the model locally\n",
    "> * Deploy the model to ACI\n",
    "> * Test the deployed model\n",
    " \n",
    "You can also try out the [regression tutorial](regression-part1-data-prep.ipynb)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/tutorials/img-classification-part2-deploy.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part2-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial #3:  Deploy an image classification model for encrypted inferencing in Azure Container Instance (ACI)\n",
    "\n",
    "This tutorial is **a new addition to the two-part series**. In the [previous tutorial](img-classification-part1-training.ipynb), you trained machine learning models and then registered a model in your workspace on the cloud.  \n",
    "\n",
    "Now, you're ready to deploy the model as a encrypted inferencing web service in [Azure Container Instances](https://docs.microsoft.com/azure/container-instances/) (ACI). A web service is an image, in this case a Docker image, that encapsulates the scoring logic and the model itself. \n",
    "\n",
    "In this part of the tutorial, you use Azure Machine Learning service (Preview) to:\n",
    "\n",
    "> * Set up your testing environment\n",
    "> * Retrieve the model from your workspace\n",
    "> * Test the model locally\n",
    "> * Deploy the model to ACI\n",
    "> * Test the deployed model\n",
    "\n",
    "ACI is a great solution for testing and understanding the workflow. For scalable production deployments, consider using Azure Kubernetes Service. For more information, see [how to deploy and where](https://docs.microsoft.com/azure/machine-learning/service/how-to-deploy-and-where).\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Complete the model training in the [Tutorial #1: Train an image classification model with Azure Machine Learning](train-models.ipynb) notebook.  \n",
    "\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you did NOT complete the tutorial, you can instead run this cell \n",
    "# This will register a model and download the data needed for this tutorial\n",
    "# These prerequisites are created in the training tutorial\n",
    "# Feel free to skip this cell if you completed the training tutorial \n",
    "\n",
    "# register a model\n",
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "from azureml.core.model import Model\n",
    "\n",
    "model_name = \"sklearn_mnist\"\n",
    "model = Model.register(model_path=\"sklearn_mnist_model.pkl\",\n",
    "                        model_name=model_name,\n",
    "                        tags={\"data\": \"mnist\", \"model\": \"classification\"},\n",
    "                        description=\"Mnist handwriting recognition\",\n",
    "                        workspace=ws)\n",
    "\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the Environment \n",
    "\n",
    "Add `encrypted-inference` package as a conda dependency "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# to install required packages\n",
    "env = Environment('tutorial-encryption-env')\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-dataset-runtime[pandas,fuse]', 'azureml-defaults', 'azure-storage-blob', 'encrypted-inference==0.9'], conda_packages = ['scikit-learn==0.22.1'])\n",
    "\n",
    "env.python.conda_dependencies = cd\n",
    "\n",
    "# Register environment to re-use later\n",
    "env.register(workspace = ws)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "\n",
    "Start by setting up a testing environment.\n",
    "\n",
    "### Import packages\n",
    "\n",
    "Import the Python packages needed for this tutorial."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "check version"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import azureml.core\n",
    "\n",
    "# display the core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Homomorphic Encryption based library for Secure Inferencing\n",
    "\n",
    "Our library is based on [Microsoft SEAL](https://github.com/Microsoft/SEAL) and pubished to [PyPi.org](https://pypi.org/project/encrypted-inference) as an easy to use package "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install encrypted-inference==0.9"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy as web service\n",
    "\n",
    "Deploy the model as a web service hosted in ACI. \n",
    "\n",
    "To build the correct environment for ACI, provide the following:\n",
    "* A scoring script to show how to use the model\n",
    "* A configuration file to build the ACI\n",
    "* The model you trained before\n",
    "\n",
    "### Create scoring script\n",
    "\n",
    "Create the scoring script, called score.py, used by the web service call to show how to use the model.\n",
    "\n",
    "You must include two required functions into the scoring script:\n",
    "* The `init()` function, which typically loads the model into a global object. This function is run only once when the Docker container is started. \n",
    "\n",
    "* The `run(input_data)` function uses the model to predict a value based on the input data. Inputs and outputs to the run typically use JSON for serialization and de-serialization, but other formats are supported. The function fetches homomorphic encryption based public keys that are uploaded by the service caller. \n",
    "\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient, PublicAccess\n",
    "from encrypted.inference.eiserver import EIServer\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
    "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'sklearn_mnist_model.pkl')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "    global server\n",
    "    server = EIServer(model.coef_, model.intercept_, verbose=True)\n",
    "\n",
    "def run(raw_data):\n",
    "\n",
    "    json_properties = json.loads(raw_data)\n",
    "\n",
    "    key_id = json_properties['key_id']\n",
    "    conn_str = json_properties['conn_str']\n",
    "    container = json_properties['container']\n",
    "    data = json_properties['data']\n",
    "\n",
    "    # download the Galois keys from blob storage\n",
    "    #TODO optimize by caching the keys locally  \n",
    "    blob_service_client = BlobServiceClient.from_connection_string(conn_str=conn_str)\n",
    "    blob_client = blob_service_client.get_blob_client(container=container, blob=key_id)\n",
    "    public_keys = blob_client.download_blob().readall()\n",
    "    \n",
    "    result = {}\n",
    "    # make prediction\n",
    "    result = server.predict(data, public_keys)\n",
    "\n",
    "    # you can return any data type as long as it is JSON-serializable\n",
    "    return result"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create configuration file\n",
    "\n",
    "Create a deployment configuration file and specify the number of CPUs and gigabyte of RAM needed for your ACI container. While it depends on your model, the default of 1 core and 1 gigabyte of RAM is usually sufficient for many models. If you feel you need more later, you would have to recreate the image and redeploy the service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "configure web service",
     "aci"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags={\"data\": \"MNIST\",  \"method\" : \"sklearn\"}, \n",
    "                                               description='Encrypted Predict MNIST with sklearn + SEAL')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy in ACI\n",
    "Estimated time to complete: **about 2-5 minutes**\n",
    "\n",
    "Configure the image and deploy. The following code goes through these steps:\n",
    "\n",
    "1. Create environment object containing dependencies needed by the model using the environment file (`myenv.yml`)\n",
    "1. Create inference configuration necessary to deploy the model as a web service using:\n",
    "   * The scoring file (`score.py`)\n",
    "   * envrionment object created in previous step\n",
    "1. Deploy the model to the ACI container.\n",
    "1. Get the web service HTTP endpoint."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "configure image",
     "create image",
     "deploy web service",
     "aci"
    ]
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import uuid\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "model = Model(ws, 'sklearn_mnist')\n",
    "\n",
    "myenv = Environment.get(workspace=ws, name=\"tutorial-encryption-env\")\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\n",
    "\n",
    "service_name = 'sklearn-mnist-svc-' + str(uuid.uuid4())[:4]\n",
    "service = Model.deploy(workspace=ws, \n",
    "                       name=service_name, \n",
    "                       models=[model], \n",
    "                       inference_config=inference_config, \n",
    "                       deployment_config=aciconfig)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the scoring web service's HTTP endpoint, which accepts REST client calls. This endpoint can be shared with anyone who wants to test the web service or integrate it into an application."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "get scoring uri"
    ]
   },
   "outputs": [],
   "source": [
    "print(service.scoring_uri)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download test data\n",
    "Download the test data to the **./data/** directory"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core import Dataset\n",
    "from azureml.opendatasets import MNIST\n",
    "\n",
    "data_folder = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "mnist_file_dataset = MNIST.get_file_dataset()\n",
    "mnist_file_dataset.download(data_folder, overwrite=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test data\n",
    "\n",
    "Load the test data from the **./data/** directory created during the training tutorial."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data\n",
    "import os\n",
    "import glob\n",
    "\n",
    "data_folder = os.path.join(os.getcwd(), 'data')\n",
    "# note we also shrink the intensity values (X) from 0-255 to 0-1. This helps the neural network converge faster\n",
    "X_test = load_data(glob.glob(os.path.join(data_folder,\"**/t10k-images-idx3-ubyte.gz\"), recursive=True)[0], False) / 255.0\n",
    "y_test = load_data(glob.glob(os.path.join(data_folder,\"**/t10k-labels-idx1-ubyte.gz\"), recursive=True)[0], True).reshape(-1)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict test data\n",
    "\n",
    "Feed the test dataset to the model to get predictions.\n",
    "\n",
    "\n",
    "The following code goes through these steps:\n",
    "\n",
    "1. Create our Homomorphic Encryption based client \n",
    "\n",
    "1. Upload HE generated public keys \n",
    "\n",
    "1. Encrypt the data\n",
    "\n",
    "1. Send the data as JSON to the web service hosted in ACI. \n",
    "\n",
    "1. Use the SDK's `run` API to invoke the service. You can also make raw calls using any HTTP tool such as curl."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create our Homomorphic Encryption based client \n",
    "\n",
    "Create a new EILinearRegressionClient and setup the public keys "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encrypted.inference.eiclient import EILinearRegressionClient\n",
    "\n",
    "# Create a new Encrypted inference client and a new secret key.\n",
    "edp = EILinearRegressionClient(verbose=True)\n",
    "\n",
    "public_keys_blob, public_keys_data = edp.get_public_keys()\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload HE generated public keys\n",
    "\n",
    "Upload the public keys to the workspace default blob store. This will allow us to share the keys with the inference server"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Datastore\n",
    "import os\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "container_name=datastore.container_name\n",
    "\n",
    "# Create a local file and write the keys to it\n",
    "public_keys = open(public_keys_blob, \"wb\")\n",
    "public_keys.write(public_keys_data)\n",
    "public_keys.close()\n",
    "\n",
    "# Upload the file to blob store\n",
    "datastore.upload_files([public_keys_blob])\n",
    "\n",
    "# Delete the local file\n",
    "os.remove(public_keys_blob)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encrypt the data "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose any one sample from the test data \n",
    "sample_index = 1\n",
    "\n",
    "#encrypt the data\n",
    "raw_data = edp.encrypt(X_test[sample_index])\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send the test data to the webservice hosted in ACI\n",
    "\n",
    "Feed the test dataset to the model to get predictions. We will need to send the connection string to the blob storage where the public keys were uploaded \n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from azureml.core import Webservice\n",
    "\n",
    "service = Webservice(ws, service_name)\n",
    "\n",
    "#pass the connection string for blob storage to give the server access to the uploaded public keys \n",
    "conn_str_template = 'DefaultEndpointsProtocol={};AccountName={};AccountKey={};EndpointSuffix=core.windows.net'\n",
    "conn_str = conn_str_template.format(datastore.protocol, datastore.account_name, datastore.account_key)\n",
    "\n",
    "#build the json \n",
    "data = json.dumps({\"data\": raw_data, \"key_id\" : public_keys_blob, \"conn_str\" : conn_str, \"container\" : container_name })\n",
    "data = bytes(data, encoding='ASCII')\n",
    "\n",
    "print ('Making an encrypted inference web service call ')\n",
    "eresult = service.run(input_data=data)\n",
    "\n",
    "print ('Received encrypted inference results')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decrypt the data\n",
    "\n",
    "Use the client to decrypt the results"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "results = edp.decrypt(eresult)\n",
    "\n",
    "print ('Decrypted the results ', results)\n",
    "\n",
    "#Apply argmax to identify the prediction result\n",
    "prediction = np.argmax(results)\n",
    "\n",
    "print ( ' Prediction : ', prediction)\n",
    "print ( ' Actual Label : ', y_test[sample_index])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up resources\n",
    "\n",
    "To keep the resource group and workspace for other tutorials and exploration, you can delete only the ACI deployment using this API call:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete web service"
    ]
   },
   "outputs": [],
   "source": [
    "service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If you're not going to use what you've created here, delete the resources you just created with this quickstart so you don't incur any charges. In the Azure portal, select and delete your resource group. You can also keep the resource group, but delete a single workspace by displaying the workspace properties and selecting the Delete button.\n",
    "\n",
    "\n",
    "## Next steps\n",
    "\n",
    "In this Azure Machine Learning tutorial, you used Python to:\n",
    "\n",
    "> * Set up your testing environment\n",
    "> * Retrieve the model from your workspace\n",
    "> * Test the model locally\n",
    "> * Deploy the model to ACI\n",
    "> * Test the deployed model\n",
    " \n",
    "You can also try out the [regression tutorial](regression-part1-data-prep.ipynb)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/tutorials/img-classification-part2-deploy.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part3-deploy-encrypted.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial #1: Train an image classification model with Azure Machine Learning\n",
    "\n",
    "In this tutorial, you train a machine learning model on remote compute resources. You'll use the training and deployment workflow for Azure Machine Learning service (preview) in a Python Jupyter notebook.  You can then use the notebook as a template to train your own machine learning model with your own data. This tutorial is **part one of a two-part tutorial series**.  \n",
    "\n",
    "This tutorial trains a simple logistic regression using the [MNIST](https://azure.microsoft.com/services/open-datasets/catalog/mnist/) dataset and [scikit-learn](http://scikit-learn.org) with Azure Machine Learning.  MNIST is a popular dataset consisting of 70,000 grayscale images. Each image is a handwritten digit of 28x28 pixels, representing a number from 0 to 9. The goal is to create a multi-class classifier to identify the digit a given image represents. \n",
    "\n",
    "Learn how to:\n",
    "\n",
    "> * Set up your development environment\n",
    "> * Access and examine the data\n",
    "> * Train a simple logistic regression model on a remote cluster\n",
    "> * Review training results, find and register the best model\n",
    "\n",
    "You'll learn how to select a model and deploy it in [part two of this tutorial](deploy-models.ipynb) later. \n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "See prerequisites in the [Azure Machine Learning documentation](https://docs.microsoft.com/azure/machine-learning/service/tutorial-train-models-with-aml#prerequisites).\n",
    "\n",
    "On the computer running this notebook, conda install matplotlib, numpy, scikit-learn=0.22.1"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your development environment\n",
    "\n",
    "All the setup for your development work can be accomplished in a Python notebook.  Setup includes:\n",
    "\n",
    "* Importing Python packages\n",
    "* Connecting to a workspace to enable communication between your local computer and remote resources\n",
    "* Creating an experiment to track all your runs\n",
    "* Creating a remote compute target to use for training\n",
    "\n",
    "### Import packages\n",
    "\n",
    "Import Python packages you need in this session. Also display the Azure Machine Learning SDK version."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "check version"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to workspace\n",
    "\n",
    "Create a workspace object from the existing workspace. `Workspace.from_config()` reads the file **config.json** and loads the details into an object named `ws`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "load workspace"
    ]
   },
   "outputs": [],
   "source": [
    "# load workspace configuration from the config.json file in the current folder.\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, sep='\\t')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create experiment\n",
    "\n",
    "Create an experiment to track the runs in your workspace. A workspace can have muliple experiments. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create experiment"
    ]
   },
   "outputs": [],
   "source": [
    "experiment_name = 'Tutorial-sklearn-mnist'\n",
    "\n",
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Attach existing compute resource\n",
    "By using Azure Machine Learning Compute, a managed service, data scientists can train machine learning models on clusters of Azure virtual machines. Examples include VMs with GPU support. In this tutorial, you create Azure Machine Learning Compute as your training environment. You will submit Python code to run on this VM later in the tutorial. \n",
    "The code below creates the compute clusters for you if they don't already exist in your workspace.\n",
    "\n",
    "**Creation of compute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace the code will skip the creation process."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create mlc",
     "amlcompute"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpu-cluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print(\"found compute target: \" + compute_name)\n",
    "else:\n",
    "    print(\"creating new compute target...\")\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have the necessary packages and compute resources to train a model in the cloud. \n",
    "\n",
    "## Explore data\n",
    "\n",
    "Before you train a model, you need to understand the data that you are using to train it. In this section you learn how to:\n",
    "\n",
    "* Download the MNIST dataset\n",
    "* Display some sample images\n",
    "\n",
    "### Download the MNIST dataset\n",
    "\n",
    "Use Azure Open Datasets to get the raw MNIST data files. [Azure Open Datasets](https://docs.microsoft.com/azure/open-datasets/overview-what-are-open-datasets) are curated public datasets that you can use to add scenario-specific features to machine learning solutions for more accurate models. Each dataset has a corrseponding class, `MNIST` in this case, to retrieve the data in different ways.\n",
    "\n",
    "This code retrieves the data as a `FileDataset` object, which is a subclass of `Dataset`. A `FileDataset` references single or multiple files of any format in your datastores or public urls. The class provides you with the ability to download or mount the files to your compute by creating a reference to the data source location. Additionally, you register the Dataset to your workspace for easy retrieval during training.\n",
    "\n",
    "Follow the [how-to](https://aka.ms/azureml/howto/createdatasets) to learn more about Datasets and their usage in the SDK."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "from azureml.opendatasets import MNIST\n",
    "\n",
    "data_folder = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "mnist_file_dataset = MNIST.get_file_dataset()\n",
    "mnist_file_dataset.download(data_folder, overwrite=True)\n",
    "\n",
    "mnist_file_dataset = mnist_file_dataset.register(workspace=ws,\n",
    "                                                 name='mnist_opendataset',\n",
    "                                                 description='training and test dataset',\n",
    "                                                 create_new_version=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display some sample images\n",
    "\n",
    "Load the compressed files into `numpy` arrays. Then use `matplotlib` to plot 30 random images from the dataset with their labels above them. Note this step requires a `load_data` function that's included in an `utils.py` file. This file is included in the sample folder. Please make sure it is placed in the same folder as this notebook. The `load_data` function simply parses the compresse files into numpy arrays."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure utils.py is in the same directory as this code\n",
    "from utils import load_data\n",
    "import glob\n",
    "\n",
    "\n",
    "# note we also shrink the intensity values (X) from 0-255 to 0-1. This helps the model converge faster.\n",
    "X_train = load_data(glob.glob(os.path.join(data_folder,\"**/train-images-idx3-ubyte.gz\"), recursive=True)[0], False) / 255.0\n",
    "X_test = load_data(glob.glob(os.path.join(data_folder,\"**/t10k-images-idx3-ubyte.gz\"), recursive=True)[0], False) / 255.0\n",
    "y_train = load_data(glob.glob(os.path.join(data_folder,\"**/train-labels-idx1-ubyte.gz\"), recursive=True)[0], True).reshape(-1)\n",
    "y_test = load_data(glob.glob(os.path.join(data_folder,\"**/t10k-labels-idx1-ubyte.gz\"), recursive=True)[0], True).reshape(-1)\n",
    "\n",
    "\n",
    "# now let's show some randomly chosen images from the traininng set.\n",
    "count = 0\n",
    "sample_size = 30\n",
    "plt.figure(figsize = (16, 6))\n",
    "for i in np.random.permutation(X_train.shape[0])[:sample_size]:\n",
    "    count = count + 1\n",
    "    plt.subplot(1, sample_size, count)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    plt.text(x=10, y=-10, s=y_train[i], fontsize=18)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap=plt.cm.Greys)\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on a remote cluster\n",
    "\n",
    "For this task, you submit the job to run on the remote training cluster you set up earlier.  To submit a job you:\n",
    "* Create a directory\n",
    "* Create a training script\n",
    "* Create a script run configuration\n",
    "* Submit the job \n",
    "\n",
    "### Create a directory\n",
    "\n",
    "Create a directory to deliver the necessary code from your computer to the remote resource."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = os.path.join(os.getcwd(), \"sklearn-mnist\")\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training script\n",
    "\n",
    "To submit the job to the cluster, first create a training script. Run the following code to create the training script called `train.py` in the directory you just created. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "from azureml.core import Run\n",
    "from utils import load_data\n",
    "\n",
    "# let user feed in 2 parameters, the dataset to mount or download, and the regularization rate of the logistic regression model\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\n",
    "parser.add_argument('--regularization', type=float, dest='reg', default=0.01, help='regularization rate')\n",
    "args = parser.parse_args()\n",
    "\n",
    "data_folder = args.data_folder\n",
    "print('Data folder:', data_folder)\n",
    "\n",
    "# load train and test set into numpy arrays\n",
    "# note we scale the pixel intensity values to 0-1 (by dividing it with 255.0) so the model can converge faster.\n",
    "X_train = load_data(glob.glob(os.path.join(data_folder, '**/train-images-idx3-ubyte.gz'), recursive=True)[0], False) / 255.0\n",
    "X_test = load_data(glob.glob(os.path.join(data_folder, '**/t10k-images-idx3-ubyte.gz'), recursive=True)[0], False) / 255.0\n",
    "y_train = load_data(glob.glob(os.path.join(data_folder, '**/train-labels-idx1-ubyte.gz'), recursive=True)[0], True).reshape(-1)\n",
    "y_test = load_data(glob.glob(os.path.join(data_folder, '**/t10k-labels-idx1-ubyte.gz'), recursive=True)[0], True).reshape(-1)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, sep = '\\n')\n",
    "\n",
    "# get hold of the current run\n",
    "run = Run.get_context()\n",
    "\n",
    "print('Train a logistic regression model with regularization rate of', args.reg)\n",
    "clf = LogisticRegression(C=1.0/args.reg, solver=\"liblinear\", multi_class=\"auto\", random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('Predict the test set')\n",
    "y_hat = clf.predict(X_test)\n",
    "\n",
    "# calculate accuracy on the prediction\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy is', acc)\n",
    "\n",
    "run.log('regularization rate', np.float(args.reg))\n",
    "run.log('accuracy', np.float(acc))\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
    "joblib.dump(value=clf, filename='outputs/sklearn_mnist_model.pkl')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the script gets data and saves models:\n",
    "\n",
    "+ The training script reads an argument to find the directory containing the data.  When you submit the job later, you point to the dataset for this argument:\n",
    "`parser.add_argument('--data-folder', type=str, dest='data_folder', help='data directory mounting point')`"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "+ The training script saves your model into a directory named outputs. <br/>\n",
    "`joblib.dump(value=clf, filename='outputs/sklearn_mnist_model.pkl')`<br/>\n",
    "Anything written in this directory is automatically uploaded into your workspace. You'll access your model from this directory later in the tutorial."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `utils.py` is referenced from the training script to load the dataset correctly.  Copy this script into the script folder so that it can be accessed along with the training script on the remote resource."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.copy('utils.py', script_folder)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the training job\n",
    "\n",
    "Create a ScriptRunConfig object to specify the configuration details of your training job, including your training script, environment to use, and the compute target to run on. Configure the ScriptRunConfig by specifying:\n",
    "\n",
    "* The directory that contains your scripts. All the files in this directory are uploaded into the cluster nodes for execution. \n",
    "* The compute target.  In this case you will use the AmlCompute you created\n",
    "* The training script name, train.py\n",
    "* An environment that contains the libraries needed to run the script\n",
    "* Arguments required from the training script. \n",
    "\n",
    "In this tutorial, the target is AmlCompute. All files in the script folder are uploaded into the cluster nodes for execution. The data_folder is set to use the dataset.\n",
    "\n",
    "First, create the environment that contains: the scikit-learn library, azureml-dataset-runtime required for accessing the dataset, and azureml-defaults which contains the dependencies for logging metrics. The azureml-defaults also contains the dependencies required for deploying the model as a web service later in the part 2 of the tutorial.\n",
    "\n",
    "Once the environment is defined, register it with the Workspace to re-use it in part 2 of the tutorial."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# to install required packages\n",
    "env = Environment('tutorial-env')\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-dataset-runtime[pandas,fuse]', 'azureml-defaults'], conda_packages = ['scikit-learn==0.22.1'])\n",
    "\n",
    "env.python.conda_dependencies = cd\n",
    "\n",
    "# Register environment to re-use later\n",
    "env.register(workspace = ws)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, create the ScriptRunConfig by specifying the training script, compute target and environment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "configure estimator"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "args = ['--data-folder', mnist_file_dataset.as_mount(), '--regularization', 0.5]\n",
    "\n",
    "src = ScriptRunConfig(source_directory=script_folder,\n",
    "                      script='train.py', \n",
    "                      arguments=args,\n",
    "                      compute_target=compute_target,\n",
    "                      environment=env)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the job to the cluster\n",
    "\n",
    "Run the experiment by submitting the ScriptRunConfig object. And you can navigate to Azure portal to monitor the run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remote run",
     "amlcompute",
     "scikit-learn"
    ]
   },
   "outputs": [],
   "source": [
    "run = exp.submit(config=src)\n",
    "run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the call is asynchronous, it returns a **Preparing** or **Running** state as soon as the job is started.\n",
    "\n",
    "## Monitor a remote run\n",
    "\n",
    "In total, the first run takes **approximately 10 minutes**. But for subsequent runs, as long as the dependencies in the Azure ML environment don't change, the same image is reused and hence the container start up time is much faster.\n",
    "\n",
    "Here is what's happening while you wait:\n",
    "\n",
    "- **Image creation**: A Docker image is created matching the Python environment specified by the Azure ML environment. The image is built and stored in the ACR (Azure Container Registry) associated with your workspace. Image creation and uploading takes **about 5 minutes**. \n",
    "\n",
    "  This stage happens once for each Python environment since the container is cached for subsequent runs.  During image creation, logs are streamed to the run history. You can monitor the image creation progress using these logs.\n",
    "\n",
    "- **Scaling**: If the remote cluster requires more nodes to execute the run than currently available, additional nodes are added automatically. Scaling typically takes **about 5 minutes.**\n",
    "\n",
    "- **Running**: In this stage, the necessary scripts and files are sent to the compute target, then data stores are mounted/copied, then the entry_script is run. While the job is running, stdout and the files in the ./logs directory are streamed to the run history. You can monitor the run's progress using these logs.\n",
    "\n",
    "- **Post-Processing**: The ./outputs directory of the run is copied over to the run history in your workspace so you can access these results.\n",
    "\n",
    "\n",
    "You can check the progress of a running job in multiple ways. This tutorial uses a Jupyter widget as well as a `wait_for_completion` method. \n",
    "\n",
    "### Jupyter widget\n",
    "\n",
    "Watch the progress of the run with a Jupyter widget.  Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "use notebook widget"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, if you need to cancel a run, you can follow [these instructions](https://aka.ms/aml-docs-cancel-run)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get log results upon completion\n",
    "\n",
    "Model training happens in the background. You can use `wait_for_completion` to block and wait until the model has completed training before running more code. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remote run",
     "amlcompute",
     "scikit-learn"
    ]
   },
   "outputs": [],
   "source": [
    "# specify show_output to True for a verbose log\n",
    "run.wait_for_completion(show_output=True) "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display run results\n",
    "\n",
    "You now have a model trained on a remote cluster.  Retrieve all the metrics logged during the run, including the accuracy of the model:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "get metrics"
    ]
   },
   "outputs": [],
   "source": [
    "print(run.get_metrics())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next tutorial you will explore this model in more detail.\n",
    "\n",
    "## Register model\n",
    "\n",
    "The last step in the training script wrote the file `outputs/sklearn_mnist_model.pkl` in a directory named `outputs` in the VM of the cluster where the job is executed. `outputs` is a special directory in that all content in this  directory is automatically uploaded to your workspace.  This content appears in the run record in the experiment under your workspace. Hence, the model file is now also available in your workspace.\n",
    "\n",
    "You can see files associated with that run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "query history"
    ]
   },
   "outputs": [],
   "source": [
    "print(run.get_file_names())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the model in the workspace so that you (or other collaborators) can later query, examine, and deploy this model."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "register model from history"
    ]
   },
   "outputs": [],
   "source": [
    "# register model \n",
    "model = run.register_model(model_name='sklearn_mnist', model_path='outputs/sklearn_mnist_model.pkl')\n",
    "print(model.name, model.id, model.version, sep='\\t')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "In this Azure Machine Learning tutorial, you used Python to:\n",
    "\n",
    "> * Set up your development environment\n",
    "> * Access and examine the data\n",
    "> * Train multiple models on a remote cluster using the popular scikit-learn machine learning library\n",
    "> * Review training details and register the best model\n",
    "\n",
    "You are ready to deploy this registered model using the instructions in the next part of the tutorial series:\n",
    "\n",
    "> [Tutorial 2 - Deploy models](img-classification-part2-deploy.ipynb)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/tutorials/img-classification-part1-training.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved. \n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/quickstart/azureml-quickstart.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/tutorials/quickstart/azureml-quickstart.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/quickstart/azureml-quickstart.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Azure Machine Learning Quickstart\n",
    "\n",
    "In this tutorial, you learn how to quickly get started with Azure Machine Learning. Using  a *compute instance* - a fully managed cloud-based VM that is pre-configured with the latest data science tools - you will train an image classification model using the CIFAR10 dataset.\n",
    "\n",
    "In this tutorial you will learn how to:\n",
    "\n",
    "* Create a compute instance and attach to a notebook\n",
    "* Train an image classification model and log metrics\n",
    "* Deploy the model\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. An Azure Machine Learning workspace\n",
    "1. Familiar with the Python language and machine learning workflows.\n",
    "\n",
    "\n",
    "## Create compute & attach to notebook\n",
    "\n",
    "To run this notebook you will need to create an Azure Machine Learning _compute instance_. The benefits of a compute instance over a local machine (e.g. laptop) or cloud VM are as follows:\n",
    "\n",
    "* It is a pre-configured with all the latest data science libaries (e.g. panads, scikit, TensorFlow, PyTorch) and tools (Jupyter, RStudio). In this tutorial we make extensive use of PyTorch, AzureML SDK, matplotlib and we do not need to install these components on a compute instance.\n",
    "* Notebooks are seperate from the compute instance - this means that you can develop your notebook on a small VM size, and then seamlessly scale up (and/or use a GPU-enabled) the machine when needed to train a model.\n",
    "* You can easily turn on/off the instance to control costs. \n",
    "\n",
    "To create compute, click on the + button at the top of the notebook viewer in Azure Machine Learning Studio:\n",
    "\n",
    "<img src=\"https://dsvmamlstorage127a5f726f.blob.core.windows.net/images/ci-create.PNG\" width=\"500\"/>\n",
    "\n",
    "This will pop up the __New compute instance__ blade, provide a valid __Compute name__ (valid characters are upper and lower case letters, digits, and the - character). Then click on __Create__. \n",
    "\n",
    "It will take approximately 3 minutes for the compute to be ready. When the compute is ready you will see a green light next to the compute name at the top of the notebook viewer:\n",
    "\n",
    "<img src=\"https://dsvmamlstorage127a5f726f.blob.core.windows.net/images/ci-create2.PNG\" width=\"500\"/>\n",
    "\n",
    "You will also notice that the notebook is attached to the __Python 3.6 - AzureML__ jupyter Kernel. Other kernels can be selected such as R. In addition, if you did have other instances you can switch to them by simply using the dropdown menu next to the Compute label.\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/quickstart/azureml-quickstart.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "\n",
    "For this tutorial, you will use the CIFAR10 dataset. It has the classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. The images in CIFAR-10 three-channel color images of 32x32 pixels in size.\n",
    "\n",
    "The code cell below uses the PyTorch API to download the data to your compute instance, which should be quick (around 15 seconds). The data is divided into training and test sets.\n",
    "\n",
    "* **NOTE: The data is downloaded to the compute instance (in the `/tmp` directory) and not a durable cloud-based store like Azure Blob Storage or Azure Data Lake. This means if you delete the compute instance the data will be lost. The [getting started with Azure Machine Learning tutorial series](https://docs.microsoft.com/azure/machine-learning/tutorial-1st-experiment-sdk-setup-local) shows how to create an Azure Machine Learning *dataset*, which aids durability, versioning, and collaboration.**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/quickstart/azureml-quickstart.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1600881820920
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='/tmp/data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='/tmp/data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/quickstart/azureml-quickstart.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at the data\n",
    "In the following cell, you have some python code that displays the first batch of 4 CIFAR10 images:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/quickstart/azureml-quickstart.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1600882160868
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/quickstart/azureml-quickstart.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model and log metrics\n",
    "\n",
    "In the directory `model` you will see a file called [model.py](./model/model.py) that defines the neural network architecture. The model is trained using the code below.\n",
    "\n",
    "* **Note: The model training take around 4 minutes to complete. The benefit of a compute instance is that the notebooks are separate from the compute - therefore you can easily switch to a different size/type of instance. For example, you could switch to run this training on a GPU-based compute instance if you had one provisioned. In the code below you can see that we have included `torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")`, which detects whether you are using a CPU or GPU machine.**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/quickstart/azureml-quickstart.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1600882387754
    },
    "tags": [
     "local run"
    ]
   },
   "outputs": [],
   "source": [
    "from model.model import Net\n",
    "from azureml.core import Experiment\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "exp = Experiment(workspace=ws, name=\"cifar10-experiment\")\n",
    "run = exp.start_logging(snapshot_directory=None)\n",
    "\n",
    "# define convolutional network\n",
    "net = Net()\n",
    "net.to(device)\n",
    "\n",
    "# set up pytorch loss /  optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "run.log(\"learning rate\", 0.001)\n",
    "run.log(\"momentum\", 0.9)\n",
    "\n",
    "# train the network\n",
    "for epoch in range(1):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # unpack the data\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:\n",
    "            loss = running_loss / 2000\n",
    "            run.log(\"loss\", loss)\n",
    "            print(f'epoch={epoch + 1}, batch={i + 1:5}: loss {loss:.2f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/quickstart/azureml-quickstart.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have executed the cell below you can view the metrics updating in real time in the Azure Machine Learning studio:\n",
    "\n",
    "1. Select **Experiments** (left-hand menu)\n",
    "1. Select **cifar10-experiment**\n",
    "1. Select **Run 1**\n",
    "1. Select the **Metrics** Tab\n",
    "\n",
    "The metrics tab will display the following graph:\n",
    "\n",
    "<img src=\"https://dsvmamlstorage127a5f726f.blob.core.windows.net/images/metrics-capture.PNG\" alt=\"dataset details\" width=\"500\"/>"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/quickstart/azureml-quickstart.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understand the code\n",
    "\n",
    "The code is based on the [Pytorch 60minute Blitz](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py) where we have also added a few additional lines of code to track the loss metric as the neural network trains.\n",
    "\n",
    "| Code       | Description     | \n",
    "| ------------- | ---------- |\n",
    "|  `experiment =  Experiment( ... )` | [Experiment](https://docs.microsoft.com/python/api/azureml-core/azureml.core.experiment.experiment?view=azure-ml-py&preserve-view=true) provides a simple way to organize multiple runs under a single name. Later you can see how experiments make it easy to compare metrics between dozens of runs.   |\n",
    "| `run.log()`   | This will log the metrics to Azure Machine Learning. |"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/quickstart/azureml-quickstart.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version control models with the Model Registry\n",
    "\n",
    "You can use model registration to store and version your models in your workspace. Registered models are identified by name and version. Each time you register a model with the same name as an existing one, the registry increments the version. Azure Machine Learning supports any model that can be loaded through Python 3.\n",
    "\n",
    "The code below does:\n",
    "\n",
    "1. Saves the model on the compute instance\n",
    "1. Uploads the model file to the run (if you look in the experiment on Azure Machine Learning studio you should see on the **Outputs + logs** tab the model has been saved in the run)\n",
    "1. Registers the uploaded model file\n",
    "1. Transitions the run to a completed state"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/quickstart/azureml-quickstart.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1600888071066
    },
    "tags": [
     "register model from file"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "PATH = 'cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n",
    "run.upload_file(name=PATH, path_or_stream=PATH)\n",
    "model = run.register_model(model_name='cifar10-model', \n",
    "                                model_path=PATH,\n",
    "                                model_framework=Model.Framework.PYTORCH,\n",
    "                                description='cifar10 model')\n",
    "            \n",
    "run.complete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/quickstart/azureml-quickstart.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View model in the model registry\n",
    "\n",
    "You can see the stored model by navigating to **Models** in the left-hand menu bar of Azure Machine Learning Studio. Click on the **cifar10-model** and you can see the details of the model like the experiement run id that created the model."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/quickstart/azureml-quickstart.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model\n",
    "\n",
    "The next cell deploys the model to an Azure Container Instance so that you can score data in real-time (Azure Machine Learning also provides mechanisms to do batch scoring). A real-time endpoint allows application developers to integrate machine learning into their apps.\n",
    "\n",
    "* **Note: The deployment takes around 3 minutes to complete.**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/quickstart/azureml-quickstart.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "deploy service",
     "aci"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import Environment, Model\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "environment = Environment.get(ws, \"AzureML-PyTorch-1.6-CPU\")\n",
    "model = Model(ws, \"cifar10-model\")\n",
    "\n",
    "service_name = 'cifar-service'\n",
    "inference_config = InferenceConfig(entry_script='score.py', environment=environment)\n",
    "aci_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)\n",
    "\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name=service_name,\n",
    "                       models=[model],\n",
    "                       inference_config=inference_config,\n",
    "                       deployment_config=aci_config,\n",
    "                       overwrite=True)\n",
    "service.wait_for_deployment(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/quickstart/azureml-quickstart.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the code\n",
    "\n",
    "| Code       | Description     | \n",
    "| ------------- | ---------- |\n",
    "|  `environment = Environment.get()` | [Environment](https://docs.microsoft.com/python/api/overview/azure/ml/?view=azure-ml-py#environment) specify the Python packages, environment variables, and software settings around your training and scoring scripts. In this case, you are using a *curated environment* that has all the packages to run PyTorch.   |\n",
    "|  `inference_config = InferenceConfig()`   | This specifies the inference (scoring) configuration for the deployment such as the script to use when scoring (see below) and on what environment. |\n",
    "| `service = Model.deploy()` | Deploy the model. |\n",
    "\n",
    "The [*scoring script*](score.py) file is has two functions:\n",
    "\n",
    "1. an `init` function that executes once when the service starts - in this function you normally get the model from the registry and set global variables\n",
    "1. a `run(data)` function that executes each time a call is made to the service. In this function, you normally deserialize the json, run a prediction and output the predicted result.\n",
    "\n",
    "\n",
    "## Test the model service\n",
    "\n",
    "In the next cell, you get some unseen data from the test loader:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/quickstart/azureml-quickstart.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/quickstart/azureml-quickstart.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the next cell runs scores the above images using the deployed model service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/quickstart/azureml-quickstart.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "input_payload = json.dumps({\n",
    "    'data': images.tolist()\n",
    "})\n",
    "\n",
    "output = service.run(input_payload)\n",
    "print(output)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/quickstart/azureml-quickstart.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up resources\n",
    "\n",
    "To clean up the resources after this quickstart, firstly delete the Model service using:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/quickstart/azureml-quickstart.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/quickstart/azureml-quickstart.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next stop the compute instance by following these steps:\n",
    "\n",
    "1. Go to **Compute** in the left-hand menu of the Azure Machine Learning studio\n",
    "1. Select your compute instance\n",
    "1. Select **Stop**\n",
    "\n",
    "\n",
    "**Important: The resources you created can be used as prerequisites to other Azure Machine Learning tutorials and how-to articles.** If you don't plan to use the resources you created, delete them, so you don't incur any charges:\n",
    "\n",
    "1. In the Azure portal, select **Resource groups** on the far left.\n",
    "1. From the list, select the resource group you created.\n",
    "1. Select **Delete resource group**.\n",
    "1. Enter the resource group name. Then select **Delete**.\n",
    "\n",
    "You can also keep the resource group but delete a single workspace. Display the workspace properties and select **Delete**."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/quickstart/azureml-quickstart.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In this tutorial, you have seen how to run your machine learning code on a fully managed, pre-configured cloud-based VM called a *compute instance*. Having a compute instance for your development environment removes the burden of installing data science tooling and libraries (for example, Jupyter, PyTorch, TensorFlow, Scikit) and allows you to easily scale up/down the compute power (RAM, cores) since the notebooks are separated from the VM. \n",
    "\n",
    "It is often the case that once you have your machine learning code working in a development environment that you want to productionize this by running as a **_job_** - ideally on a schedule or trigger (for example, arrival of new data). To this end, we recommend that you follow [**the day 1 getting started with Azure Machine Learning tutorial**](https://docs.microsoft.com/azure/machine-learning/tutorial-1st-experiment-sdk-setup-local). This day 1 tutorial is focussed on running jobs-based machine learning code in the cloud."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/quickstart/azureml-quickstart.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Azure Machine Learning Pipelines for batch prediction\n",
    "In this tutorial, you use Azure Machine Learning service pipelines to run a batch scoring image classification job. The example job uses the pre-trained [Inception-V3](https://arxiv.org/abs/1512.00567) CNN (convolutional neural network) Tensorflow model to classify unlabeled images. Machine learning pipelines optimize your workflow with speed, portability, and reuse so you can focus on your expertise, machine learning, rather than on infrastructure and automation. After building and publishing a pipeline, you can configure a REST endpoint to enable triggering the pipeline from any HTTP library on any platform.\n",
    "\n",
    "In this tutorial, you learn the following tasks:\n",
    "\n",
    "> * Configure workspace and download sample data\n",
    "> * Create data objects to fetch and output data\n",
    "> * Download, prepare, and register the model to your workspace\n",
    "> * Provision compute targets and create a scoring script\n",
    "> * Use ParallelRunStep to do batch scoring\n",
    "> * Build, run, and publish a pipeline\n",
    "> * Enable a REST endpoint for the pipeline\n",
    "\n",
    "If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning service](https://aka.ms/AMLFree) today."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* Complete the [setup tutorial](https://docs.microsoft.com/azure/machine-learning/service/tutorial-1st-experiment-sdk-setup) if you don't already have an Azure Machine Learning service workspace or notebook virtual machine.\n",
    "* After you complete the setup tutorial, open the **tutorials/tutorial-pipeline-batch-scoring-classification.ipynb** notebook using the same notebook server.\n",
    "\n",
    "This tutorial is also available on [GitHub](https://github.com/Azure/MachineLearningNotebooks/tree/master/tutorials) if you wish to run it in your own [local environment](how-to-configure-environment.md#local). Run `pip install azureml-sdk[notebooks] azureml-pipeline-core azureml-pipeline-steps pandas requests` to get the required packages."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure workspace and create datastore"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a workspace object from the existing workspace. A [Workspace](https://docs.microsoft.com/python/api/azureml-core/azureml.core.workspace.workspace?view=azure-ml-py) is a class that accepts your Azure subscription and resource information. It also creates a cloud resource to monitor and track your model runs. `Workspace.from_config()` reads the file **config.json** and loads the authentication details into an object named `ws`. `ws` is used throughout the rest of the code in this tutorial."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a datastore for sample images\n",
    "\n",
    "Get the ImageNet evaluation public data sample from the public blob container `sampledata` on the account `pipelinedata`. Calling `register_azure_blob_container()` makes the data available to the workspace under the name `images_datastore`. Then specify the workspace default datastore as the output datastore, which you use for scoring output in the pipeline."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.datastore import Datastore\n",
    "\n",
    "batchscore_blob = Datastore.register_azure_blob_container(ws, \n",
    "                      datastore_name=\"images_datastore\", \n",
    "                      container_name=\"sampledata\", \n",
    "                      account_name=\"pipelinedata\", \n",
    "                      overwrite=True)\n",
    "\n",
    "def_data_store = ws.get_default_datastore()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data objects\n",
    "\n",
    "When building pipelines, `Dataset` objects are used for reading data from workspace datastores, and `PipelineData` objects are used for transferring intermediate data between pipeline steps.\n",
    "\n",
    "This batch scoring example only uses one pipeline step, but in use-cases with multiple steps, the typical flow will include:\n",
    "\n",
    "1. Using `Dataset` objects as **inputs** to fetch raw data, performing some transformations, then **outputting** a `PipelineData` object.\n",
    "1. Use the previous step's `PipelineData` **output object** as an *input object*, repeated for subsequent steps.\n",
    "\n",
    "For this scenario you create `Dataset` objects corresponding to the datastore directories for both the input images and the classification labels (y-test values). You also create a `PipelineData` object for the batch scoring output data."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "from azureml.pipeline.core import PipelineData\n",
    "\n",
    "input_images = Dataset.File.from_files((batchscore_blob, \"batchscoring/images/\"))\n",
    "label_ds = Dataset.File.from_files((batchscore_blob, \"batchscoring/labels/\"))\n",
    "output_dir = PipelineData(name=\"scores\", datastore=def_data_store)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to register the datasets with the workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_images = input_images.register(workspace=ws, name=\"input_images\")\n",
    "label_ds = label_ds.register(workspace=ws, name=\"label_ds\", create_new_version=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and register the model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the pre-trained Tensorflow model to use it for batch scoring in the pipeline. First create a local directory where you store the model, then download and extract it."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "if not os.path.isdir(\"models\"):\n",
    "    os.mkdir(\"models\")\n",
    "    \n",
    "response = urllib.request.urlretrieve(\"http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz\", \"model.tar.gz\")\n",
    "tar = tarfile.open(\"model.tar.gz\", \"r:gz\")\n",
    "tar.extractall(\"models\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you register the model to your workspace, which allows you to easily retrieve it in the pipeline process. In the `register()` static function, the `model_name` parameter is the key you use to locate your model throughout the SDK."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# register downloaded model \n",
    "model = Model.register(model_path=\"models/inception_v3.ckpt\",\n",
    "                       model_name=\"inception\",\n",
    "                       tags={\"pretrained\": \"inception\"},\n",
    "                       description=\"Imagenet trained tensorflow inception\",\n",
    "                       workspace=ws)\n",
    "# remove the downloaded dir after registration if you wish\n",
    "shutil.rmtree(\"models\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and attach remote compute target\n",
    "\n",
    "Azure Machine Learning service pipelines cannot be run locally, and only run on cloud resources. Remote compute targets are reusable virtual compute environments where you run experiments and work-flows. Run the following code to create a GPU-enabled [`AmlCompute`](https://docs.microsoft.com/python/api/azureml-core/azureml.core.compute.amlcompute.amlcompute?view=azure-ml-py) target, and attach it to your workspace. See the [conceptual article](https://docs.microsoft.com/azure/machine-learning/service/concept-compute-target) for more information on compute targets."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "compute_name = \"gpu-cluster\"\n",
    "\n",
    "# checks to see if compute target already exists in workspace, else create it\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=compute_name)\n",
    "except ComputeTargetException:\n",
    "    config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_NC6\",\n",
    "                                                   vm_priority=\"lowpriority\", \n",
    "                                                   min_nodes=0, \n",
    "                                                   max_nodes=1)\n",
    "\n",
    "    compute_target = ComputeTarget.create(workspace=ws, name=compute_name, provisioning_configuration=config)\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a scoring script"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do the scoring, you create a batch scoring script `batch_scoring.py`, and write it to the current directory. The script takes a minibatch of input images, applies the classification model, and outputs the predictions to a results file.\n",
    "\n",
    "The script `batch_scoring.py` takes the following parameters, which get passed from the `ParallelRunStep` that you create later:\n",
    "\n",
    "- `--model_name`: the name of the model being used\n",
    "- `--labels_dir` : the directory path having the `labels.txt` file \n",
    "\n",
    "The pipelines infrastructure uses the `ArgumentParser` class to pass parameters into pipeline steps. For example, in the code below the first argument `--model_name` is given the property identifier `model_name`. In the `main()` function, this property is accessed using `Model.get_model_path(args.model_name)`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline in this tutorial only has one step and writes the output to a file, but for multi-step pipelines, you also use `ArgumentParser` to define a directory to write output data for input to subsequent steps. See the [notebook](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb) for an example of passing data between multiple pipeline steps using the `ArgumentParser` design pattern."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and run the pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the pipeline, you create an object that defines the python environment and dependencies needed by your script `batch_scoring.py`. The main dependency required is Tensorflow, but you also install `azureml-core` and `azureml-dataset-runtime[fuse]` for background processes from the SDK. Create a `RunConfiguration` object using the dependencies, and also specify Docker and Docker-GPU support."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_GPU_IMAGE\n",
    "\n",
    "cd = CondaDependencies.create(pip_packages=[\"tensorflow-gpu==1.15.2\",\n",
    "                                            \"azureml-core\", \"azureml-dataset-runtime[fuse]\"])\n",
    "\n",
    "env = Environment(name=\"parallelenv\")\n",
    "env.python.conda_dependencies=cd\n",
    "env.docker.base_image = DEFAULT_GPU_IMAGE"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the configuration to wrap the inference script\n",
    "Create the pipeline step using the script, environment configuration, and parameters. Specify the compute target you already attached to your workspace as the target of execution of the script. We will use PythonScriptStep to create the pipeline step."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import ParallelRunConfig\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    environment=env,\n",
    "    entry_script=\"batch_scoring.py\",\n",
    "    source_directory=\"scripts\",\n",
    "    output_action=\"append_row\",\n",
    "    append_row_file_name=\"parallel_run_step.txt\",\n",
    "    mini_batch_size=\"20\",\n",
    "    error_threshold=1,\n",
    "    compute_target=compute_target,\n",
    "    process_count_per_node=2,\n",
    "    node_count=1\n",
    ")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the pipeline step\n",
    "\n",
    "A pipeline step is an object that encapsulates everything you need for running a pipeline including:\n",
    "\n",
    "* environment and dependency settings\n",
    "* the compute resource to run the pipeline on\n",
    "* input and output data, and any custom parameters\n",
    "* reference to a script or SDK-logic to run during the step\n",
    "\n",
    "There are multiple classes that inherit from the parent class [`PipelineStep`](https://docs.microsoft.com/python/api/azureml-pipeline-steps/azureml.pipeline.steps.parallelrunstep?view=azure-ml-py) to assist with building a step using certain frameworks and stacks. In this example, you use the [`ParallelRunStep`](https://docs.microsoft.com/en-us/python/api/azureml-contrib-pipeline-steps/azureml.contrib.pipeline.steps.parallelrunstep?view=azure-ml-py) class to define your step logic using a scoring script. \n",
    "\n",
    "An object reference in the `outputs` array becomes available as an **input** for a subsequent pipeline step, for scenarios where there is more than one step."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import ParallelRunStep\n",
    "from datetime import datetime\n",
    "\n",
    "parallel_step_name = \"batchscoring-\" + datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "label_config = label_ds.as_named_input(\"labels_input\")\n",
    "\n",
    "batch_score_step = ParallelRunStep(\n",
    "    name=parallel_step_name,\n",
    "    inputs=[input_images.as_named_input(\"input_images\")],\n",
    "    output=output_dir,\n",
    "    arguments=[\"--model_name\", \"inception\",\n",
    "               \"--labels_dir\", label_config],\n",
    "    side_inputs=[label_config],\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    allow_reuse=False\n",
    ")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a list of all classes for different step types, see the [steps package](https://docs.microsoft.com/python/api/azureml-pipeline-steps/azureml.pipeline.steps?view=azure-ml-py)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline\n",
    "\n",
    "Now you run the pipeline. First create a `Pipeline` object with your workspace reference and the pipeline step you created. The `steps` parameter is an array of steps, and in this case there is only one step for batch scoring. To build pipelines with multiple steps, you place the steps in order in this array.\n",
    "\n",
    "Next use the `Experiment.submit()` function to submit the pipeline for execution. You also specify the custom parameter `param_batch_size`. The `wait_for_completion` function will output logs during the pipeline build process, which allows you to see current progress.\n",
    "\n",
    "Note: The first pipeline run takes roughly **15 minutes**, as all dependencies must be downloaded, a Docker image is created, and the Python environment is provisioned/created. Running it again takes significantly less time as those resources are reused. However, total run time depends on the workload of your scripts and processes running in each pipeline step."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[batch_score_step])\n",
    "pipeline_run = Experiment(ws, \"Tutorial-Batch-Scoring\").submit(pipeline)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will output information of the pipeline run, including the link to the details page of portal.\n",
    "pipeline_run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait the run for completion and show output log to console\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and review output"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to download the output file created from the `batch_scoring.py` script, then explore the scoring results."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tempfile\n",
    "\n",
    "batch_run = pipeline_run.find_step_run(batch_score_step.name)[0]\n",
    "batch_output = batch_run.get_output_data(output_dir.name)\n",
    "\n",
    "target_dir = tempfile.mkdtemp()\n",
    "batch_output.download(local_path=target_dir)\n",
    "result_file = os.path.join(target_dir, batch_output.path_on_datastore, parallel_run_config.append_row_file_name)\n",
    "\n",
    "df = pd.read_csv(result_file, delimiter=\":\", header=None)\n",
    "df.columns = [\"Filename\", \"Prediction\"]\n",
    "print(\"Prediction has \", df.shape[0], \" rows\")\n",
    "df.head(10) "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish and run from REST endpoint"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to publish the pipeline to your workspace. In your workspace in the portal, you can see metadata for the pipeline including run history and durations. You can also run the pipeline manually from the portal.\n",
    "\n",
    "Additionally, publishing the pipeline enables a REST endpoint to rerun the pipeline from any HTTP library on any platform."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline_run.publish_pipeline(\n",
    "    name=\"Inception_v3_scoring\", description=\"Batch scoring using Inception v3 model\", version=\"1.0\")\n",
    "\n",
    "published_pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the pipeline from the REST endpoint, you first need an OAuth2 Bearer-type authentication header. This example uses interactive authentication for illustration purposes, but for most production scenarios requiring automated or headless authentication, use service principle authentication as [described in this notebook](https://aka.ms/pl-restep-auth).\n",
    "\n",
    "Service principle authentication involves creating an **App Registration** in **Azure Active Directory**, generating a client secret, and then granting your service principal **role access** to your machine learning workspace. You then use the [`ServicePrincipalAuthentication`](https://docs.microsoft.com/python/api/azureml-core/azureml.core.authentication.serviceprincipalauthentication?view=azure-ml-py) class to manage your auth flow. \n",
    "\n",
    "Both `InteractiveLoginAuthentication` and `ServicePrincipalAuthentication` inherit from `AbstractAuthentication`, and in both cases you use the `get_authentication_header()` function in the same way to fetch the header."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "interactive_auth = InteractiveLoginAuthentication()\n",
    "auth_header = interactive_auth.get_authentication_header()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the REST url from the `endpoint` property of the published pipeline object. You can also find the REST url in your workspace in the portal. Build an HTTP POST request to the endpoint, specifying your authentication header. Additionally, add a JSON payload object with the experiment name and the batch size parameter. As a reminder, the `process_count_per_node` is passed through to `ParallelRunStep` because you defined it is defined as a `PipelineParameter` object in the step configuration.\n",
    "\n",
    "Make the request to trigger the run. Access the `Id` key from the response dict to get the value of the run id."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "rest_endpoint = published_pipeline.endpoint\n",
    "response = requests.post(rest_endpoint, \n",
    "                         headers=auth_header, \n",
    "                         json={\"ExperimentName\": \"Tutorial-Batch-Scoring\",\n",
    "                               \"ParameterAssignments\": {\"process_count_per_node\": 6}})"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response.raise_for_status()\n",
    "except Exception:    \n",
    "    raise Exception(\"Received bad response from the endpoint: {}\\n\"\n",
    "                    \"Response Code: {}\\n\"\n",
    "                    \"Headers: {}\\n\"\n",
    "                    \"Content: {}\".format(rest_endpoint, response.status_code, response.headers, response.content))\n",
    "\n",
    "run_id = response.json().get('Id')\n",
    "print('Submitted pipeline run: ', run_id)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the run id to monitor the status of the new run. This will take another 10-15 min to run and will look similar to the previous pipeline run, so if you don't need to see another pipeline run, you can skip watching the full output."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core.run import PipelineRun\n",
    "\n",
    "published_pipeline_run = PipelineRun(ws.experiments[\"Tutorial-Batch-Scoring\"], run_id)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detail information of the run\n",
    "published_pipeline_run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up resources\n",
    "\n",
    "Do not complete this section if you plan on running other Azure Machine Learning service tutorials.\n",
    "\n",
    "### Stop the notebook VM\n",
    "\n",
    "If you used a cloud notebook server, stop the VM when you are not using it to reduce cost.\n",
    "\n",
    "1. In your workspace, select **Compute**.\n",
    "1. Select the **Notebook VMs** tab in the compute page.\n",
    "1. From the list, select the VM.\n",
    "1. Select **Stop**.\n",
    "1. When you're ready to use the server again, select **Start**.\n",
    "\n",
    "### Delete everything\n",
    "\n",
    "If you don't plan to use the resources you created, delete them, so you don't incur any charges.\n",
    "\n",
    "1. In the Azure portal, select **Resource groups** on the far left.\n",
    "1. From the list, select the resource group you created.\n",
    "1. Select **Delete resource group**.\n",
    "1. Enter the resource group name. Then select **Delete**.\n",
    "\n",
    "You can also keep the resource group but delete a single workspace. Display the workspace properties and select **Delete**."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "In this machine learning pipelines tutorial, you did the following tasks:\n",
    "\n",
    "> * Built a pipeline with environment dependencies to run on a remote GPU compute resource\n",
    "> * Created a scoring script to run batch predictions with a pre-trained Tensorflow model\n",
    "> * Published a pipeline and enabled it to be run from a REST endpoint\n",
    "\n",
    "See the [how-to](https://docs.microsoft.com/azure/machine-learning/service/how-to-create-your-first-pipeline?view=azure-devops) for additional detail on building pipelines with the machine learning SDK."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/machine-learning-pipelines-advanced/tutorial-pipeline-batch-scoring-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/tutorials/tutorial-quickstart-train-model.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Train your first model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is **part two of a two-part tutorial series**. In the previous tutorial, you created a workspace and chose a development environment. In this tutorial, you learn the foundational design patterns in Azure Machine Learning service, and train a simple scikit-learn model based on the diabetes data set. After completing this tutorial, you will have the practical knowledge of the SDK to scale up to developing more-complex experiments and workflows. \n",
    "\n",
    "In this tutorial, you learn the following tasks:\n",
    "\n",
    "> * Connect your workspace and create an experiment \n",
    "> * Load data and train a scikit-learn model\n",
    "> * View training results in the studio\n",
    "> * Retrieve the best model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "The only prerequisite is to run the previous tutorial, Setup environment and workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect workspace and create experiment"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the `Workspace` class, and load your subscription information from the file `config.json` using the function `from_config().` This looks for the JSON file in the current directory by default, but you can also specify a path parameter to point to the file using `from_config(path=\"your/file/path\")`. If you are running this notebook in a cloud notebook server in your workspace, the file is automatically in the root directory.\n",
    "\n",
    "If the following code asks for additional authentication, simply paste the link in a browser and enter the authentication token. In addition, if you have more than one tenant linked to your user, you will need to add the following lines:\n",
    "```\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "interactive_auth = InteractiveLoginAuthentication(tenant_id=\"your-tenant-id\")\n",
    "Additional details on authentication can be found here: https://aka.ms/aml-notebook-auth \n",
    "```\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create an experiment in your workspace. An experiment is another foundational cloud resource that represents a collection of trials (individual model runs). In this tutorial you use the experiment to create runs and track your model training in the Azure Machine Learning studio. Parameters include your workspace reference, and a string name for the experiment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "experiment = Experiment(workspace=ws, name=\"diabetes-experiment\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and prepare for training"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, you use the diabetes data set, which uses features like age, gender, and BMI to predict diabetes disease progression. Load the data from the Azure Open Datasets class, and split it into training and test sets using `train_test_split()`. This function segregates the data so the model has unseen data to use for testing following training."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.opendatasets import Diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_df = Diabetes.get_tabular_dataset().to_pandas_dataframe().dropna()\n",
    "y_df = x_df.pop(\"Y\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.2, random_state=66)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a simple scikit-learn model can easily be done locally for small-scale training, but when training many iterations with dozens of different feature permutations and hyperparameter settings, it is easy to lose track of what models you've trained and how you trained them. The following design pattern shows how to leverage the SDK to easily keep track of your training in the cloud.\n",
    "\n",
    "Build a script that trains ridge models in a loop through different hyperparameter alpha values."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.externals import joblib\n",
    "import math\n",
    "\n",
    "alphas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "for alpha in alphas:\n",
    "    run = experiment.start_logging()\n",
    "    run.log(\"alpha_value\", alpha)\n",
    "    \n",
    "    model = Ridge(alpha=alpha)\n",
    "    model.fit(X=X_train, y=y_train)\n",
    "    y_pred = model.predict(X=X_test)\n",
    "    rmse = math.sqrt(mean_squared_error(y_true=y_test, y_pred=y_pred))\n",
    "    run.log(\"rmse\", rmse)\n",
    "    \n",
    "    model_name = \"model_alpha_\" + str(alpha) + \".pkl\"\n",
    "    filename = \"outputs/\" + model_name\n",
    "    \n",
    "    joblib.dump(value=model, filename=filename)\n",
    "    run.upload_file(name=model_name, path_or_stream=filename)\n",
    "    run.complete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code accomplishes the following:\n",
    "\n",
    "1. For each alpha hyperparameter value in the `alphas` array, a new run is created within the experiment. The alpha value is logged to differentiate between each run.\n",
    "1. In each run, a Ridge model is instantiated, trained, and used to run predictions. The root-mean-squared-error is calculated for the actual versus predicted values, and then logged to the run. At this point the run has metadata attached for both the alpha value and the rmse accuracy.\n",
    "1. Next, the model for each run is serialized and uploaded to the run. This allows you to download the model file from the run in the studio.\n",
    "1. At the end of each iteration the run is completed by calling `run.complete()`.\n",
    "\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training has completed, call the `experiment` variable to fetch a link to the experiment in the studio."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View training results in studio"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the **Link to Azure Machine Learning studio** takes you to the main experiment page. Here you see all the individual runs in the experiment. Any custom-logged values (`alpha_value` and `rmse`, in this case) become fields for each run, and also become available for the charts and tiles at the top of the experiment page. To add a logged metric to a chart or tile, hover over it, click the edit button, and find your custom-logged metric.\n",
    "\n",
    "When training models at scale over hundreds and thousands of runs, this page makes it easy to see every model you trained, specifically how they were trained, and how your unique metrics have changed over time."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Main Experiment page in the studio](./imgs/experiment_main.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a run number link in the `RUN NUMBER` column to see the page for an individual run. The default tab **Details** shows you more-detailed information on each run. Navigate to the **Outputs + logs** tab, and you see the `.pkl` file for the model that was uploaded to the run during each training iteration. Here you can download the model file, rather than having to retrain it manually."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Run details page in the studio](./imgs/model_download.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the best model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to being able to download model files from the experiment in the studio, you can also download them programmatically. The following code iterates through each run in the experiment, and accesses both the logged run metrics and the run details (which contains the run_id). This keeps track of the best run, in this case the run with the lowest root-mean-squared-error."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_rmse_runid = None\n",
    "minimum_rmse = None\n",
    "\n",
    "for run in experiment.get_runs():\n",
    "    run_metrics = run.get_metrics()\n",
    "    run_details = run.get_details()\n",
    "    # each logged metric becomes a key in this returned dict\n",
    "    run_rmse = run_metrics[\"rmse\"]\n",
    "    run_id = run_details[\"runId\"]\n",
    "    \n",
    "    if minimum_rmse is None:\n",
    "        minimum_rmse = run_rmse\n",
    "        minimum_rmse_runid = run_id\n",
    "    else:\n",
    "        if run_rmse < minimum_rmse:\n",
    "            minimum_rmse = run_rmse\n",
    "            minimum_rmse_runid = run_id\n",
    "\n",
    "print(\"Best run_id: \" + minimum_rmse_runid)\n",
    "print(\"Best run_id rmse: \" + str(minimum_rmse))    "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the best run id to fetch the individual run using the `Run` constructor along with the experiment object. Then call `get_file_names()` to see all the files available for download from this run. In this case, you only uploaded one file for each run during training."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Run\n",
    "best_run = Run(experiment=experiment, run_id=minimum_rmse_runid)\n",
    "print(best_run.get_file_names())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `download()` on the run object, specifying the model file name to download. By default this function downloads to the current directory."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run.download_file(name=\"model_alpha_0.1.pkl\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up resources\n",
    "\n",
    "Do not complete this section if you plan on running other Azure Machine Learning service tutorials.\n",
    "\n",
    "### Stop the notebook VM\n",
    "\n",
    "If you used a cloud notebook server, stop the VM when you are not using it to reduce cost.\n",
    "\n",
    "1. In your workspace, select **Compute**.\n",
    "\n",
    "1. Select the **Notebook VMs** tab in the compute page.\n",
    "\n",
    "1. From the list, select the VM.\n",
    "\n",
    "1. Select **Stop**.\n",
    "\n",
    "1. When you're ready to use the server again, select **Start**.\n",
    "\n",
    "### Delete everything\n",
    "\n",
    "If you don't plan to use the resources you created, delete them, so you don't incur any charges:\n",
    "\n",
    "1. In the Azure portal, select **Resource groups** on the far left.\n",
    "\n",
    "1. From the list, select the resource group you created.\n",
    "\n",
    "1. Select **Delete resource group**.\n",
    "\n",
    "1. Enter the resource group name. Then select **Delete**.\n",
    "\n",
    "You can also keep the resource group but delete a single workspace. Display the workspace properties and select **Delete**."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "In this tutorial, you did the following tasks:\n",
    "\n",
    "> * Connected your workspace and created an experiment\n",
    "> * Loaded data and trained scikit-learn models\n",
    "> * Viewed training results in the studio and retrieved models\n",
    "\n",
    "[Deploy your model](https://docs.microsoft.com/azure/machine-learning/service/tutorial-deploy-models-with-aml) with Azure Machine Learning.\n",
    "Learn how to develop [automated machine learning](https://docs.microsoft.com/azure/machine-learning/service/tutorial-auto-train-models) experiments."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/create-first-ml-experiment/tutorial-1st-experiment-sdk-train.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part4-data.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/tutorials/get-started-day1/day1-part4-data.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part4-data.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Bring your own data (Part 4 of 4)\n",
    "\n",
    "---\n",
    "## Introduction\n",
    "\n",
    "In the previous [Tutorial: Train a model in the cloud](day1-part3-train-model.ipynb) article, the CIFAR10 data was downloaded using the inbuilt `torchvision.datasets.CIFAR10` method in the PyTorch API. However, in many cases you are going to want to use your own data in a remote training run. This article focuses on the workflow you can leverage such that you can work with your own data in Azure Machine Learning. \n",
    "\n",
    "By the end of this tutorial you would have a better understanding of:\n",
    "\n",
    "- How to upload your data to Azure\n",
    "- Best practices for working with cloud data in Azure Machine Learning\n",
    "- Working with command-line arguments\n",
    "\n",
    "This notebook follows the steps provided on the [Python (day 1) - bring your own data documentation page](https://aka.ms/day1aml).\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- You have completed:\n",
    "  - Setup on your [Azure Machine Learning Compute Cluster](day1-part1-setup.ipynb)\n",
    "  - [Tutorial: Hello World](day1-part2-hello-world.ipynb)\n",
    "  - [Tutorial: Train a model in the cloud](day1-part3-train-model.ipynb)\n",
    "- Familiarity with Python and Machine Learning concepts\n",
    "- If you are using a compute instance in Azure Machine Learning to run this notebook series, you are all set. Otherwise, please follow the [Configure a development environment for Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/how-to-configure-environment)\n",
    "\n",
    "---\n",
    "\n",
    "## Your machine learning code\n",
    "\n",
    "By now you have your training script running in Azure Machine Learning, and can monitor the model performance. Let's _parametrize_ the training script by introducing\n",
    "arguments. Using arguments will allow you to easily compare different hyperparmeters.\n",
    "\n",
    "Presently our training script is set to download the CIFAR10 dataset on each run. The python code in [code/pytorch-cifar10-your-data/train.py](code/pytorch-cifar10-your-data/train.py) now uses **`argparse` to parametize the script.**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part4-data.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding your machine learning code changes\n",
    "\n",
    "The code used in `train.py` has leveraged the `argparse` library to set up the `data_path`, `learning_rate`, and `momentum`.\n",
    "\n",
    "```python\n",
    "# .... other code\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_path', type=str, help='Path to the training data')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.001, help='Learning rate for SGD')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='Momentum for SGD')\n",
    "args = parser.parse_args()\n",
    "# ... other code\n",
    "```\n",
    "\n",
    "Also the `train.py` script was adapted to update the optimizer to use the user-defined parameters:\n",
    "\n",
    "```python\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(),\n",
    "    lr=args.learning_rate,     # get learning rate from command-line argument\n",
    "    momentum=args.momentum,    # get momentum from command-line argument\n",
    ")\n",
    "```\n",
    "\n",
    "## Test your machine learning code locally\n",
    "\n",
    "To run the modified training script locally, run the python command below.\n",
    "\n",
    "You avoid having to download the CIFAR10 dataset by passing in a local path to the\n",
    "data. Also you can experiment with different values for _learning rate_ and\n",
    "_momentum_ hyperparameters without having to hard-code them in the training script.\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part4-data.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python code/pytorch-cifar10-your-data/train.py --data_path ./data --learning_rate 0.003 --momentum 0.92"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part4-data.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload your data to Azure\n",
    "\n",
    "In order to run this script in Azure Machine Learning, you need to make your training data available in Azure. Your Azure Machine Learning workspace comes equipped with a _default_ **Datastore** - an Azure Blob storage account - that you can use to store your training data.\n",
    "\n",
    "> <span style=\"color:purple; font-weight:bold\">! NOTE <br>\n",
    "> Azure Machine Learning allows you to connect other cloud-based datastores that store your data. For more details, see [datastores documentation](./concept-data.md).</span>\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part4-data.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()\n",
    "datastore = ws.get_default_datastore()\n",
    "datastore.upload(src_dir='./data', target_path='datasets/cifar10', overwrite=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part4-data.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `target_path` specifies the path on the datastore where the CIFAR10 data will be uploaded.\n",
    "\n",
    "## Submit your machine learning code to Azure Machine Learning\n",
    "\n",
    "As you have done previously, create a new Python control script:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part4-data.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remote run",
     "batchai",
     "configure run",
     "use notebook widget",
     "get metrics",
     "use datastore"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Environment, ScriptRunConfig, Dataset\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "dataset = Dataset.File.from_files(path=(datastore, 'datasets/cifar10'))\n",
    "\n",
    "experiment = Experiment(workspace=ws, name='day1-experiment-data')\n",
    "\n",
    "config = ScriptRunConfig(source_directory='./code/pytorch-cifar10-your-data',\n",
    "                         script='train.py',\n",
    "                         compute_target='cpu-cluster',\n",
    "                         arguments=[\n",
    "                            '--data_path', dataset.as_named_input('input').as_mount(),\n",
    "                            '--learning_rate', 0.003,\n",
    "                            '--momentum', 0.92])\n",
    "\n",
    "# set up pytorch environment\n",
    "env = Environment.from_conda_specification(name='pytorch-aml-env',file_path='configuration/pytorch-aml-env.yml')\n",
    "config.run_config.environment = env\n",
    "\n",
    "run = experiment.submit(config)\n",
    "RunDetails(run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part4-data.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the control code\n",
    "\n",
    "The above control code has the following additional code compared to the control code written in [previous tutorial](03-train-model.ipynb)\n",
    "\n",
    "**`dataset = Dataset.File.from_files(path=(datastore, 'datasets/cifar10'))`**: A Dataset is used to reference the data you uploaded to the Azure Blob Store. Datasets are an abstraction layer on top of your data that are designed to improve reliability and trustworthiness.\n",
    "\n",
    "\n",
    "**`config = ScriptRunConfig(...)`**: We modified the `ScriptRunConfig` to include a list of arguments that will be passed into `train.py`. We also specified `dataset.as_named_input('input').as_mount()`, which means the directory specified will be _mounted_ to the compute target.\n",
    "\n",
    "## Inspect the 70_driver_log log file\n",
    "\n",
    "In the navigate to the 70_driver_log.txt file - you should see the following output:\n",
    "\n",
    "```\n",
    "Processing 'input'.\n",
    "Processing dataset FileDataset\n",
    "{\n",
    "  \"source\": [\n",
    "    \"('workspaceblobstore', 'datasets/cifar10')\"\n",
    "  ],\n",
    "  \"definition\": [\n",
    "    \"GetDatastoreFiles\"\n",
    "  ],\n",
    "  \"registration\": {\n",
    "    \"id\": \"XXXXX\",\n",
    "    \"name\": null,\n",
    "    \"version\": null,\n",
    "    \"workspace\": \"Workspace.create(name='XXXX', subscription_id='XXXX', resource_group='X')\"\n",
    "  }\n",
    "}\n",
    "Mounting input to /tmp/tmp9kituvp3.\n",
    "Mounted input to /tmp/tmp9kituvp3 as folder.\n",
    "Exit __enter__ of DatasetContextManager\n",
    "Entering Run History Context Manager.\n",
    "Current directory:  /mnt/batch/tasks/shared/LS_root/jobs/dsvm-aml/azureml/tutorial-session-3_1600171983_763c5381/mounts/workspaceblobstore/azureml/tutorial-session-3_1600171983_763c5381\n",
    "Preparing to call script [ train.py ] with arguments: ['--data_path', '$input', '--learning_rate', '0.003', '--momentum', '0.92']\n",
    "After variable expansion, calling script [ train.py ] with arguments: ['--data_path', '/tmp/tmp9kituvp3', '--learning_rate', '0.003', '--momentum', '0.92']\n",
    "\n",
    "Script type = None\n",
    "===== DATA =====\n",
    "DATA PATH: /tmp/tmp9kituvp3\n",
    "LIST FILES IN DATA PATH...\n",
    "['cifar-10-batches-py', 'cifar-10-python.tar.gz']\n",
    "```\n",
    "\n",
    "Notice:\n",
    "\n",
    "1. Azure Machine Learning has mounted the blob store to the compute cluster automatically for you.\n",
    "2. The ``dataset.as_named_input('input').as_mount()`` used in the control script resolves to the mount point\n",
    "3. In the machine learning code we include a line to list the directorys under the data directory - you can see the list above.\n",
    "\n",
    "## Clean up resources\n",
    "\n",
    "The compute cluster will scale down to zero after 40minutes of idle time. When the compute is idle you will not be charged. If you want to delete the cluster use:\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part4-data.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "ct = ws.compute_targets['cpu-cluster']\n",
    "# ct.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part4-data.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're not going to use what you've created here, delete the resources you just created with this quickstart so you don't incur any charges for storage. In the Azure portal, select and delete your resource group.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To learn more about the capabilities of Azure Machine Learning please refer to the following documentation:\n",
    "\n",
    "* [Azure Machine Learning Pipelines](https://docs.microsoft.com/en-us/azure/machine-learning/concept-ml-pipelines#building-pipelines-with-the-python-sdk)\n",
    "* [Deploy models for real-time scoring](https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-deploy-models-with-aml)\n",
    "* [Hyper parameter tuning with Azure Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)\n",
    "* [Prep your code for production](https://docs.microsoft.com/azure/machine-learning/tutorial-convert-ml-experiment-to-production)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part4-data.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part3-train-model.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/tutorials/get-started-day1/day1-part3-train-model.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part3-train-model.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Train your first ML model (Part 3 of 4)\n",
    "\n",
    "---\n",
    "## Introduction\n",
    "In the [previous tutorial](day1-part2-hello-world.ipynb), you ran a trivial \"Hello world!\" script in the cloud using Azure Machine Learning's Python SDK. This time you take it a step further by submitting a script that will train a machine learning model. This example will help you understand how Azure Machine Learning eases consistent behavior between debugging on a compute instance or laptop development environment, and remote runs.\n",
    "\n",
    "Learning these concepts means that by the end of this session, you can:\n",
    "\n",
    "* Use Conda to define an Azure Machine Learning environment.\n",
    "* Train a model in the cloud.\n",
    "* Log metrics to Azure Machine Learning.\n",
    "\n",
    "This notebook follows the steps provided on the [Python (day 1) - train a model documentation page](https://aka.ms/day1aml).\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- You have completed the following:\n",
    "  - [Setup on your compute cluster](day1-part1-setup.ipynb)\n",
    "  - [Tutorial: Hello World example](day1-part2-hello-world.md)\n",
    "- Familiarity with Python and Machine Learning concepts\n",
    "- If you are using a compute instance in Azure Machine Learning to run this notebook series, you are all set. Otherwise, please follow the [Configure a development environment for Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/how-to-configure-environment)\n",
    "---\n",
    "\n",
    "## Your machine learning code\n",
    "\n",
    "This tutorial shows you how to train a PyTorch model on the CIFAR 10 dataset using an Azure Machine Learning Cluster. In this case you will be using a CPU cluster, but this could equally be a GPU cluster. Whilst this tutorial uses PyTorch, the steps we show you apply to *any* machine learning code. \n",
    "\n",
    "In the `code/pytorch-cifar10-train` subdirectory you will see 2 files:\n",
    "\n",
    "1. [model.py](code/pytorch-cifar10-train/model.py) - this defines the neural network architecture\n",
    "1. [train.py](code/pytorch-cifar10-train/train.py) - This is the training script. This script downloads the CIFAR10 dataset using PyTorch `torchvision.dataset` APIs, sets up the network defined in\n",
    "`model.py`, and trains it for two epochs using standard SGD and cross-entropy loss.\n",
    "\n",
    "Note the code is based on [this introductory example from PyTorch](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html). "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part3-train-model.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Python environment for your machine learning code\n",
    "\n",
    "For demonstration purposes, we're going to use a Conda environment but the steps for a pip virtual environment are almost identical. This environment has all the dependencies that your model and training script require. \n",
    "\n",
    "In the `configuration` directory there is a *conda dependencies* file called [pytorch-env.yml](configuration/pytorch-env.yml) that specifies the dependencies to run the python code. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part3-train-model.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test in your development environment\n",
    "\n",
    "Test your script runs on either your compute instance or laptop using this environment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part3-train-model.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python code/pytorch-cifar10-train/train.py"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part3-train-model.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You should notice that the script has downloaded the data into a directory called `data`.**\n",
    "\n",
    "## Submit your machine learning code to Azure Machine Learning\n",
    "\n",
    "The difference to the control script below and the one used to submit \"hello world\" is that you adjust the environment to be set from the conda dependencies file you created earlier.\n",
    "\n",
    "> <span style=\"color:purple; font-weight:bold\">! NOTE <br>\n",
    "> The first time you run this script, Azure Machine Learning will build a new docker image from your PyTorch environment. The whole run could take 5-10 minutes to complete. You can see the docker build logs in the widget by selecting the `20_image_build_log.txt` in the log files dropdown. This image will be reused in future runs making them run much quicker.</span>\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part3-train-model.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remote run",
     "batchai",
     "configure run",
     "use notebook widget"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Environment, ScriptRunConfig\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "experiment = Experiment(workspace=ws, name='day1-experiment-train')\n",
    "config = ScriptRunConfig(source_directory='code/pytorch-cifar10-train/', script='train.py', compute_target='cpu-cluster')\n",
    "\n",
    "env = Environment.from_conda_specification(name='pytorch-env', file_path='configuration/pytorch-env.yml')\n",
    "config.run_config.environment = env\n",
    "\n",
    "run = experiment.submit(config)\n",
    "\n",
    "RunDetails(run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part3-train-model.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the control code\n",
    "\n",
    "Compared to the control script that submitted the \"hello world\" example, this control script introduces the following:\n",
    "\n",
    "| Code | Description\n",
    "| --- | --- |\n",
    "| `env = Environment.from_conda_specification( ...)` | Azure Machine Learning provides the concept of an `Environment` to represent a reproducible, <br>versioned Python environment for running experiments. Here you have created it from a yaml conda dependencies file.|\n",
    "| `config.run_config.environment = env` | adds the environment to the ScriptRunConfig. |\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part3-train-model.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are many ways to create AML environments, including [from a pip requirements.txt](https://docs.microsoft.com/python/api/azureml-core/azureml.core.environment.environment?view=azure-ml-py&preserve-view=true#from-pip-requirements-name--file-path-), or even [from an existing local Conda environment](https://docs.microsoft.com/python/api/azureml-core/azureml.core.environment.environment?view=azure-ml-py&preserve-view=true#from-existing-conda-environment-name--conda-environment-name-).**\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part3-train-model.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your image is built, select `70_driver_log.txt` to see the output of your training script, which should look like:\n",
    "\n",
    "```txt\n",
    "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
    "...\n",
    "Files already downloaded and verified\n",
    "epoch=1, batch= 2000: loss 2.19\n",
    "...\n",
    "epoch=2, batch=12000: loss 1.27\n",
    "Finished Training\n",
    "```\n",
    "\n",
    "Environments can be registered to a workspace with `env.register(ws)`, allowing them to be easily shared, reused, and versioned. Environments make it easy to reproduce previous results and to collaborate with your team.\n",
    "\n",
    "Azure Machine Learning also maintains a collection of curated environments. These environments cover common ML scenarios and are backed by cached Docker images. Cached Docker images make the first remote run faster.\n",
    "\n",
    "In short, using registered environments can save you time! More details can be found on the [environments documentation](./how-to-use-environments.md)\n",
    "\n",
    "## Log training metrics\n",
    "\n",
    "Now that you have a model training in Azure Machine Learning, start tracking some performance metrics.\n",
    "The current training script prints metrics to the terminal. Azure Machine Learning provides a\n",
    "mechanism for logging metrics with more functionality. By adding a few lines of code, you gain the ability to visualize metrics in the studio and to compare metrics between multiple runs.\n",
    "\n",
    "### Machine learning code updates\n",
    "\n",
    "In the `code/pytorch-cifar10-train-with-logging` directory you will notice the [train.py](code/pytorch-cifar10-train-with-logging/train.py) script has been modified with two additional lines that will log the loss to the Azure Machine Learning Studio:\n",
    "\n",
    "```python\n",
    "# in train.py\n",
    "run = Run.get_context()\n",
    "...\n",
    "run.log('loss', loss)\n",
    "```\n",
    "\n",
    "Metrics in Azure Machine Learning are:\n",
    "\n",
    "- Organized by experiment and run so it's easy to keep track of and\n",
    "compare metrics.\n",
    "- Equipped with a UI so we can visualize training performance in the studio or in the notebook widget.\n",
    "- **Designed to scale** You can submit concurrent experiments and the Azure Machine Learning cluster will scale out (up to the maximum node count of the cluster) to run the experiments in parallel."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part3-train-model.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the Environment for your machine learning code\n",
    "\n",
    "The `train.py` script just took a new dependency on `azureml.core`. Therefore, the conda dependecies file [pytorch-aml-env](configuration/pytorch-aml-env.yml) reflects this change."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part3-train-model.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit your machine learning code to Azure Machine Learning\n",
    "Submit your code once more. This time the widget includes the metrics where you can now see live updates on the model training loss!"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part3-train-model.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remote run",
     "batchai",
     "configure run",
     "use notebook widget",
     "get metrics"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Environment, ScriptRunConfig\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "experiment = Experiment(workspace=ws, name='day1-experiment-train')\n",
    "config = ScriptRunConfig(source_directory='code/pytorch-cifar10-train-with-logging', script='train.py', compute_target='cpu-cluster')\n",
    "\n",
    "env = Environment.from_conda_specification(name='pytorch-aml-env', file_path='configuration/pytorch-aml-env.yml')\n",
    "config.run_config.environment = env\n",
    "\n",
    "run = experiment.submit(config)\n",
    "RunDetails(run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part3-train-model.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "In this session, you upgraded from a basic \"Hello world!\" script to a more realistic\n",
    "training script that required a specific Python environment to run. You saw how\n",
    "to take a local Conda environment to the cloud with Azure Machine Learning Environments. Finally, you\n",
    "saw how in a few lines of code you can log metrics to Azure Machine Learning.\n",
    "\n",
    "In the next session, you'll see how to work with data in Azure Machine Learning by uploading the CIFAR10\n",
    "dataset to Azure.\n",
    "\n",
    "[Tutorial: Bring your own data](day1-part4-data.ipynb)\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part3-train-model.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part1-setup.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/tutorials/day1-part1-setup.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part1-setup.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Get started (day 1) with Azure Machine Learning (Part 1 of 4)\n",
    "\n",
    "---\n",
    "## Introduction <a id='intro'></a>\n",
    "\n",
    "In this **four-part tutorial series**, you will learn the fundamentals of Azure Machine Learning and complete jobs-based Python machine learning tasks in the Azure cloud, including:\n",
    "\n",
    "1. Set up a compute cluster\n",
    "2. Run code in the cloud using Azure Machine Learning's Python SDK.\n",
    "3. Manage the Python environment you use for model training.\n",
    "4. Upload data to Azure and consume that data in training.\n",
    "\n",
    "In this first part of the tutorial series you learn how to create an Azure Machine Learning Compute Cluster that will be used in subsequent parts of the series to submit jobs to. This notebook follows the steps provided on the [Python (day 1) - set up local computer documentation page](https://aka.ms/day1aml).\n",
    "\n",
    "## Pre-requisites <a id='pre-reqs'></a>\n",
    "\n",
    "- An Azure Subscription. If you don't have an Azure subscription, create a free account before you begin. Try [Azure Machine Learning](https://aka.ms/AMLFree) today.\n",
    "- Familiarity with Python and Machine Learning concepts. For example, environments, training, scoring, and so on.\n",
    "- If you are using a compute instance in Azure Machine Learning to run this notebook series, you are all set. Otherwise, please follow the [Configure a development environment for Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/how-to-configure-environment)\n",
    "\n",
    "---"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part1-setup.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure you have the latest Azure Machine Learning Python SDK\n",
    "\n",
    "This tutorial series depends on having the Azure Machine Learning SDK version 1.14.0 onwards installed. You can check your version using the code cell below."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part1-setup.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import VERSION\n",
    "\n",
    "print ('Version: ' + VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part1-setup.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your version is below 1.14.0, then upgrade the SDK using `pip` (**Note: You may need to restart your kernel for the changes to take effect. Re-run the cell above to ensure you have the right version**).\n",
    "\n",
    "```bash\n",
    "!pip install -U azureml-sdk\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part1-setup.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Azure Machine Learning compute cluster <a id='createcc'></a>\n",
    "\n",
    "As this tutorial focuses on jobs-based machine learning tasks, you will be submitting python code to run on an Azure Machine Learning **Compute cluster**, which is well suited for large jobs and production. Therefore, you create an Azure Machine Learning compute cluster that will auto-scale between zero and four nodes:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part1-setup.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create mlc",
     "batchai"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "ws = Workspace.from_config() # this automatically looks for a directory .azureml\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"cpu-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                            max_nodes=4, \n",
    "                                                            idle_seconds_before_scaledown=2400)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "cpu_cluster.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part1-setup.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:darkblue;font-weight:bold\"> ! INFORMATION   \n",
    "> When the cluster has been created it will have 0 nodes provisioned. Therefore, the cluster does not incur costs until you submit a job. This cluster will scale down when it has been idle for 2400 seconds (40 minutes).</span>"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part1-setup.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In the next tutorial, you walk through submitting a script to the Azure Machine Learning compute cluster.\n",
    "\n",
    "[Tutorial: Run \"Hello World\" Python Script on Azure](day1-part2-hello-world.ipynb)\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part1-setup.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part2-hello-world.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/tutorials/get-started-day1/day1-part2-hello-world.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part2-hello-world.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: \"Hello World\" (Part 2 of 4)\n",
    "\n",
    "---\n",
    "## Introduction\n",
    "In **part 2 of this get started series**, you will submit a trivial \"hello world\" python script to the cloud by:\n",
    "\n",
    "- Running Python code in the cloud with Azure Machine Learning SDK\n",
    "- Switching between debugging locally on a compute instance.\n",
    "- Submitting remote runs in the cloud\n",
    "- Monitoring and recording runs in the Azure Machine Learning studio\n",
    "\n",
    "This notebook follows the steps provided on the [Python (day 1) - \"hello world\" documentation page](https://aka.ms/day1aml). This tutorial is part of a **four-part tutorial series** in which you learn the fundamentals of Azure Machine Learning and complete simple jobs-based machine learning tasks in the Azure cloud. It builds off the work you completed in [Tutorial part 1: set up an Azure Machine Learning compute cluster](day1-part1-setup.ipynb).\n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "- Complete [Tutorial part 1: set up an Azure Machine Learning compute cluster](day1-part1-setup.ipynb) if you don't already have an Azure Machine Learning compute cluster.\n",
    "- Familiarity with Python and Machine Learning concepts.\n",
    "- If you are using a compute instance in Azure Machine Learning to run this notebook series, you are all set. Otherwise, please follow the [Configure a development environment for Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/how-to-configure-environment)\n",
    "---"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part2-hello-world.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your code\n",
    "\n",
    "In the `code/hello` subdirectory you will find a trivial python script [hello.py](code/hello/hello.py) that has the following code:\n",
    "\n",
    "```Python\n",
    "# code/hello/hello.py\n",
    "print(\"hello world!\")\n",
    "```\n",
    "\n",
    "In this tutorial you are going to submit this trivial python script to an Azure Machine Learning Compute Cluster."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part2-hello-world.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test in your development environment\n",
    "\n",
    "You can test your code works on a compute instance or locally (for example, a laptop), which has the benefit of interactive debugging of code:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part2-hello-world.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python code/hello/hello.py"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part2-hello-world.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit your code to Azure Machine Learning\n",
    "\n",
    "Below you create a __*control script*__ this is where you specify _how_ your code is submitted to Azure Machine Learning. The code you submit to Azure Machine Learning (in this case `hello.py`) does not need anything specific to Azure Machine Learning - it can be any valid Python code. It is only the control script that is Azure Machine Learning specific.\n",
    "\n",
    "The code below will show a Jupyter widget that tracks the progress of your run, and displays logs.\n",
    "\n",
    "> <span style=\"color:purple; font-weight:bold\">! NOTE <br>\n",
    "> The very first run will take 5-10minutes to complete. This is because in the background a docker image is built in the cloud, the compute cluster is resized from 0 to 1 node, and the docker image is downloaded to the compute. Subsequent runs are much quicker (~15 seconds) as the docker image is cached on the compute - you can test this by resubmitting the code below after the first run has completed.</span>"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part2-hello-world.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remote run",
     "batchai",
     "configure run",
     "use notebook widget"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, ScriptRunConfig\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "experiment = Experiment(workspace=ws, name='day1-experiment-hello')\n",
    "\n",
    "config = ScriptRunConfig(source_directory='./code/hello', script='hello.py', compute_target='cpu-cluster')\n",
    "\n",
    "run = experiment.submit(config)\n",
    "RunDetails(run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part2-hello-world.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the control code\n",
    "\n",
    "| Code |Description  | \n",
    "|---|---|\n",
    "| `ws = Workspace.from_config()` |   [Workspace](https://docs.microsoft.com/python/api/azureml-core/azureml.core.workspace.workspace?view=azure-ml-py&preserve-view=true) connects to your Azure Machine Learning workspace, so that you can communicate with your Azure Machine Learning resources. |\n",
    "| `experiment =  Experiment( ... )`   | [Experiment](https://docs.microsoft.com/python/api/azureml-core/azureml.core.experiment.experiment?view=azure-ml-py&preserve-view=true) provides a simple way to organize multiple runs under a single name. <br>Later you can see how experiments make it easy to compare metrics between dozens of runs.  |\n",
    "|  `config = ScriptRunConfig( ... )` |  [ScriptRunConfig](https://docs.microsoft.com/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py&preserve-view=true) wraps your `hello.py` code and passes it to your workspace.<br> As the name suggests, you can use this class to _configure_ how you want your _script_ to _run_ in Azure Machine Learning. <br>Also specifies what compute target the script will run on.  <br>In this code, the target is the compute cluster you created in the [setup tutorial](tutorial-1st-experiment-sdk-setup-local.md). |\n",
    "| `run = experiment.submit(config)` |  Submits your script. This submission is called a [Run](https://docs.microsoft.com/python/api/azureml-core/azureml.core.run(class)?view=azure-ml-py&preserve-view=true).  <br>A run encapsulates a single execution of your code. Use a run to monitor the script progress, capture the output,<br> analyze the results, visualize metrics and more. |\n",
    "| `aml_url = run.get_portal_url()` | The `run` object provides a handle on the execution of your code. Monitor its progress from <br> the Azure Machine Learning Studio with the URL that is printed from the python script. |\n",
    "|`RunDetails(run).show()` | There is an Azure Machine Learning widget that shows the progress of your job along with streaming the log files.\n",
    "\n",
    "## View the logs\n",
    "\n",
    "The widget has a dropdown box titled **Output logs** select `70_driver_log.txt`, which shows the following standard output: \n",
    "\n",
    "```\n",
    " 1: [2020-08-04T22:15:44.407305] Entering context manager injector.\n",
    " 2: [context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError', 'UserExceptions:context_managers.UserExceptions'], invocation=['hello.py'])\n",
    " 3: Starting the daemon thread to refresh tokens in background for process with pid = 31263\n",
    " 4: Entering Run History Context Manager.\n",
    " 5: Preparing to call script [ hello.py ] with arguments: []\n",
    " 6: After variable expansion, calling script [ hello.py ] with arguments: []\n",
    " 7:\n",
    " 8: Hello world!\n",
    " 9: Starting the daemon thread to refresh tokens in background for process with pid = 31263\n",
    "10:\n",
    "11:\n",
    "12: The experiment completed successfully. Finalizing run...\n",
    "13: Logging experiment finalizing status in history service.\n",
    "14: [2020-08-04T22:15:46.541334] TimeoutHandler __init__\n",
    "15: [2020-08-04T22:15:46.541396] TimeoutHandler __enter__\n",
    "16: Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
    "17: 1 items cleaning up...\n",
    "18: Cleanup took 0.1812913417816162 seconds\n",
    "19: [2020-08-04T22:15:47.040203] TimeoutHandler __exit__\n",
    "```\n",
    "\n",
    "On line 8 above, you see the \"Hello world!\" output. The 70_driver_log.txt file contains the standard output from run and can be useful when debugging remote runs in the cloud. You can also view the run by clicking on the **Click here to see the run in Azure Machine Learning studio** link in the wdiget.\n",
    "\n",
    "## Next steps\n",
    "\n",
    "In this tutorial, you took a simple \"hello world\" script and ran it on Azure. You saw how to connect to your Azure Machine Learning workspace, create an Experiment, and submit your `hello.py` code to the cloud.\n",
    "\n",
    "In the [next tutorial](day1-part3-train-model.ipynb), you build on these learnings by running something more interesting than `print(\"Hello world!\")`.\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/tutorials/get-started-day1/day1-part2-hello-world.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication in Azure Machine Learning\n",
    "\n",
    "This notebook shows you how to authenticate to your Azure ML Workspace using\n",
    "\n",
    " 1. Interactive Login Authentication\n",
    " 2. Azure CLI Authentication\n",
    " 3. Managed Service Identity (MSI) Authentication\n",
    " 4. Service Principal Authentication\n",
    " 5. Token Authentication\n",
    " \n",
    "The interactive authentication is suitable for local experimentation on your own computer. Azure CLI authentication is suitable if you are already using Azure CLI for managing Azure resources, and want to sign in only once. The MSI and Service Principal authentication are suitable for automated workflows, for example as part of Azure Devops build."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Authentication\n",
    "\n",
    "Interactive authentication is the default mode when using Azure ML SDK.\n",
    "\n",
    "When you connect to your workspace using workspace.from_config, you will get an interactive login dialog."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, if you explicitly specify the subscription ID, resource group and workspace name, you will get the dialog."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace(subscription_id=\"my-subscription-id\",\n",
    "               resource_group=\"my-ml-rg\",\n",
    "               workspace_name=\"my-ml-workspace\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the user you're authenticated as must have access to the subscription and resource group. If you receive an error\n",
    "\n",
    "```\n",
    "AuthenticationException: You don't have access to xxxxxx-xxxx-xxx-xxx-xxxxxxxxxx subscription. All the subscriptions that you have access to = ...\n",
    "```\n",
    "\n",
    "check that the you used correct login and entered the correct subscription ID."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, you may see a version of the error message containing text: ```All the subscriptions that you have access to = []```\n",
    "\n",
    "In such a case, you may have to specify the tenant ID of the Azure Active Directory you're using. An example would be accessing a subscription as a guest to a tenant that is not your default. You specify the tenant by explicitly instantiating _InteractiveLoginAuthentication_ with Tenant ID as argument. The Tenant ID can be found, for example, from https://portal.azure.com under **Azure Active Directory**,  **Properties** as Directory ID."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "sample-interactiveloginauth-tenantid"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "interactive_auth = InteractiveLoginAuthentication(tenant_id=\"my-tenant-id\")\n",
    "\n",
    "ws = Workspace(subscription_id=\"my-subscription-id\",\n",
    "               resource_group=\"my-ml-rg\",\n",
    "               workspace_name=\"my-ml-workspace\",\n",
    "               auth=interactive_auth)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite having access to the workspace, you may sometimes see the following error when retrieving it:\n",
    "\n",
    "```\n",
    "You are currently logged-in to xxxxxxxx-xxx-xxxx-xxxx-xxxxxxxxxxxx tenant. You don't have access to xxxxxx-xxxx-xxx-xxx-xxxxxxxxxx subscription, please check if it is in this tenant.\n",
    "```\n",
    "\n",
    "This error sometimes occurs when you are trying to access a subscription to which you were recently added. In this case, you need to force authentication again to avoid using a cached authentication token that has not picked up the new permissions. You can do so by setting `force=true` on the `InteractiveLoginAuthentication()` object's constructor as follows:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forced_interactive_auth = InteractiveLoginAuthentication(tenant_id=\"my-tenant-id\", force=True)\n",
    "\n",
    "ws = Workspace(subscription_id=\"my-subscription-id\",\n",
    "               resource_group=\"my-ml-rg\",\n",
    "               workspace_name=\"my-ml-workspace\",\n",
    "               auth=forced_interactive_auth)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure CLI Authentication\n",
    "\n",
    "If you have installed azure-cli package, and used ```az login``` command to log in to your Azure Subscription, you can use _AzureCliAuthentication_ class.\n",
    "\n",
    "Note that interactive authentication described above won't use existing Azure CLI auth tokens. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "sample-azurecliauth"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.authentication import AzureCliAuthentication\n",
    "\n",
    "cli_auth = AzureCliAuthentication()\n",
    "\n",
    "ws = Workspace(subscription_id=\"my-subscription-id\",\n",
    "               resource_group=\"my-ml-rg\",\n",
    "               workspace_name=\"my-ml-workspace\",\n",
    "               auth=cli_auth)\n",
    "\n",
    "print(\"Found workspace {} at location {}\".format(ws.name, ws.location))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSI Authentication\n",
    "\n",
    "__Note__: _MSI authentication is supported only when using SDK from Azure Virtual Machine. The code below will fail on local computer._\n",
    "\n",
    "When using Azure ML SDK on Azure Virtual Machine (VM), you can use Managed Service Identity (MSI) based authentication. This mode allows the VM connect to the Workspace without storing credentials in the Python code.\n",
    "\n",
    "As a prerequisite, enable System-assigned Managed Identity for your VM as described in [Configure managed identities for Azure resources on a VM using the Azure portal](https://docs.microsoft.com/azure/active-directory/managed-identities-azure-resources/qs-configure-portal-windows-vm).\n",
    "\n",
    "Then, assign the VM access to your Workspace. For example from Azure Portal, navigate to your workspace, select __Access Control (IAM)__, __Add Role Assignment__,  specify __Virtual Machine__ for __Assign Access To__ dropdown, and select your VM's identity.\n",
    "\n",
    "![msi assignment](images/msiaccess.PNG)\n",
    "\n",
    "After completing these steps, you can use authenticate using MsiAuthentication instance."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "sample-msiauth"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.authentication import MsiAuthentication\n",
    "\n",
    "msi_auth = MsiAuthentication()\n",
    "\n",
    "ws = Workspace(subscription_id=\"my-subscription-id\",\n",
    "               resource_group=\"my-ml-rg\",\n",
    "               workspace_name=\"my-ml-workspace\",\n",
    "               auth=msi_auth)\n",
    "\n",
    "print(\"Found workspace {} at location {}\".format(ws.name, ws.location))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Service Principal Authentication\n",
    "\n",
    "When setting up a machine learning workflow as an automated process, we recommend using Service Principal Authentication. This approach decouples the authentication from any specific user login, and allows managed access control.\n",
    "\n",
    "Note that you must have administrator privileges over the Azure subscription to complete these steps.\n",
    "\n",
    "The first step is to create a service principal. First, go to [Azure Portal](https://portal.azure.com), select **Azure Active Directory** and **App Registrations**. Then select **+New application**, give your service principal a name, for example _my-svc-principal_. You can leave other parameters as is.\n",
    "\n",
    "Then click **Register**.\n",
    "\n",
    "![service principal creation](images/svc-pr-1.PNG)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the page for your newly created service principal, copy the _Application ID_ and _Tenant ID_ as they are needed later.\n",
    "![application and tenant id](images/svc-pr-2.PNG)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then select **Certificates & secrets**, and **+New client secret** write a description for your key, and select duration. Then click **Add**, and copy the value of client secret to a secure location.\n",
    "\n",
    "\n",
    "![tenant id](images/svc-pr-3.PNG)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you need to give the service principal permissions to access your workspace. Navigate to **Resource Groups**, to the resource group for your Machine Learning Workspace. \n",
    "\n",
    "Then select **Access Control (IAM)** and **Add a role assignment**. For _Role_, specify which level of access you need to grant, for example _Contributor_. Start entering your service principal name and once it is found, select it, and click **Save**.\n",
    "\n",
    "![add role](images/svc-pr-4.PNG)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to use the service principal authentication. For example, to connect to your Workspace, see code below and enter your own values for tenant ID, application ID, subscription ID, resource group and workspace.\n",
    "\n",
    "**We strongly recommended that you do not insert the secret password to code**. Instead, you can use environment variables to pass it to your code, for example through Azure Key Vault, or through secret build variables in Azure DevOps. For local testing, you can for example use following PowerShell command to set the environment variable.\n",
    "\n",
    "```\n",
    "$env:AZUREML_PASSWORD = \"my-password\"\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "sample-serviceprincipalauth-tenantid"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "\n",
    "svc_pr_password = os.environ.get(\"AZUREML_PASSWORD\")\n",
    "\n",
    "svc_pr = ServicePrincipalAuthentication(\n",
    "    tenant_id=\"my-tenant-id\",\n",
    "    service_principal_id=\"my-application-id\",\n",
    "    service_principal_password=svc_pr_password)\n",
    "\n",
    "\n",
    "ws = Workspace(\n",
    "    subscription_id=\"my-subscription-id\",\n",
    "    resource_group=\"my-ml-rg\",\n",
    "    workspace_name=\"my-ml-workspace\",\n",
    "    auth=svc_pr\n",
    "    )\n",
    "\n",
    "print(\"Found workspace {} at location {}\".format(ws.name, ws.location))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [Register an application with the Microsoft identity platform](https://docs.microsoft.com/en-us/azure/active-directory/develop/quickstart-register-app) quickstart for more details about application registrations. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Authentication\n",
    "\n",
    "When token generation and its refresh needs to be outside on AML SDK, we recommend using Token Authentication. It can be used for getting token for AML or ARM audience. Thus giving more granular control over token generated.\n",
    "\n",
    "This authentication class requires users to provide method `get_token_for_audience` which will be called to retrieve the token based on the audience passed.\n",
    "\n",
    "Audience that is passed to `get_token_for_audience` can be ARM or AML. Exact value that will be passed as audience will depend on cloud and type for audience."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import TokenAuthentication, Audience\n",
    "\n",
    "# This is a sample method to retrieve token and will be passed to TokenAuthentication\n",
    "def get_token_for_audience(audience):\n",
    "    from adal import AuthenticationContext\n",
    "    client_id = \"my-client-id\"\n",
    "    client_secret = \"my-client-secret\"\n",
    "    tenant_id = \"my-tenant-id\"\n",
    "    auth_context = AuthenticationContext(\"https://login.microsoftonline.com/{}\".format(tenant_id))\n",
    "    resp = auth_context.acquire_token_with_client_credentials(audience,client_id,client_secret)\n",
    "    token = resp[\"accessToken\"]\n",
    "    return token\n",
    "\n",
    "\n",
    "token_auth = TokenAuthentication(get_token_for_audience=get_token_for_audience)\n",
    "\n",
    "ws = Workspace(\n",
    "    subscription_id=\"my-subscription-id\",\n",
    "    resource_group=\"my-ml-rg\",\n",
    "    workspace_name=\"my-ml-workspace\",\n",
    "    auth=token_auth\n",
    "    )\n",
    "\n",
    "print(\"Found workspace {} at location {}\".format(ws.name, ws.location))\n",
    "\n",
    "token_aml_audience = token_auth.get_token(Audience.aml)\n",
    "token_arm_audience = token_auth.get_token(Audience.arm)\n",
    "\n",
    "# Value of audience pass to `get_token_for_audience` can be retrieved as follows:\n",
    "# aud_aml_val = token_auth.get_aml_resource_id() # For AML\n",
    "# aud_arm_val = token_auth._cloud_type.endpoints.active_directory_resource_id # For ARM\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token authentication object can be used to retrieve token for either AML or ARM audience,\n",
    "which can be used by other clients to authenticate to AML or ARM"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Secrets in Remote Runs\n",
    "\n",
    "Sometimes, you may have to pass a secret to a remote run, for example username and password to authenticate against external data source.\n",
    "\n",
    "Azure ML SDK enables this use case through Key Vault associated with your workspace. The workflow for adding a secret is following.\n",
    "\n",
    "On local computer:\n",
    "\n",
    " 1. Read in a local secret, for example from environment variable or user input. To keep them secret, do not insert secret values into code as hard-coded strings.\n",
    " 2. Obtain a reference to the keyvault\n",
    " 3. Add the secret name-value pair in the key vault.\n",
    " \n",
    "The secret is then available for remote runs as shown further below.\n",
    "\n",
    "__Note__: The _azureml.core.keyvault.Keyvault_ is different from _azure.keyvault_ library. It is intended as simplified wrapper for setting, getting and listing user secrets in Workspace Key Vault."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "sample-keyvault"
    ]
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "local_secret = os.environ.get(\"LOCAL_SECRET\", default = str(uuid.uuid4())) # Use random UUID as a substitute for real secret.\n",
    "keyvault = ws.get_default_keyvault()\n",
    "keyvault.set_secret(name=\"secret-name\", value = local_secret)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _set_secret_ method adds a new secret if one doesn't exist, or updates an existing one with new value.\n",
    "\n",
    "You can list secret names you've added. This method doesn't return the values of the secrets."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyvault.list_secrets()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can retrieve the value of the secret, and validate that it matches the original value. \n",
    "\n",
    "__Note__: This method returns the secret value. Take care not to write the the secret value to output."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_secret = keyvault.get_secret(name=\"secret-name\")\n",
    "local_secret==retrieved_secret"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In submitted runs on local and remote compute, you can use the get_secret method of Run instance to get the secret value from Key Vault. \n",
    "\n",
    "The method gives you a simple shortcut: the Run instance is aware of its Workspace and Keyvault, so it can directly obtain the secret without you having to instantiate the Workspace and Keyvault within remote run.\n",
    "\n",
    "__Note__: This method returns the secret value. Take care not to write the secret to output.\n",
    "\n",
    "For example, let's create a simple script _get_secret.py_ that gets the secret we set earlier. In an actual appication, you would use the secret, for example to access a database or other password-protected resource."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile get_secret.py\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "run = Run.get_context()\n",
    "secret_value = run.get_secret(name=\"secret-name\")\n",
    "print(\"Got secret value {} , but don't write it out!\".format(len(secret_value) * \"*\"))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, submit the script as a regular script run, and find the obfuscated secret value in run output. You can use the same approach to other kinds of runs, such as Estimator ones."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.core.script_run_config import ScriptRunConfig\n",
    "\n",
    "exp = Experiment(workspace = ws, name=\"try-secret\")\n",
    "src = ScriptRunConfig(source_directory=\".\", script=\"get_secret.py\")\n",
    "\n",
    "run = exp.submit(src)\n",
    "run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, you can set and get multiple secrets using set_secrets and get_secrets methods."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved. \n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Azure Machine Learning Pipelines for Batch Inference\n",
    "\n",
    "In this notebook, we will demonstrate how to make predictions on large quantities of data asynchronously using the ML pipelines with Azure Machine Learning. Batch inference (or batch scoring) provides cost-effective inference, with unparalleled throughput for asynchronous applications. Batch prediction pipelines can scale to perform inference on terabytes of production data. Batch prediction is optimized for high throughput, fire-and-forget predictions for a large collection of data.\n",
    "\n",
    "> **Tip**\n",
    "If your system requires low-latency processing (to process a single document or small set of documents quickly), use [real-time scoring](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-consume-web-service) instead of batch prediction.\n",
    "\n",
    "In this example will be take a digit identification model already-trained on MNIST dataset using the [AzureML training with deep learning example notebook](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training-with-deep-learning/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb), and run that trained model on some of the MNIST test images in batch.  \n",
    "\n",
    "The input dataset used for this notebook differs from a standard MNIST dataset in that it has been converted to PNG images to demonstrate use of files as inputs to Batch Inference. A sample of PNG-converted images of the MNIST dataset were take from [this repository](https://github.com/myleott/mnist_png). \n",
    "\n",
    "The outline of this notebook is as follows:\n",
    "\n",
    "- Create a DataStore referencing MNIST images stored in a blob container.\n",
    "- Register the pretrained MNIST model into the model registry. \n",
    "- Use the registered model to do batch inference on the images in the data blob container.\n",
    "\n",
    "## Prerequisites\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the configuration Notebook located at https://github.com/Azure/MachineLearningNotebooks first. This sets you up with a working config file that has information on your workspace, subscription id, etc. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to workspace\n",
    "Create a workspace object from the existing workspace. Workspace.from_config() reads the file config.json and loads the details into an object named ws."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Attach existing compute resource\n",
    "By using Azure Machine Learning Compute, a managed service, data scientists can train machine learning models on clusters of Azure virtual machines. Examples include VMs with GPU support. In this tutorial, you create Azure Machine Learning Compute as your training environment. The code below creates the compute clusters for you if they don't already exist in your workspace.\n",
    "\n",
    "**Creation of compute takes approximately 5 minutes. If the AmlCompute with that name is already in your workspace the code will skip the creation process.**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpu-cluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a datastore containing sample images\n",
    "The input dataset used for this notebook differs from a standard MNIST dataset in that it has been converted to PNG images to demonstrate use of files as inputs to Batch Inference. A sample of PNG-converted images of the MNIST dataset were take from [this repository](https://github.com/myleott/mnist_png).\n",
    "\n",
    "We have created a public blob container `sampledata` on an account named `pipelinedata`, containing these images from the MNIST dataset. In the next step, we create a datastore with the name `images_datastore`, which points to this blob container. In the call to `register_azure_blob_container` below, setting the `overwrite` flag to `True` overwrites any datastore that was created previously with that name. \n",
    "\n",
    "This step can be changed to point to your blob container by providing your own `datastore_name`, `container_name`, and `account_name`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.datastore import Datastore\n",
    "\n",
    "account_name = \"pipelinedata\"\n",
    "datastore_name = \"mnist_datastore\"\n",
    "container_name = \"sampledata\"\n",
    "\n",
    "mnist_data = Datastore.register_azure_blob_container(ws, \n",
    "                      datastore_name=datastore_name, \n",
    "                      container_name=container_name, \n",
    "                      account_name=account_name,\n",
    "                      overwrite=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's specify the default datastore for the outputs."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_data_store = ws.get_default_datastore()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a FileDataset\n",
    "A [FileDataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.filedataset?view=azure-ml-py) references single or multiple files in your datastores or public urls. The files can be of any format. FileDataset provides you with the ability to download or mount the files to your compute. By creating a dataset, you create a reference to the data source location. If you applied any subsetting transformations to the dataset, they will be stored in the dataset as well. The data remains in its existing location, so no extra storage cost is incurred.",
    "\n",
    "You can use dataset objects as inputs. Register the datasets to the workspace if you want to reuse them later."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "mnist_ds_name = 'mnist_sample_data'\n",
    "\n",
    "path_on_datastore = mnist_data.path('mnist')\n",
    "input_mnist_ds = Dataset.File.from_files(path=path_on_datastore, validate=False)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input dataset can be specified as a pipeline parameter, so that you can pass in new data when rerun the PRS pipeline."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig\n",
    "from azureml.pipeline.core import PipelineParameter\n",
    "\n",
    "pipeline_param = PipelineParameter(name=\"mnist_param\", default_value=input_mnist_ds)\n",
    "input_mnist_ds_consumption = DatasetConsumptionConfig(\"minist_param_config\", pipeline_param).as_mount()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediate/Output Data\n",
    "Intermediate data (or output of a Step) is represented by [PipelineData](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinedata?view=azure-ml-py) object. PipelineData can be produced by one step and consumed in another step by providing the PipelineData object as an output of one step and the input of one or more steps."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "\n",
    "output_dir = PipelineData(name=\"inferences\", datastore=def_data_store)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Model\n",
    "\n",
    "Download and extract the model from https://pipelinedata.blob.core.windows.net/mnist-model/mnist-tf.tar.gz to \"models\" directory"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "# create directory for model\n",
    "model_dir = 'models'\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "url=\"https://pipelinedata.blob.core.windows.net/mnist-model/mnist-tf.tar.gz\"\n",
    "response = urllib.request.urlretrieve(url, \"model.tar.gz\")\n",
    "tar = tarfile.open(\"model.tar.gz\", \"r:gz\")\n",
    "tar.extractall(model_dir)\n",
    "\n",
    "os.listdir(model_dir)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the model with Workspace\n",
    "A registered model is a logical container for one or more files that make up your model. For example, if you have a model that's stored in multiple files, you can register them as a single model in the workspace. After you register the files, you can then download or deploy the registered model and receive all the files that you registered.\n",
    "\n",
    "Using tags, you can track useful information such as the name and version of the machine learning library used to train the model. Note that tags must be alphanumeric. Learn more about registering models [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where#registermodel) "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "# register downloaded model \n",
    "model = Model.register(model_path=\"models/\",\n",
    "                       model_name=\"mnist-prs\", # this is the name the model is registered as\n",
    "                       tags={'pretrained': \"mnist\"},\n",
    "                       description=\"Mnist trained tensorflow model\",\n",
    "                       workspace=ws)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using your model to make batch predictions\n",
    "To use the model to make batch predictions, you need an **entry script** and a list of **dependencies**:\n",
    "\n",
    "#### An entry script\n",
    "This script accepts requests, scores the requests by using the model, and returns the results.\n",
    "- __init()__ - Typically this function loads the model into a global object. This function is run only once at the start of batch processing per worker node/process. Init method can make use of following environment variables (ParallelRunStep input):\n",
    "    1.\tAZUREML_BI_OUTPUT_PATH \u00e2\u20ac\u201c output folder path\n",
    "- __run(mini_batch)__ - The method to be parallelized. Each invocation will have one minibatch.<BR>\n",
    "__mini_batch__: Batch inference will invoke run method and pass either a list or Pandas DataFrame as an argument to the method. Each entry in min_batch will be - a filepath if input is a FileDataset, a Pandas DataFrame if input is a TabularDataset.<BR>\n",
    "__run__ method response: run() method should return a Pandas DataFrame or an array. For append_row output_action, these returned elements are appended into the common output file. For summary_only, the contents of the elements are ignored. For all output actions, each returned output element indicates one successful inference of input element in the input mini-batch.\n",
    "    User should make sure that enough data is included in inference result to map input to inference. Inference output will be written in output file and not guaranteed to be in order, user should use some key in the output to map it to input.\n",
    "    \n",
    "\n",
    "#### Dependencies\n",
    "Helper scripts or Python/Conda packages required to run the entry script."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts_folder = \"Code\"\n",
    "script_file = \"digit_identification.py\"\n",
    "\n",
    "# peek at contents\n",
    "with open(os.path.join(scripts_folder, script_file)) as inference_file:\n",
    "    print(inference_file.read())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and run the batch inference pipeline\n",
    "The data, models, and compute resource are now available. Let's put all these together in a pipeline."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Specify the environment to run the script\n",
    "Specify the conda dependencies for your script. This will allow us to install pip packages as well as configure the inference environment.\n",
    "* Always include **azureml-core** and **azureml-dataset-runtime\\[fuse\\]** in the pip package list to make ParallelRunStep run properly.\n",
    "\n",
    "If you're using custom image (`batch_env.python.user_managed_dependencies = True`), you need to install the package to your image."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import CondaDependencies, DEFAULT_CPU_IMAGE\n",
    "\n",
    "batch_conda_deps = CondaDependencies.create(pip_packages=[\"tensorflow==1.15.2\", \"pillow\", \n",
    "                                                          \"azureml-core\", \"azureml-dataset-runtime[fuse]\"])\n",
    "batch_env = Environment(name=\"batch_environment\")\n",
    "batch_env.python.conda_dependencies = batch_conda_deps\n",
    "batch_env.docker.enabled = True\n",
    "batch_env.docker.base_image = DEFAULT_CPU_IMAGE"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create the configuration to wrap the inference script"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineParameter\n",
    "from azureml.pipeline.steps import ParallelRunStep, ParallelRunConfig\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory=scripts_folder,\n",
    "    entry_script=script_file,\n",
    "    mini_batch_size=PipelineParameter(name=\"batch_size_param\", default_value=\"5\"),\n",
    "    error_threshold=10,\n",
    "    output_action=\"append_row\",\n",
    "    append_row_file_name=\"mnist_outputs.txt\",\n",
    "    environment=batch_env,\n",
    "    compute_target=compute_target,\n",
    "    process_count_per_node=PipelineParameter(name=\"process_count_param\", default_value=2),\n",
    "    node_count=2\n",
    ")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the pipeline step\n",
    "Create the pipeline step using the script, environment configuration, and parameters. Specify the compute target you already attached to your workspace as the target of execution of the script. We will use ParallelRunStep to create the pipeline step."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallelrun_step = ParallelRunStep(\n",
    "    name=\"predict-digits-mnist\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[ input_mnist_ds_consumption ],\n",
    "    output=output_dir,\n",
    "    allow_reuse=False\n",
    ")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline\n",
    "At this point you can run the pipeline and examine the output it produced. The Experiment object is used to track the run of the pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\n",
    "experiment = Experiment(ws, 'digit_identification')\n",
    "pipeline_run = experiment.submit(pipeline)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor the run\n",
    "\n",
    "The pipeline run status could be checked in Azure Machine Learning portal (https://ml.azure.com). The link to the pipeline run could be retrieved by inspecting the `pipeline_run` object."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will output information of the pipeline run, including the link to the details page of portal.\n",
    "pipeline_run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: View detailed logs (streaming) "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait the run for completion and show output log to console\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the prediction results per input image\n",
    "In the digit_identification.py file above you can see that the ResultList with the filename and the prediction result gets returned. These are written to the DataStore specified in the PipelineData object as the output data, which in this case is called *inferences*. This containers the outputs from  all of the worker nodes used in the compute cluster. You can download this data to view the results ... below just filters to the first 10 rows"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tempfile\n",
    "\n",
    "batch_run = pipeline_run.find_step_run(parallelrun_step.name)[0]\n",
    "batch_output = batch_run.get_output_data(output_dir.name)\n",
    "\n",
    "target_dir = tempfile.mkdtemp()\n",
    "batch_output.download(local_path=target_dir)\n",
    "result_file = os.path.join(target_dir, batch_output.path_on_datastore, parallel_run_config.append_row_file_name)\n",
    "\n",
    "df = pd.read_csv(result_file, delimiter=\":\", header=None)\n",
    "df.columns = [\"Filename\", \"Prediction\"]\n",
    "print(\"Prediction has \", df.shape[0], \" rows\")\n",
    "df.head(10) "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resubmit a with different dataset\n",
    "Since we made the input a `PipelineParameter`, we can resubmit with a different dataset without having to create an entirely new experiment. We'll use the same datastore but use only a single image."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_on_datastore = mnist_data.path('mnist/0.png')\n",
    "single_image_ds = Dataset.File.from_files(path=path_on_datastore, validate=False)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run_2 = experiment.submit(pipeline, \n",
    "                                   pipeline_parameters={\"mnist_param\": single_image_ds, \n",
    "                                                        \"batch_size_param\": \"1\",\n",
    "                                                        \"process_count_param\": 1}\n",
    ")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will output information of the pipeline run, including the link to the details page of portal.\n",
    "pipeline_run_2"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait the run for completion and show output log to console\n",
    "pipeline_run_2.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Compute resources\n",
    "\n",
    "For re-occurring jobs, it may be wise to keep compute the compute resources and allow compute nodes to scale down to 0. However, since this is just a single-run job, we are free to release the allocated compute resources."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below and run if compute resources are no longer needed \n",
    "# compute_target.delete() "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Azure Machine Learning Pipelines for Batch Inference for CSV Files\n",
    "\n",
    "In this notebook, we will demonstrate how to make predictions on large quantities of data asynchronously using the ML pipelines with Azure Machine Learning. Batch inference (or batch scoring) provides cost-effective inference, with unparalleled throughput for asynchronous applications. Batch prediction pipelines can scale to perform inference on terabytes of production data. Batch prediction is optimized for high throughput, fire-and-forget predictions for a large collection of data.\n",
    "\n",
    "> **Tip**\n",
    "If your system requires low-latency processing (to process a single document or small set of documents quickly), use [real-time scoring](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-consume-web-service) instead of batch prediction.\n",
    "\n",
    "In this example we will take use a machine learning model already trained to predict different types of iris flowers and run that trained model on some of the data in a CSV file which has characteristics of different iris flowers.  However, the same example can be extended to manipulating data to any embarrassingly-parallel processing through a python script.\n",
    "\n",
    "The outline of this notebook is as follows:\n",
    "\n",
    "- Create a DataStore referencing the CSV files stored in a blob container.\n",
    "- Register the pretrained model into the model registry. \n",
    "- Use the registered model to do batch inference on the CSV files in the data blob container.\n",
    "\n",
    "## Prerequisites\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the configuration Notebook located at https://github.com/Azure/MachineLearningNotebooks first. This sets you up with a working config file that has information on your workspace, subscription id, etc. \n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to workspace\n",
    "Create a workspace object from the existing workspace. Workspace.from_config() reads the file config.json and loads the details into an object named ws."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Attach existing compute resource\n",
    "By using Azure Machine Learning Compute, a managed service, data scientists can train machine learning models on clusters of Azure virtual machines. Examples include VMs with GPU support. In this tutorial, you create Azure Machine Learning Compute as your training environment. The code below creates the compute clusters for you if they don't already exist in your workspace.\n",
    "\n",
    "**Creation of compute takes approximately 5 minutes. If the AmlCompute with that name is already in your workspace the code will skip the creation process.**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpu-cluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a datastore containing sample images\n",
    "The input dataset used for this notebook is CSV data which has attributes of different iris flowers. We have created a public blob container `sampledata` on an account named `pipelinedata`, containing iris data set. In the next step, we create a datastore with the name `iris_datastore`, which points to this container. In the call to `register_azure_blob_container` below, setting the `overwrite` flag to `True` overwrites any datastore that was created previously with that name. \n",
    "\n",
    "This step can be changed to point to your blob container by providing your own `datastore_name`, `container_name`, and `account_name`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.datastore import Datastore\n",
    "\n",
    "account_name = \"pipelinedata\"\n",
    "datastore_name=\"iris_datastore_data\"\n",
    "container_name=\"sampledata\"\n",
    "\n",
    "iris_data = Datastore.register_azure_blob_container(ws, \n",
    "                      datastore_name=datastore_name, \n",
    "                      container_name= container_name, \n",
    "                      account_name=account_name, \n",
    "                      overwrite=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a TabularDataset\n",
    "A [TabularDataSet](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabulardataset?view=azure-ml-py) references single or multiple files which contain data in a tabular structure (ie like CSV files) in your datastores or public urls. TabularDatasets provides you with the ability to download or mount the files to your compute. By creating a dataset, you create a reference to the data source location. If you applied any subsetting transformations to the dataset, they will be stored in the dataset as well. The data remains in its existing location, so no extra storage cost is incurred.\n",
    "You can use dataset objects as inputs. Register the datasets to the workspace if you want to reuse them later."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "iris_ds_name = 'iris_data'\n",
    "\n",
    "path_on_datastore = iris_data.path('iris/')\n",
    "input_iris_ds = Dataset.Tabular.from_delimited_files(path=path_on_datastore, validate=False)\n",
    "named_iris_ds = input_iris_ds.as_named_input(iris_ds_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediate/Output Data\n",
    "Intermediate data (or output of a Step) is represented by [PipelineData](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinedata?view=azure-ml-py) object. PipelineData can be produced by one step and consumed in another step by providing the PipelineData object as an output of one step and the input of one or more steps."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "output_folder = PipelineData(name='inferences', datastore=datastore)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering the Model with the Workspace\n",
    "Get the pretrained model from a publicly available Azure Blob container, then register it to use in your workspace"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_container_name=\"iris-model\"\n",
    "model_datastore_name=\"iris_model_datastore\"\n",
    "\n",
    "model_datastore = Datastore.register_azure_blob_container(ws, \n",
    "                      datastore_name=model_datastore_name, \n",
    "                      container_name= model_container_name, \n",
    "                      account_name=account_name, \n",
    "                      overwrite=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model_datastore.download('iris_model.pkl')\n",
    "\n",
    "# register downloaded model\n",
    "model = Model.register(model_path = \"iris_model.pkl/iris_model.pkl\",\n",
    "                       model_name = \"iris-prs\", # this is the name the model is registered as\n",
    "                       tags = {'pretrained': \"iris\"},\n",
    "                       workspace = ws)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using your model to make batch predictions\n",
    "To use the model to make batch predictions, you need an **entry script** and a list of **dependencies**:\n",
    "\n",
    "#### An entry script\n",
    "This script accepts requests, scores the requests by using the model, and returns the results.\n",
    "- __init()__ - Typically this function loads the model into a global object. This function is run only once at the start of batch processing per worker node/process. init method can make use of following environment variables (ParallelRunStep input):\n",
    "    1.\tAZUREML_BI_OUTPUT_PATH \u00e2\u20ac\u201c output folder path\n",
    "- __run(mini_batch)__ - The method to be parallelized. Each invocation will have one minibatch.<BR>\n",
    "__mini_batch__: Batch inference will invoke run method and pass either a list or Pandas DataFrame as an argument to the method. Each entry in min_batch will be - a filepath if input is a FileDataset, a Pandas DataFrame if input is a TabularDataset.<BR>\n",
    "__run__ method response: run() method should return a Pandas DataFrame or an array. For append_row output_action, these returned elements are appended into the common output file. For summary_only, the contents of the elements are ignored. For all output actions, each returned output element indicates one successful inference of input element in the input mini-batch.\n",
    "    User should make sure that enough data is included in inference result to map input to inference. Inference output will be written in output file and not guaranteed to be in order, user should use some key in the output to map it to input.\n",
    "    \n",
    "\n",
    "#### Dependencies\n",
    "Helper scripts or Python/Conda packages required to run the entry script.\n",
    "\n",
    "## Print inferencing script"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts_folder = \"Code\"\n",
    "script_file = \"iris_score.py\"\n",
    "\n",
    "# peek at contents\n",
    "with open(os.path.join(scripts_folder, script_file)) as inference_file:\n",
    "    print(inference_file.read())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and run the batch inference pipeline\n",
    "The data, models, and compute resource are now available. Let's put all these together in a pipeline."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Specify the environment to run the script\n",
    "Specify the conda dependencies for your script. This will allow us to install pip packages as well as configure the inference environment.\n",
    "* Always include **azureml-core** and **azureml-dataset-runtime\\[fuse\\]** in the pip package list to make ParallelRunStep run properly.\n",
    "* For TabularDataset, add **pandas** as `run(mini_batch)` uses `pandas.DataFrame` as mini_batch type.\n",
    "\n",
    "If you're using custom image (`batch_env.python.user_managed_dependencies = True`), you need to install the package to your image."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import CondaDependencies\n",
    "\n",
    "predict_conda_deps = CondaDependencies.create(pip_packages=[\"scikit-learn==0.20.3\",\n",
    "                                                            \"azureml-core\", \"azureml-dataset-runtime[pandas,fuse]\"])\n",
    "\n",
    "predict_env = Environment(name=\"predict_environment\")\n",
    "predict_env.python.conda_dependencies = predict_conda_deps\n",
    "predict_env.docker.enabled = True\n",
    "predict_env.spark.precache_packages = False"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create the configuration to wrap the inference script"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import ParallelRunStep, ParallelRunConfig\n",
    "\n",
    "# In a real-world scenario, you'll want to shape your process per node and nodes to fit your problem domain.\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory=scripts_folder,\n",
    "    entry_script=script_file,  # the user script to run against each input\n",
    "    mini_batch_size='1KB',\n",
    "    error_threshold=5,\n",
    "    output_action='append_row',\n",
    "    append_row_file_name=\"iris_outputs.txt\",\n",
    "    environment=predict_env,\n",
    "    compute_target=compute_target, \n",
    "    node_count=2,\n",
    "    run_invocation_timeout=600\n",
    ")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the pipeline step\n",
    "Create the pipeline step using the script, environment configuration, and parameters. Specify the compute target you already attached to your workspace as the target of execution of the script. We will use ParallelRunStep to create the pipeline step."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributed_csv_iris_step = ParallelRunStep(\n",
    "    name='example-iris',\n",
    "    inputs=[named_iris_ds],\n",
    "    output=output_folder,\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    arguments=['--model_name', 'iris-prs'],\n",
    "    allow_reuse=False\n",
    ")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline\n",
    "At this point you can run the pipeline and examine the output it produced. The Experiment object is used to track the run of the pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[distributed_csv_iris_step])\n",
    "\n",
    "pipeline_run = Experiment(ws, 'iris-prs').submit(pipeline)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View progress of Pipeline run\n",
    "\n",
    "The pipeline run status could be checked in Azure Machine Learning portal (https://ml.azure.com). The link to the pipeline run could be retrieved by inspecting the `pipeline_run` object."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will output information of the pipeline run, including the link to the details page of portal.\n",
    "pipeline_run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: View detailed logs (streaming) "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Wait the run for completion and show output log to console\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results\n",
    "In the iris_score.py file above you can see that the Result with the prediction of the iris variety gets returned and then appended to the original input of the row from the csv file. These results are written to the DataStore specified in the PipelineData object as the output data, which in this case is called *inferences*. This contains the outputs from  all of the worker nodes used in the compute cluster. You can download this data to view the results ... below just filters to a random 20 rows"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tempfile\n",
    "\n",
    "prediction_run = pipeline_run.find_step_run(distributed_csv_iris_step.name)[0]\n",
    "prediction_output = prediction_run.get_output_data(output_folder.name)\n",
    "\n",
    "target_dir = tempfile.mkdtemp()\n",
    "prediction_output.download(local_path=target_dir)\n",
    "result_file = os.path.join(target_dir, prediction_output.path_on_datastore, parallel_run_config.append_row_file_name)\n",
    "\n",
    "# cleanup output format\n",
    "df = pd.read_csv(result_file, delimiter=\" \", header=None)\n",
    "df.columns = [\"sepal.length\", \"sepal.width\", \"petal.length\", \"petal.width\", \"variety\"]\n",
    "print(\"Prediction has \", df.shape[0], \" rows\")\n",
    "\n",
    "random_subset = df.sample(n=20)\n",
    "random_subset.head(20)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup compute resources\n",
    "For re-occurring jobs, it may be wise to keep compute the compute resources and allow compute nodes to scale down to 0. However, since this is just a single run job, we are free to release the allocated compute resources."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below and run if compute resources are no longer needed \n",
    "# compute_target.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi Data Regression Model\n",
    "This is an [Azure Machine Learning Pipelines](https://aka.ms/aml-pipelines) version of two-part tutorial ([Part 1](https://docs.microsoft.com/en-us/azure/machine-learning/service/tutorial-data-prep), [Part 2](https://docs.microsoft.com/en-us/azure/machine-learning/service/tutorial-auto-train-models)) available for Azure Machine Learning.\n",
    "\n",
    "You can combine the two part tutorial into one using AzureML Pipelines as Pipelines provide a way to stitch together various steps involved (like data preparation and training in this case) in a machine learning workflow.\n",
    "\n",
    "In this notebook, you learn how to prepare data for regression modeling by using open source library [pandas](https://pandas.pydata.org/). You run various transformations to filter and combine two different NYC taxi datasets. Once you prepare the NYC taxi data for regression modeling, then you will use [AutoMLStep](https://docs.microsoft.com/python/api/azureml-train-automl-runtime/azureml.train.automl.runtime.automl_step.automlstep?view=azure-ml-py) available with [Azure Machine Learning Pipelines](https://aka.ms/aml-pipelines) to define your machine learning goals and constraints as well as to launch the automated machine learning process. The automated machine learning technique iterates over many combinations of algorithms and hyperparameters until it finds the best model based on your criterion.\n",
    "\n",
    "After you complete building the model, you can predict the cost of a taxi trip by training a model on data features. These features include the pickup day and time, the number of passengers, and the pickup location.\n",
    "\n",
    "## Prerequisite\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the configuration Notebook located at https://github.com/Azure/MachineLearningNotebooks first if you haven't. This sets you up with a working config file that has information on your workspace, subscription id, etc."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for regression modeling\n",
    "First, we will prepare data for regression modeling. We will leverage the convenience of Azure Open Datasets along with the power of Azure Machine Learning service to create a regression model to predict NYC taxi fare prices. Perform `pip install azureml-opendatasets` to get the open dataset package.  The Open Datasets package contains a class representing each data source (NycTlcGreen and NycTlcYellow) to easily filter date parameters before downloading.\n",
    "\n",
    "\n",
    "### Load data\n",
    "Begin by creating a dataframe to hold the taxi data. When working in a non-Spark environment, Open Datasets only allows downloading one month of data at a time with certain classes to avoid MemoryError with large datasets. To download a year of taxi data, iteratively fetch one month at a time, and before appending it to green_df_raw, randomly sample 500 records from each month to avoid bloating the dataframe. Then preview the data. To keep this process short, we are sampling data of only 1 month.\n",
    "\n",
    "Note: Open Datasets has mirroring classes for working in Spark environments where data size and memory aren't a concern."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.opendatasets import NycTlcGreen, NycTlcYellow\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "green_df_raw = pd.DataFrame([])\n",
    "start = datetime.strptime(\"1/1/2016\",\"%m/%d/%Y\")\n",
    "end = datetime.strptime(\"1/31/2016\",\"%m/%d/%Y\")\n",
    "\n",
    "number_of_months = 1\n",
    "sample_size = 5000\n",
    "\n",
    "for sample_month in range(number_of_months):\n",
    "    temp_df_green = NycTlcGreen(start + relativedelta(months=sample_month), end + relativedelta(months=sample_month)) \\\n",
    "        .to_pandas_dataframe()\n",
    "    green_df_raw = green_df_raw.append(temp_df_green.sample(sample_size))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_df_raw = pd.DataFrame([])\n",
    "start = datetime.strptime(\"1/1/2016\",\"%m/%d/%Y\")\n",
    "end = datetime.strptime(\"1/31/2016\",\"%m/%d/%Y\")\n",
    "\n",
    "sample_size = 500\n",
    "\n",
    "for sample_month in range(number_of_months):\n",
    "    temp_df_yellow = NycTlcYellow(start + relativedelta(months=sample_month), end + relativedelta(months=sample_month)) \\\n",
    "        .to_pandas_dataframe()\n",
    "    yellow_df_raw = yellow_df_raw.append(temp_df_yellow.sample(sample_size))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the data"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "display(green_df_raw.head(5))\n",
    "display(yellow_df_raw.head(5))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data locally and then upload to Azure Blob\n",
    "This is a one-time process to save the dave in the default datastore. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dataDir = \"data\"\n",
    "\n",
    "if not os.path.exists(dataDir):\n",
    "    os.mkdir(dataDir)\n",
    "\n",
    "greenDir = dataDir + \"/green\"\n",
    "yelloDir = dataDir + \"/yellow\"\n",
    "\n",
    "if not os.path.exists(greenDir):\n",
    "    os.mkdir(greenDir)\n",
    "    \n",
    "if not os.path.exists(yelloDir):\n",
    "    os.mkdir(yelloDir)\n",
    "    \n",
    "greenTaxiData = greenDir + \"/unprepared.parquet\"\n",
    "yellowTaxiData = yelloDir + \"/unprepared.parquet\"\n",
    "\n",
    "green_df_raw.to_csv(greenTaxiData, index=False)\n",
    "yellow_df_raw.to_csv(yellowTaxiData, index=False)\n",
    "\n",
    "print(\"Data written to local folder.\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(\"Workspace: \" + ws.name, \"Region: \" + ws.location, sep = '\\n')\n",
    "\n",
    "# Default datastore\n",
    "default_store = ws.get_default_datastore() \n",
    "\n",
    "default_store.upload_files([greenTaxiData], \n",
    "                           target_path = 'green', \n",
    "                           overwrite = True, \n",
    "                           show_progress = True)\n",
    "\n",
    "default_store.upload_files([yellowTaxiData], \n",
    "                           target_path = 'yellow', \n",
    "                           overwrite = True, \n",
    "                           show_progress = True)\n",
    "\n",
    "print(\"Upload calls completed.\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and register datasets\n",
    "\n",
    "By creating a dataset, you create a reference to the data source location. If you applied any subsetting transformations to the dataset, they will be stored in the dataset as well. You can learn more about the what subsetting capabilities are supported by referring to [our documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabular_dataset.tabulardataset?view=azure-ml-py#remarks). The data remains in its existing location, so no extra storage cost is incurred."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "green_taxi_data = Dataset.Tabular.from_delimited_files(default_store.path('green/unprepared.parquet'))\n",
    "yellow_taxi_data = Dataset.Tabular.from_delimited_files(default_store.path('yellow/unprepared.parquet'))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the taxi datasets with the workspace so that you can reuse them in other experiments or share with your colleagues who have access to your workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_taxi_data = green_taxi_data.register(ws, 'green_taxi_data')\n",
    "yellow_taxi_data = yellow_taxi_data.register(ws, 'yellow_taxi_data')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Compute\n",
    "#### Create new or use an existing compute"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "amlcompute_cluster_name = \"cpu-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    aml_compute = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=4)\n",
    "    aml_compute = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
    "\n",
    "aml_compute.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define RunConfig for the compute\n",
    "We will also use `pandas`, `scikit-learn` and `automl`, `pyarrow` for the pipeline steps. Defining the `runconfig` for that."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Create a new runconfig object\n",
    "aml_run_config = RunConfiguration()\n",
    "\n",
    "# Use the aml_compute you created above. \n",
    "aml_run_config.target = aml_compute\n",
    "\n",
    "# Enable Docker\n",
    "aml_run_config.environment.docker.enabled = True\n",
    "\n",
    "# Use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "aml_run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# Specify CondaDependencies obj, add necessary packages\n",
    "aml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
    "    conda_packages=['pandas','scikit-learn'], \n",
    "    pip_packages=['azureml-sdk[automl]', 'pyarrow'])\n",
    "\n",
    "print (\"Run configuration created.\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "Now we will prepare for regression modeling by using `pandas`. We run various transformations to filter and combine two different NYC taxi datasets.\n",
    "\n",
    "We achieve this by creating a separate step for each transformation as this allows us to reuse the steps and saves us from running all over again in case of any change. We will keep data preparation scripts in one subfolder and training scripts in another.\n",
    "\n",
    "> The best practice is to use separate folders for scripts and its dependent files for each step and specify that folder as the `source_directory` for the step. This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted). Since changes in any files in the `source_directory` would trigger a re-upload of the snapshot, this helps keep the reuse of the step when there are no changes in the `source_directory` of the step."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Useful Columns\n",
    "Here we are defining a set of \"useful\" columns for both Green and Yellow taxi data."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(green_df_raw.columns)\n",
    "display(yellow_df_raw.columns)\n",
    "\n",
    "# useful columns needed for the Azure Machine Learning NYC Taxi tutorial\n",
    "useful_columns = str([\"cost\", \"distance\", \"dropoff_datetime\", \"dropoff_latitude\", \n",
    "                      \"dropoff_longitude\", \"passengers\", \"pickup_datetime\", \n",
    "                      \"pickup_latitude\", \"pickup_longitude\", \"store_forward\", \"vendor\"]).replace(\",\", \";\")\n",
    "\n",
    "print(\"Useful columns defined.\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanse Green taxi data"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "# python scripts folder\n",
    "prepare_data_folder = './scripts/prepdata'\n",
    "\n",
    "# rename columns as per Azure Machine Learning NYC Taxi tutorial\n",
    "green_columns = str({ \n",
    "    \"vendorID\": \"vendor\",\n",
    "    \"lpepPickupDatetime\": \"pickup_datetime\",\n",
    "    \"lpepDropoffDatetime\": \"dropoff_datetime\",\n",
    "    \"storeAndFwdFlag\": \"store_forward\",\n",
    "    \"pickupLongitude\": \"pickup_longitude\",\n",
    "    \"pickupLatitude\": \"pickup_latitude\",\n",
    "    \"dropoffLongitude\": \"dropoff_longitude\",\n",
    "    \"dropoffLatitude\": \"dropoff_latitude\",\n",
    "    \"passengerCount\": \"passengers\",\n",
    "    \"fareAmount\": \"cost\",\n",
    "    \"tripDistance\": \"distance\"\n",
    "}).replace(\",\", \";\")\n",
    "\n",
    "# Define output after cleansing step\n",
    "cleansed_green_data = PipelineData(\"cleansed_green_data\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Cleanse script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# cleansing step creation\n",
    "# See the cleanse.py for details about input and output\n",
    "cleansingStepGreen = PythonScriptStep(\n",
    "    name=\"Cleanse Green Taxi Data\",\n",
    "    script_name=\"cleanse.py\", \n",
    "    arguments=[\"--useful_columns\", useful_columns,\n",
    "               \"--columns\", green_columns,\n",
    "               \"--output_cleanse\", cleansed_green_data],\n",
    "    inputs=[green_taxi_data.as_named_input('raw_data')],\n",
    "    outputs=[cleansed_green_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"cleansingStepGreen created.\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanse Yellow taxi data"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_columns = str({\n",
    "    \"vendorID\": \"vendor\",\n",
    "    \"tpepPickupDateTime\": \"pickup_datetime\",\n",
    "    \"tpepDropoffDateTime\": \"dropoff_datetime\",\n",
    "    \"storeAndFwdFlag\": \"store_forward\",\n",
    "    \"startLon\": \"pickup_longitude\",\n",
    "    \"startLat\": \"pickup_latitude\",\n",
    "    \"endLon\": \"dropoff_longitude\",\n",
    "    \"endLat\": \"dropoff_latitude\",\n",
    "    \"passengerCount\": \"passengers\",\n",
    "    \"fareAmount\": \"cost\",\n",
    "    \"tripDistance\": \"distance\"\n",
    "}).replace(\",\", \";\")\n",
    "\n",
    "# Define output after cleansing step\n",
    "cleansed_yellow_data = PipelineData(\"cleansed_yellow_data\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Cleanse script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# cleansing step creation\n",
    "# See the cleanse.py for details about input and output\n",
    "cleansingStepYellow = PythonScriptStep(\n",
    "    name=\"Cleanse Yellow Taxi Data\",\n",
    "    script_name=\"cleanse.py\", \n",
    "    arguments=[\"--useful_columns\", useful_columns,\n",
    "               \"--columns\", yellow_columns,\n",
    "               \"--output_cleanse\", cleansed_yellow_data],\n",
    "    inputs=[yellow_taxi_data.as_named_input('raw_data')],\n",
    "    outputs=[cleansed_yellow_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"cleansingStepYellow created.\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge cleansed Green and Yellow datasets\n",
    "We are creating a single data source by merging the cleansed versions of Green and Yellow taxi data."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output after merging step\n",
    "merged_data = PipelineData(\"merged_data\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Merge script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# merging step creation\n",
    "# See the merge.py for details about input and output\n",
    "mergingStep = PythonScriptStep(\n",
    "    name=\"Merge Taxi Data\",\n",
    "    script_name=\"merge.py\", \n",
    "    arguments=[\"--output_merge\", merged_data],\n",
    "    inputs=[cleansed_green_data.parse_parquet_files(),\n",
    "            cleansed_yellow_data.parse_parquet_files()],\n",
    "    outputs=[merged_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"mergingStep created.\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter data\n",
    "This step filters out coordinates for locations that are outside the city border. We use a TypeConverter object to change the latitude and longitude fields to decimal type. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output after merging step\n",
    "filtered_data = PipelineData(\"filtered_data\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Filter script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# filter step creation\n",
    "# See the filter.py for details about input and output\n",
    "filterStep = PythonScriptStep(\n",
    "    name=\"Filter Taxi Data\",\n",
    "    script_name=\"filter.py\", \n",
    "    arguments=[\"--output_filter\", filtered_data],\n",
    "    inputs=[merged_data.parse_parquet_files()],\n",
    "    outputs=[filtered_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig = aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"FilterStep created.\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize data\n",
    "In this step, we split the pickup and dropoff datetime values into the respective date and time columns and then we rename the columns to use meaningful names."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output after normalize step\n",
    "normalized_data = PipelineData(\"normalized_data\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Normalize script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# normalize step creation\n",
    "# See the normalize.py for details about input and output\n",
    "normalizeStep = PythonScriptStep(\n",
    "    name=\"Normalize Taxi Data\",\n",
    "    script_name=\"normalize.py\", \n",
    "    arguments=[\"--output_normalize\", normalized_data],\n",
    "    inputs=[filtered_data.parse_parquet_files()],\n",
    "    outputs=[normalized_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig = aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"normalizeStep created.\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform data\n",
    "Transform the normalized taxi data to final required format. This steps does the following:\n",
    "\n",
    "- Split the pickup and dropoff date further into the day of the week, day of the month, and month values. \n",
    "- To get the day of the week value, uses the derive_column_by_example() function. The function takes an array parameter of example objects that define the input data, and the preferred output. The function automatically determines the preferred transformation. For the pickup and dropoff time columns, split the time into the hour, minute, and second by using the split_column_by_example() function with no example parameter.\n",
    "- After new features are generated, use the drop_columns() function to delete the original fields as the newly generated features are preferred. \n",
    "- Rename the rest of the fields to use meaningful descriptions."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output after transform step\n",
    "transformed_data = PipelineData(\"transformed_data\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Transform script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# transform step creation\n",
    "# See the transform.py for details about input and output\n",
    "transformStep = PythonScriptStep(\n",
    "    name=\"Transform Taxi Data\",\n",
    "    script_name=\"transform.py\", \n",
    "    arguments=[\"--output_transform\", transformed_data],\n",
    "    inputs=[normalized_data.parse_parquet_files()],\n",
    "    outputs=[transformed_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig = aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"transformStep created.\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and test sets\n",
    "This function segregates the data into dataset for model training and dataset for testing."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_folder = './scripts/trainmodel'\n",
    "\n",
    "# train and test splits output\n",
    "output_split_train = PipelineData(\"output_split_train\", datastore=default_store).as_dataset()\n",
    "output_split_test = PipelineData(\"output_split_test\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Data spilt script is in {}.'.format(os.path.realpath(train_model_folder)))\n",
    "\n",
    "# test train split step creation\n",
    "# See the train_test_split.py for details about input and output\n",
    "testTrainSplitStep = PythonScriptStep(\n",
    "    name=\"Train Test Data Split\",\n",
    "    script_name=\"train_test_split.py\", \n",
    "    arguments=[\"--output_split_train\", output_split_train,\n",
    "               \"--output_split_test\", output_split_test],\n",
    "    inputs=[transformed_data.parse_parquet_files()],\n",
    "    outputs=[output_split_train, output_split_test],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig = aml_run_config,\n",
    "    source_directory=train_model_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"testTrainSplitStep created.\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use automated machine learning to build regression model\n",
    "Now we will use **automated machine learning** to build the regression model. We will use [AutoMLStep](https://docs.microsoft.com/python/api/azureml-train-automl-runtime/azureml.train.automl.runtime.automl_step.automlstep?view=azure-ml-py) in AML Pipelines for this part. Perform `pip install azureml-sdk[automl]`to get the automated machine learning package. These functions use various features from the data set and allow an automated model to build relationships between the features and the price of a taxi trip."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatically train a model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create experiment"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment = Experiment(ws, 'NYCTaxi_Tutorial_Pipelines')\n",
    "\n",
    "print(\"Experiment created\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define settings for autogeneration and tuning\n",
    "\n",
    "Here we define the experiment parameter and model settings for autogeneration and tuning. We can specify automl_settings as **kwargs as well.\n",
    "\n",
    "Use your defined training settings as a parameter to an `AutoMLConfig` object. Additionally, specify your training data and the type of model, which is `regression` in this case.\n",
    "\n",
    "Note: When using AmlCompute, we can't pass Numpy arrays directly to the fit method."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "# Change iterations to a reasonable number (50) to get better accuracy\n",
    "automl_settings = {\n",
    "    \"iteration_timeout_minutes\" : 10,\n",
    "    \"iterations\" : 2,\n",
    "    \"primary_metric\" : 'spearman_correlation',\n",
    "    \"n_cross_validations\": 5\n",
    "}\n",
    "\n",
    "training_dataset = output_split_train.parse_parquet_files().keep_columns(['pickup_weekday','pickup_hour', 'distance','passengers', 'vendor', 'cost'])\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'regression',\n",
    "                             debug_log = 'automated_ml_errors.log',\n",
    "                             path = train_model_folder,\n",
    "                             compute_target = aml_compute,\n",
    "                             featurization = 'auto',\n",
    "                             training_data = training_dataset,\n",
    "                             label_column_name = 'cost',\n",
    "                             **automl_settings)\n",
    "                             \n",
    "print(\"AutoML config created.\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define AutoMLStep"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import AutoMLStep\n",
    "\n",
    "trainWithAutomlStep = AutoMLStep(name='AutoML_Regression',\n",
    "                                 automl_config=automl_config,\n",
    "                                 allow_reuse=True)\n",
    "print(\"trainWithAutomlStep created.\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and run the pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "pipeline_steps = [trainWithAutomlStep]\n",
    "\n",
    "pipeline = Pipeline(workspace = ws, steps=pipeline_steps)\n",
    "print(\"Pipeline is built.\")\n",
    "\n",
    "pipeline_run = experiment.submit(pipeline, regenerate_outputs=False)\n",
    "\n",
    "print(\"Pipeline submitted for execution.\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(pipeline_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the results"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we proceed we need to wait for the run to complete.\n",
    "pipeline_run.wait_for_completion(show_output=False)\n",
    "\n",
    "# functions to download output to local and fetch as dataframe\n",
    "def get_download_path(download_path, output_name):\n",
    "    output_folder = os.listdir(download_path + '/azureml')[0]\n",
    "    path =  download_path + '/azureml/' + output_folder + '/' + output_name\n",
    "    return path\n",
    "\n",
    "def fetch_df(step, output_name):\n",
    "    output_data = step.get_output_data(output_name)    \n",
    "    download_path = './outputs/' + output_name\n",
    "    output_data.download(download_path, overwrite=True)\n",
    "    df_path = get_download_path(download_path, output_name) + '/processed.parquet'\n",
    "    return pd.read_parquet(df_path)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View cleansed taxi data"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_cleanse_step = pipeline_run.find_step_run(cleansingStepGreen.name)[0]\n",
    "yellow_cleanse_step = pipeline_run.find_step_run(cleansingStepYellow.name)[0]\n",
    "\n",
    "cleansed_green_df = fetch_df(green_cleanse_step, cleansed_green_data.name)\n",
    "cleansed_yellow_df = fetch_df(yellow_cleanse_step, cleansed_yellow_data.name)\n",
    "\n",
    "display(cleansed_green_df.head(5))\n",
    "display(cleansed_yellow_df.head(5))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the combined taxi data profile"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_step = pipeline_run.find_step_run(mergingStep.name)[0]\n",
    "combined_df = fetch_df(merge_step, merged_data.name)\n",
    "\n",
    "display(combined_df.describe())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the filtered taxi data profile"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_step = pipeline_run.find_step_run(filterStep.name)[0]\n",
    "filtered_df = fetch_df(filter_step, filtered_data.name)\n",
    "\n",
    "display(filtered_df.describe())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View normalized taxi data"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_step = pipeline_run.find_step_run(normalizeStep.name)[0]\n",
    "normalized_df = fetch_df(normalize_step, normalized_data.name)\n",
    "\n",
    "display(normalized_df.head(5))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View transformed taxi data"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_step = pipeline_run.find_step_run(transformStep.name)[0]\n",
    "transformed_df = fetch_df(transform_step, transformed_data.name)\n",
    "\n",
    "display(transformed_df.describe())\n",
    "display(transformed_df.head(5))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View training data used by AutoML"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_step = pipeline_run.find_step_run(testTrainSplitStep.name)[0]\n",
    "train_split = fetch_df(split_step, output_split_train.name)\n",
    "\n",
    "display(train_split.describe())\n",
    "display(train_split.head(5))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the details of the AutoML run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.automl.run import AutoMLRun\n",
    "#from azureml.widgets import RunDetails\n",
    "\n",
    "# workaround to get the automl run as its the last step in the pipeline \n",
    "# and get_steps() returns the steps from latest to first\n",
    "\n",
    "for step in pipeline_run.get_steps():\n",
    "    automl_step_run_id = step.id\n",
    "    print(step.name)\n",
    "    print(automl_step_run_id)\n",
    "    break\n",
    "\n",
    "automl_run = AutoMLRun(experiment = experiment, run_id=automl_step_run_id)\n",
    "#RunDetails(automl_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve all Child runs\n",
    "\n",
    "We use SDK methods to fetch all the child runs and see individual metrics that we log."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children = list(automl_run.get_children())\n",
    "metricslist = {}\n",
    "for run in children:\n",
    "    properties = run.get_properties()\n",
    "    metrics = {k: v for k, v in run.get_metrics().items() if isinstance(v, float)}\n",
    "    metricslist[int(properties['iteration'])] = metrics\n",
    "\n",
    "rundata = pd.DataFrame(metricslist).sort_index(1)\n",
    "rundata"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retreive the best model\n",
    "\n",
    "Uncomment the below cell to retrieve the best model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_run, fitted_model = automl_run.get_output()\n",
    "# print(best_run)\n",
    "# print(fitted_model)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get test data\n",
    "\n",
    "Uncomment the below cell to get test data"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_step = pipeline_run.find_step_run(testTrainSplitStep.name)[0]\n",
    "\n",
    "# x_test = fetch_df(split_step, output_split_test.name)[['distance','passengers', 'vendor','pickup_weekday','pickup_hour']]\n",
    "# y_test = fetch_df(split_step, output_split_test.name)[['cost']]\n",
    "\n",
    "# display(x_test.head(5))\n",
    "# display(y_test.head(5))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the best fitted model\n",
    "\n",
    "Uncomment the below cell to test the best fitted model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_predict = fitted_model.predict(x_test)\n",
    "\n",
    "# y_actual =  y_test.values.tolist()\n",
    "\n",
    "# display(pd.DataFrame({'Actual':y_actual, 'Predicted':y_predict}).head(5))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig = plt.figure(figsize=(14, 10))\n",
    "# ax1 = fig.add_subplot(111)\n",
    "\n",
    "# distance_vals = [x[0] for x in x_test.values]\n",
    "\n",
    "# ax1.scatter(distance_vals[:100], y_predict[:100], s=18, c='b', marker=\"s\", label='Predicted')\n",
    "# ax1.scatter(distance_vals[:100], y_actual[:100], s=18, c='r', marker=\"o\", label='Actual')\n",
    "\n",
    "# ax1.set_xlabel('distance (mi)')\n",
    "# ax1.set_title('Predicted and Actual Cost/Distance')\n",
    "# ax1.set_ylabel('Cost ($)')\n",
    "\n",
    "# plt.legend(loc='upper left', prop={'size': 12})\n",
    "# plt.rcParams.update({'font.size': 14})\n",
    "# plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Machine Learning Pipeline with HyperDriveStep\n",
    "\n",
    "\n",
    "This notebook is used to demonstrate the use of HyperDriveStep in AML Pipeline."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Azure Machine Learning Basics\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the [configuration Notebook](https://aka.ms/pl-config) first if you haven't. This sets you up with a working config file that has information on your workspace, subscription id, etc. \n",
    "\n",
    "## Azure Machine Learning and Pipeline SDK-specific imports"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Datastore, Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.pipeline.steps import HyperDriveStep, HyperDriveStepRun, PythonScriptStep\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, TrainingOutput\n",
    "from azureml.train.dnn import TensorFlow\n",
    "# from azureml.train.hyperdrive import *\n",
    "from azureml.train.hyperdrive import RandomParameterSampling, BanditPolicy, HyperDriveConfig, PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive import choice, loguniform\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import urllib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration. If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure the config file is present at .\\config.json"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Azure ML experiment\n",
    "Let's create an experiment named \"tf-mnist\" and a folder to hold the training scripts. \n",
    "\n",
    "> The best practice is to use separate folders for scripts and its dependent files for each step. This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted). Since changes in any files in the `source_directory` would trigger a re-upload of the snapshot, this helps keep the reuse of the step when there are no changes in the `source_directory` of the step. \n",
    "\n",
    "> The script runs will be recorded under the experiment in Azure."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_folder = './tf-mnist'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "\n",
    "exp = Experiment(workspace=ws, name='Hyperdrive_sample')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download MNIST dataset\n",
    "In order to train on the MNIST dataset we will first need to download it from Yan LeCun's web site directly and save them in a `data` folder locally."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./data/mnist', exist_ok=True)\n",
    "\n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz', filename = './data/mnist/train-images.gz')\n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz', filename = './data/mnist/train-labels.gz')\n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz', filename = './data/mnist/test-images.gz')\n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz', filename = './data/mnist/test-labels.gz')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show some sample images\n",
    "Let's load the downloaded compressed file into numpy arrays using some utility functions included in the `utils.py` library file from the current folder. Then we use `matplotlib` to plot 30 random images from the dataset along with their labels."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data\n",
    "\n",
    "# note we also shrink the intensity values (X) from 0-255 to 0-1. This helps the neural network converge faster.\n",
    "X_train = load_data('./data/mnist/train-images.gz', False) / 255.0\n",
    "y_train = load_data('./data/mnist/train-labels.gz', True).reshape(-1)\n",
    "\n",
    "X_test = load_data('./data/mnist/test-images.gz', False) / 255.0\n",
    "y_test = load_data('./data/mnist/test-labels.gz', True).reshape(-1)\n",
    "\n",
    "count = 0\n",
    "sample_size = 30\n",
    "plt.figure(figsize = (16, 6))\n",
    "for i in np.random.permutation(X_train.shape[0])[:sample_size]:\n",
    "    count = count + 1\n",
    "    plt.subplot(1, sample_size, count)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    plt.text(x = 10, y = -10, s = y_train[i], fontsize = 18)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap = plt.cm.Greys)\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload MNIST dataset to blob datastore \n",
    "A [datastore](https://docs.microsoft.com/azure/machine-learning/service/how-to-access-data) is a place where data can be stored that is then made accessible to a Run either by means of mounting or copying the data to the compute target. In the next step, we will use Azure Blob Storage and upload the training and test set into the Azure Blob datastore, which we will then later be mount on a Batch AI cluster for training."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = ws.get_default_datastore()\n",
    "datastore.upload(src_dir='./data/mnist', target_path='mnist', overwrite=True, show_progress=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure Machine Learning datasets\n",
    "By creating a dataset, you create a reference to the data source location. If you applied any subsetting transformations to the dataset, they will be stored in the dataset as well. The data remains in its existing location, so no extra storage cost is incurred."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.File.from_files(datastore.path('mnist'))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve or create a Azure Machine Learning compute\n",
    "Azure Machine Learning Compute is a service for provisioning and managing clusters of Azure virtual machines for running machine learning workloads. Let's create a new Azure Machine Learning Compute in the current workspace, if it doesn't already exist. We will then run the training script on this compute target.\n",
    "\n",
    "If we could not find the compute with the given name in the previous cell, then we will create a new compute here. This process is broken down into the following steps:\n",
    "\n",
    "1. Create the configuration\n",
    "2. Create the Azure Machine Learning compute\n",
    "\n",
    "**This process will take a few minutes and is providing only sparse output in the process. Please make sure to wait until the call returns before moving to the next cell.**\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_name = \"amlcomp\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target {}.'.format(cluster_name))\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_NC6\",\n",
    "                                                               max_nodes=4)\n",
    "\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "compute_target.wait_for_completion(show_output=True, timeout_in_minutes=20)\n",
    "\n",
    "print(\"Azure Machine Learning Compute attached\")\n",
    "\n",
    "cpu_cluster_name = \"cpu-cluster\"\n",
    "\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print(\"Found existing cpu-cluster\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new cpu-cluster\")\n",
    "    \n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_D2_V2\",\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=4)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "    \n",
    "cpu_cluster.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the training files into the script folder\n",
    "The TensorFlow training script is already created for you. You can simply copy it into the script folder, together with the utility library used to load compressed data file into numpy array."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training logic is in the tf_mnist.py file.\n",
    "shutil.copy('./tf_mnist.py', script_folder)\n",
    "\n",
    "# the utils.py just helps loading data from the downloaded MNIST dataset into numpy arrays.\n",
    "shutil.copy('./utils.py', script_folder)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TensorFlow estimator\n",
    "Next, we construct an [TensorFlow](https://docs.microsoft.com/python/api/azureml-train-core/azureml.train.dnn.tensorflow?view=azure-ml-py) estimator object.\n",
    "The TensorFlow estimator is providing a simple way of launching a TensorFlow training job on a compute target. It will automatically provide a docker image that has TensorFlow installed -- if additional pip or conda packages are required, their names can be passed in via the `pip_packages` and `conda_packages` arguments and they will be included in the resulting docker.\n",
    "\n",
    "The TensorFlow estimator also takes a `framework_version` parameter -- if no version is provided, the estimator will default to the latest version supported by AzureML. Use `TensorFlow.get_supported_versions()` to get a list of all versions supported by your current SDK version or see the [SDK documentation](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.dnn?view=azure-ml-py) for the versions supported in the most current release.\n",
    "\n",
    "The TensorFlow estimator also takes a `framework_version` parameter -- if no version is provided, the estimator will default to the latest version supported by AzureML. Use `TensorFlow.get_supported_versions()` to get a list of all versions supported by your current SDK version or see the [SDK documentation](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.dnn?view=azure-ml-py) for the versions supported in the most current release."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = TensorFlow(source_directory=script_folder,                 \n",
    "                 compute_target=compute_target,\n",
    "                 entry_script='tf_mnist.py', \n",
    "                 use_gpu=True,\n",
    "                 framework_version='2.0',\n",
    "                 pip_packages=['azureml-dataset-runtime[pandas,fuse]'])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intelligent hyperparameter tuning\n",
    "Now let's try hyperparameter tuning by launching multiple runs on the cluster. First let's define the parameter space using random sampling.\n",
    "\n",
    "In this example we will use random sampling to try different configuration sets of hyperparameters to maximize our primary metric, the best validation accuracy (`validation_acc`)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = RandomParameterSampling(\n",
    "    {\n",
    "        '--batch-size': choice(25, 50, 100),\n",
    "        '--first-layer-neurons': choice(10, 50, 200, 300, 500),\n",
    "        '--second-layer-neurons': choice(10, 50, 200, 500),\n",
    "        '--learning-rate': loguniform(-6, -1)\n",
    "    }\n",
    ")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define an early termnination policy. The `BanditPolicy` basically states to check the job every 2 iterations. If the primary metric (defined later) falls outside of the top 10% range, Azure ML terminate the job. This saves us from continuing to explore hyperparameters that don't show promise of helping reach our target metric.\n",
    "\n",
    "Refer [here](https://docs.microsoft.com/azure/machine-learning/service/how-to-tune-hyperparameters#specify-an-early-termination-policy) for more information on the BanditPolicy and other policies available."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_termination_policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to configure a run configuration object, and specify the primary metric `validation_acc` that's recorded in your training runs. If you go back to visit the training script, you will notice that this value is being logged after every epoch (a full batch set). We also want to tell the service that we are looking to maximizing this value. We also set the number of samples to 20, and maximal concurrent job to 4, which is the same as the number of nodes in our computer cluster."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hyperdriveconfig-remarks-sample"
    ]
   },
   "outputs": [],
   "source": [
    "hd_config = HyperDriveConfig(estimator=est, \n",
    "                             hyperparameter_sampling=ps,\n",
    "                             policy=early_termination_policy,\n",
    "                             primary_metric_name='validation_acc', \n",
    "                             primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "                             max_total_runs=4,\n",
    "                             max_concurrent_runs=4)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add HyperDrive as a step of pipeline\n",
    "\n",
    "### Setup an input for the hypderdrive step\n",
    "You can mount dataset to remote compute."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = dataset.as_mount()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperDriveStep\n",
    "HyperDriveStep can be used to run HyperDrive job as a step in pipeline.\n",
    "- **name:** Name of the step\n",
    "- **hyperdrive_config:** A HyperDriveConfig that defines the configuration for this HyperDrive run\n",
    "- **estimator_entry_script_arguments:** List of command-line arguments for estimator entry script\n",
    "- **inputs:** List of input port bindings\n",
    "- **outputs:** List of output port bindings\n",
    "- **metrics_output:** Optional value specifying the location to store HyperDrive run metrics as a JSON file\n",
    "- **allow_reuse:** whether to allow reuse\n",
    "- **version:** version\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hyperdrivestep-remarks-sample"
    ]
   },
   "outputs": [],
   "source": [
    "metrics_output_name = 'metrics_output'\n",
    "metrics_data = PipelineData(name='metrics_data',\n",
    "                            datastore=datastore,\n",
    "                            pipeline_output_name=metrics_output_name,\n",
    "                            training_output=TrainingOutput(\"Metrics\"))\n",
    "\n",
    "model_output_name = 'model_output'\n",
    "saved_model = PipelineData(name='saved_model',\n",
    "                            datastore=datastore,\n",
    "                            pipeline_output_name=model_output_name,\n",
    "                            training_output=TrainingOutput(\"Model\",\n",
    "                                                           model_file=\"outputs/model/saved_model.pb\"))\n",
    "\n",
    "hd_step_name='hd_step01'\n",
    "hd_step = HyperDriveStep(\n",
    "    name=hd_step_name,\n",
    "    hyperdrive_config=hd_config,\n",
    "    estimator_entry_script_arguments=['--data-folder', data_folder],\n",
    "    inputs=[data_folder],\n",
    "    outputs=[metrics_data, saved_model])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and register best model\n",
    "When all the jobs finish, we can choose to register the model that has the highest accuracy through an additional PythonScriptStep.\n",
    "\n",
    "Through this additional register_model_step, we register the chosen files as a model named `tf-dnn-mnist` under the workspace for deployment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda_dep = CondaDependencies()\n",
    "conda_dep.add_pip_package(\"azureml-sdk\")\n",
    "\n",
    "rcfg = RunConfiguration(conda_dependencies=conda_dep)\n",
    "\n",
    "register_model_step = PythonScriptStep(script_name='register_model.py',\n",
    "                                       name=\"register_model_step01\",\n",
    "                                       inputs=[saved_model],\n",
    "                                       compute_target=cpu_cluster,\n",
    "                                       arguments=[\"--saved-model\", saved_model],\n",
    "                                       allow_reuse=True,\n",
    "                                       runconfig=rcfg)\n",
    "\n",
    "register_model_step.run_after(hd_step)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[hd_step, register_model_step])\n",
    "pipeline_run = exp.submit(pipeline)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor using widget"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for the completion of this Pipeline run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the metrics\n",
    "Outputs of above run can be used as inputs of other steps in pipeline. In this tutorial, we will show the result metrics."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_output = pipeline_run.get_pipeline_output(metrics_output_name)\n",
    "num_file_downloaded = metrics_output.download('.', show_progress=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "with open(metrics_output._path_on_datastore) as f:  \n",
    "    metrics_output_result = f.read()\n",
    "    \n",
    "deserialized_metrics_output = json.loads(metrics_output_result)\n",
    "df = pd.DataFrame(deserialized_metrics_output)\n",
    "df"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model deployment, please refer to [Training, hyperparameter tune, and deploy with TensorFlow](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Machine Learning Pipelines with Data Dependency\n",
    "In this notebook, we will see how we can build a pipeline with implicit data dependency."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Azure Machine Learning Basics\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the [configuration Notebook](https://aka.ms/pl-config) first if you haven't. This sets you up with a working config file that has information on your workspace, subscription id, etc. \n",
    "\n",
    "### Azure Machine Learning and Pipeline SDK-specific Imports"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Datastore\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n",
    "\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "print(\"Pipeline SDK-specific imports completed\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Workspace\n",
    "\n",
    "Initialize a [workspace](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace(class%29) object from persisted configuration."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create workspace"
    ]
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n",
    "\n",
    "# Default datastore (Azure blob storage)\n",
    "# def_blob_store = ws.get_default_datastore()\n",
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "print(\"Blobstore's name: {}\".format(def_blob_store.name))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Directory\n",
    "The best practice is to use separate folders for scripts and its dependent files for each step and specify that folder as the `source_directory` for the step. This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted). Since changes in any files in the `source_directory` would trigger a re-upload of the snapshot, this helps keep the reuse of the step when there are no changes in the `source_directory` of the step."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source directory\n",
    "source_directory = 'data_dependency_run_train'\n",
    "    \n",
    "print('Sample scripts will be created in {} directory.'.format(source_directory))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required data and script files for the the tutorial\n",
    "Sample files required to finish this tutorial are already copied to the project folder specified above. Even though the .py provided in the samples don't have much \"ML work,\" as a data scientist, you will work on this extensively as part of your work. To complete this tutorial, the contents of these files are not very important. The one-line files are for demostration purpose only."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Targets\n",
    "See the list of Compute Targets on the workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cts = ws.compute_targets\n",
    "for ct in cts:\n",
    "    print(ct)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve or create an Aml compute\n",
    "Azure Machine Learning Compute is a service for provisioning and managing clusters of Azure virtual machines for running machine learning workloads. Let's get the default Aml Compute in the current workspace. We will then run the training script on this compute target."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "aml_compute_target = \"cpu-cluster\"\n",
    "try:\n",
    "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
    "    print(\"found existing compute target.\")\n",
    "except ComputeTargetException:\n",
    "    print(\"creating new compute target\")\n",
    "    \n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\",\n",
    "                                                                min_nodes = 1, \n",
    "                                                                max_nodes = 4)    \n",
    "    aml_compute = ComputeTarget.create(ws, aml_compute_target, provisioning_config)\n",
    "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "print(\"Aml Compute attached\")\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a more detailed view of current Azure Machine Learning Compute status, use get_status()\n",
    "# example: un-comment the following line.\n",
    "# print(aml_compute.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wait for this call to finish before proceeding (you will see the asterisk turning to a number).**\n",
    "\n",
    "Now that you have created the compute target, let's see what the workspace's compute_targets() function returns. You should now see one entry named 'amlcompute' of type AmlCompute."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Pipeline Steps with Inputs and Outputs\n",
    "As mentioned earlier, a step in the pipeline can take data as input. This data can be a data source that lives in one of the accessible data locations, or intermediate data produced by a previous step in the pipeline.\n",
    "\n",
    "### Datasources\n",
    "Datasource is represented by **[DataReference](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.data_reference.datareference?view=azure-ml-py)** object and points to data that lives in or is accessible from Datastore. DataReference could be a pointer to a file or a directory."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference the data uploaded to blob storage using DataReference\n",
    "# Assign the datasource to blob_input_data variable\n",
    "\n",
    "# DataReference(datastore, \n",
    "#               data_reference_name=None, \n",
    "#               path_on_datastore=None, \n",
    "#               mode='mount', \n",
    "#               path_on_compute=None, \n",
    "#               overwrite=False)\n",
    "\n",
    "blob_input_data = DataReference(\n",
    "    datastore=def_blob_store,\n",
    "    data_reference_name=\"test_data\",\n",
    "    path_on_datastore=\"20newsgroups/20news.pkl\")\n",
    "print(\"DataReference object created\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediate/Output Data\n",
    "Intermediate data (or output of a Step) is represented by **[PipelineData](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinedata?view=azure-ml-py)** object. PipelineData can be produced by one step and consumed in another step by providing the PipelineData object as an output of one step and the input of one or more steps.\n",
    "\n",
    "#### Constructing PipelineData\n",
    "- **name:** [*Required*] Name of the data item within the pipeline graph\n",
    "- **datastore_name:** Name of the Datastore to write this output to\n",
    "- **output_name:** Name of the output\n",
    "- **output_mode:** Specifies \"upload\" or \"mount\" modes for producing output (default: mount)\n",
    "- **output_path_on_compute:** For \"upload\" mode, the path to which the module writes this output during execution\n",
    "- **output_overwrite:** Flag to overwrite pre-existing data"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define intermediate data using PipelineData\n",
    "# Syntax\n",
    "\n",
    "# PipelineData(name, \n",
    "#              datastore=None, \n",
    "#              output_name=None, \n",
    "#              output_mode='mount', \n",
    "#              output_path_on_compute=None, \n",
    "#              output_overwrite=None, \n",
    "#              data_type=None, \n",
    "#              is_directory=None)\n",
    "\n",
    "# Naming the intermediate data as processed_data1 and assigning it to the variable processed_data1.\n",
    "processed_data1 = PipelineData(\"processed_data1\",datastore=def_blob_store)\n",
    "print(\"PipelineData object created\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines steps using datasources and intermediate data\n",
    "Machine learning pipelines can have many steps and these steps could use or reuse datasources and intermediate data. Here's how we construct such a pipeline:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a Step that consumes a datasource and produces intermediate data.\n",
    "In this step, we define a step that consumes a datasource and produces intermediate data.\n",
    "\n",
    "**Open `train.py` in the local machine and examine the arguments, inputs, and outputs for the script. That will give you a good sense of why the script argument names used below are important.** "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify conda dependencies and a base docker image through a RunConfiguration\n",
    "\n",
    "This step uses a docker image and scikit-learn, use a [**RunConfiguration**](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.runconfiguration?view=azure-ml-py) to specify these requirements and use when creating the PythonScriptStep. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "\n",
    "# create a new runconfig object\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "# enable Docker \n",
    "run_config.environment.docker.enabled = True\n",
    "\n",
    "# set Docker base image to the default CPU-based image\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "\n",
    "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# specify CondaDependencies obj\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['scikit-learn'])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step4 consumes the datasource (Datareference) in the previous step\n",
    "# and produces processed_data1\n",
    "trainStep = PythonScriptStep(\n",
    "    script_name=\"train.py\", \n",
    "    arguments=[\"--input_data\", blob_input_data, \"--output_train\", processed_data1],\n",
    "    inputs=[blob_input_data],\n",
    "    outputs=[processed_data1],\n",
    "    compute_target=aml_compute, \n",
    "    source_directory=source_directory,\n",
    "    runconfig=run_config\n",
    ")\n",
    "print(\"trainStep created\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a Step that consumes intermediate data and produces intermediate data\n",
    "In this step, we define a step that consumes an intermediate data and produces intermediate data.\n",
    "\n",
    "**Open `extract.py` in the local machine and examine the arguments, inputs, and outputs for the script. That will give you a good sense of why the script argument names used below are important.** "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step5 to use the intermediate data produced by step4\n",
    "# This step also produces an output processed_data2\n",
    "processed_data2 = PipelineData(\"processed_data2\", datastore=def_blob_store)\n",
    "source_directory = \"data_dependency_run_extract\"\n",
    "\n",
    "extractStep = PythonScriptStep(\n",
    "    script_name=\"extract.py\",\n",
    "    arguments=[\"--input_extract\", processed_data1, \"--output_extract\", processed_data2],\n",
    "    inputs=[processed_data1],\n",
    "    outputs=[processed_data2],\n",
    "    compute_target=aml_compute, \n",
    "    source_directory=source_directory)\n",
    "print(\"extractStep created\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a Step that consumes intermediate data and existing data and produces intermediate data\n",
    "In this step, we define a step that consumes multiple data types and produces intermediate data.\n",
    "\n",
    "This step uses the output generated from the previous step as well as existing data on a DataStore. The location of the existing data is specified using a [**PipelineParameter**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelineparameter?view=azure-ml-py) and a [**DataPath**](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.datapath.datapath?view=azure-ml-py). Using a PipelineParameter enables easy modification of the data location when the Pipeline is published and resubmitted.\n",
    "\n",
    "**Open `compare.py` in the local machine and examine the arguments, inputs, and outputs for the script. That will give you a good sense of why the script argument names used below are important.**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference the data uploaded to blob storage using a PipelineParameter and a DataPath\n",
    "from azureml.pipeline.core import PipelineParameter\n",
    "from azureml.data.datapath import DataPath, DataPathComputeBinding\n",
    "\n",
    "datapath = DataPath(datastore=def_blob_store, path_on_datastore='20newsgroups/20news.pkl')\n",
    "datapath_param = PipelineParameter(name=\"compare_data\", default_value=datapath)\n",
    "data_parameter1 = (datapath_param, DataPathComputeBinding(mode='mount'))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define the compare step which takes two inputs and produces an output\n",
    "processed_data3 = PipelineData(\"processed_data3\", datastore=def_blob_store)\n",
    "source_directory = \"data_dependency_run_compare\"\n",
    "\n",
    "compareStep = PythonScriptStep(\n",
    "    script_name=\"compare.py\",\n",
    "    arguments=[\"--compare_data1\", data_parameter1, \"--compare_data2\", processed_data2, \"--output_compare\", processed_data3],\n",
    "    inputs=[data_parameter1, processed_data2],\n",
    "    outputs=[processed_data3],    \n",
    "    compute_target=aml_compute, \n",
    "    source_directory=source_directory)\n",
    "print(\"compareStep created\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1 = Pipeline(workspace=ws, steps=[compareStep])\n",
    "print (\"Pipeline is built\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run1 = Experiment(ws, 'Data_dependency').submit(pipeline1)\n",
    "print(\"Pipeline is submitted for execution\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(pipeline_run1).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for pipeline run to complete"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run1.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See Outputs\n",
    "\n",
    "See where outputs of each pipeline step are located on your datastore.\n",
    "\n",
    "***Wait for pipeline run to complete, to make sure all the outputs are ready***"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Steps\n",
    "for step in pipeline_run1.get_steps():\n",
    "    print(\"Outputs of step \" + step.name)\n",
    "    \n",
    "    # Get a dictionary of StepRunOutputs with the output name as the key \n",
    "    output_dict = step.get_outputs()\n",
    "    \n",
    "    for name, output in output_dict.items():\n",
    "        \n",
    "        output_reference = output.get_port_data_reference() # Get output port data reference\n",
    "        print(\"\\tname: \" + name)\n",
    "        print(\"\\tdatastore: \" + output_reference.datastore_name)\n",
    "        print(\"\\tpath on datastore: \" + output_reference.path_on_datastore)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Outputs\n",
    "\n",
    "We can download the output of any step to our local machine using the SDK."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the step runs by name 'train.py'\n",
    "train_step = pipeline_run1.find_step_run('train.py')\n",
    "\n",
    "if train_step:\n",
    "    train_step_obj = train_step[0] # since we have only one step by name 'train.py'\n",
    "    train_step_obj.get_output_data('processed_data1').download(\"./outputs\") # download the output to current directory"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: Publishing the Pipeline and calling it from the REST endpoint\n",
    "See this [notebook](https://aka.ms/pl-pub-rep) to understand how the pipeline is published and you can call the REST endpoint to run the pipeline."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Publish a Pipeline and Invoke the REST endpoint\n",
    "In this notebook, we will see how we can publish a pipeline and then invoke the REST endpoint."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Azure Machine Learning Basics\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the [configuration Notebook](https://aka.ms/pl-config) first if you haven't. This sets you up with a working config file that has information on your workspace, subscription id, etc. \n",
    "\n",
    "### Initialization Steps"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Datastore, Experiment, Dataset\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n",
    "\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core.graph import PipelineParameter\n",
    "\n",
    "print(\"Pipeline SDK-specific imports completed\")\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n",
    "\n",
    "# Default datastore (Azure blob storage)\n",
    "# def_blob_store = ws.get_default_datastore()\n",
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "print(\"Blobstore's name: {}\".format(def_blob_store.name))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Targets\n",
    "#### Retrieve an already attached  Azure Machine Learning Compute"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "aml_compute_target = \"cpu-cluster\"\n",
    "try:\n",
    "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
    "    print(\"found existing compute target.\")\n",
    "except ComputeTargetException:\n",
    "    print(\"creating new compute target\")\n",
    "    \n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\",\n",
    "                                                                min_nodes = 1, \n",
    "                                                                max_nodes = 4)    \n",
    "    aml_compute = ComputeTarget.create(ws, aml_compute_target, provisioning_config)\n",
    "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a more detailed view of current Azure Machine Learning Compute status, use get_status()\n",
    "# example: un-comment the following line.\n",
    "# print(aml_compute.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Pipeline Steps with Inputs and Outputs\n",
    "A step in the pipeline can take [dataset](https://docs.microsoft.com/python/api/azureml-core/azureml.data.filedataset?view=azure-ml-py) as input. This dataset can be a data source that lives in one of the accessible data locations, or intermediate data produced by a previous step in the pipeline."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading data to the datastore\n",
    "data_path = def_blob_store.upload_files([\"./20news.pkl\"], target_path=\"20newsgroups\", overwrite=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference the data uploaded to blob storage using file dataset\n",
    "# Assign the datasource to blob_input_data variable\n",
    "blob_input_data = Dataset.File.from_files(data_path).as_named_input(\"test_data\")\n",
    "print(\"Dataset created\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define intermediate data using OutputFileDatasetConfig\n",
    "processed_data1 = OutputFileDatasetConfig(name=\"processed_data1\")\n",
    "print(\"Output dataset object created\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a Step that consumes a dataset and produces intermediate data.\n",
    "In this step, we define a step that consumes a dataset and produces intermediate data.\n",
    "\n",
    "**Open `train.py` in the local machine and examine the arguments, inputs, and outputs for the script. That will give you a good sense of why the script argument names used below are important.** \n",
    "\n",
    "The best practice is to use separate folders for scripts and its dependent files for each step and specify that folder as the `source_directory` for the step. This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted). Since changes in any files in the `source_directory` would trigger a re-upload of the snapshot, this helps keep the reuse of the step when there are no changes in the `source_directory` of the step."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainStep consumes the datasource (Datareference) in the previous step\n",
    "# and produces processed_data1\n",
    "\n",
    "source_directory = \"publish_run_train\"\n",
    "\n",
    "trainStep = PythonScriptStep(\n",
    "    script_name=\"train.py\", \n",
    "        arguments=[\"--input_data\", blob_input_data.as_mount(), \"--output_train\", processed_data1],\n",
    "    compute_target=aml_compute, \n",
    "    source_directory=source_directory\n",
    ")\n",
    "print(\"trainStep created\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a Step that consumes intermediate data and produces intermediate data\n",
    "In this step, we define a step that consumes an intermediate data and produces intermediate data.\n",
    "\n",
    "**Open `extract.py` in the local machine and examine the arguments, inputs, and outputs for the script. That will give you a good sense of why the script argument names used below are important.** "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extractStep to use the intermediate data produced by trainStep\n",
    "# This step also produces an output processed_data2\n",
    "processed_data2 = OutputFileDatasetConfig(name=\"processed_data2\")\n",
    "source_directory = \"publish_run_extract\"\n",
    "\n",
    "extractStep = PythonScriptStep(\n",
    "    script_name=\"extract.py\",\n",
    "    arguments=[\"--input_extract\", processed_data1.as_input(), \"--output_extract\", processed_data2],\n",
    "    compute_target=aml_compute, \n",
    "    source_directory=source_directory)\n",
    "print(\"extractStep created\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a Step that consumes multiple intermediate data and produces intermediate data\n",
    "In this step, we define a step that consumes multiple intermediate data and produces intermediate data."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PipelineParameter"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step also has a [PipelineParameter](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.graph.pipelineparameter?view=azure-ml-py) argument that help with calling the REST endpoint of the published pipeline."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this later in publishing pipeline\n",
    "pipeline_param = PipelineParameter(name=\"pipeline_arg\", default_value=10)\n",
    "print(\"pipeline parameter created\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open `compare.py` in the local machine and examine the arguments, inputs, and outputs for the script. That will give you a good sense of why the script argument names used below are important.**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define compareStep that takes two inputs (both intermediate data), and produce an output\n",
    "processed_data3 = OutputFileDatasetConfig(name=\"processed_data3\")\n",
    "\n",
    "# You can register the output as dataset after job completion\n",
    "processed_data3 = processed_data3.register_on_complete(\"compare_result\")\n",
    "\n",
    "source_directory = \"publish_run_compare\"\n",
    "\n",
    "compareStep = PythonScriptStep(\n",
    "    script_name=\"compare.py\",\n",
    "    arguments=[\"--compare_data1\", processed_data1.as_input(), \"--compare_data2\", processed_data2.as_input(), \"--output_compare\", processed_data3, \"--pipeline_param\", pipeline_param],  \n",
    "    compute_target=aml_compute, \n",
    "    source_directory=source_directory)\n",
    "print(\"compareStep created\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1 = Pipeline(workspace=ws, steps=[compareStep])\n",
    "print (\"Pipeline is built\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run published pipeline\n",
    "### Publish the pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline1 = pipeline1.publish(name=\"My_New_Pipeline\", description=\"My Published Pipeline Description\", continue_on_step_failure=True)\n",
    "published_pipeline1"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the continue_on_step_failure parameter specifies whether the execution of steps in the Pipeline will continue if one step fails. The default value is False, meaning when one step fails, the Pipeline execution will stop, canceling any running steps."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish the pipeline from a submitted PipelineRun\n",
    "It is also possible to publish a pipeline from a submitted PipelineRun"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit a pipeline run\n",
    "pipeline_run1 = Experiment(ws, 'Pipeline_experiment').submit(pipeline1)\n",
    "# publish a pipeline from the submitted pipeline run\n",
    "published_pipeline2 = pipeline_run1.publish_pipeline(name=\"My_New_Pipeline2\", description=\"My Published Pipeline Description\", version=\"0.1\", continue_on_step_failure=True)\n",
    "published_pipeline2"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get published pipeline\n",
    "\n",
    "You can get the published pipeline using **pipeline id**.\n",
    "\n",
    "To get all the published pipelines for a given workspace(ws): \n",
    "```css\n",
    "all_pub_pipelines = PublishedPipeline.get_all(ws)\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PublishedPipeline\n",
    "\n",
    "pipeline_id = published_pipeline1.id # use your published pipeline id\n",
    "published_pipeline = PublishedPipeline.get(ws, pipeline_id)\n",
    "published_pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run published pipeline using its REST endpoint\n",
    "[This notebook](https://aka.ms/pl-restep-auth) shows how to authenticate to AML workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "import requests\n",
    "\n",
    "auth = InteractiveLoginAuthentication()\n",
    "aad_token = auth.get_authentication_header()\n",
    "\n",
    "rest_endpoint1 = published_pipeline.endpoint\n",
    "\n",
    "print(\"You can perform HTTP POST on URL {} to trigger this pipeline\".format(rest_endpoint1))\n",
    "\n",
    "# specify the param when running the pipeline\n",
    "response = requests.post(rest_endpoint1, \n",
    "                         headers=aad_token, \n",
    "                         json={\"ExperimentName\": \"My_Pipeline1\",\n",
    "                               \"RunSource\": \"SDK\",\n",
    "                               \"ParameterAssignments\": {\"pipeline_arg\": 45}})"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response.raise_for_status()\n",
    "except Exception:    \n",
    "    raise Exception('Received bad response from the endpoint: {}\\n'\n",
    "                    'Response Code: {}\\n'\n",
    "                    'Headers: {}\\n'\n",
    "                    'Content: {}'.format(rest_endpoint, response.status_code, response.headers, response.content))\n",
    "\n",
    "run_id = response.json().get('Id')\n",
    "print('Submitted pipeline run: ', run_id)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: Data Transfer\n",
    "The next [notebook](https://aka.ms/pl-data-trans) will showcase data transfer steps between different types of data stores."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-pipeline-drafts.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-pipeline-drafts.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-pipeline-drafts.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use Pipeline Drafts\n",
    "In this notebook, we will show you how you can use Pipeline Drafts. Pipeline Drafts are mutable pipelines which can be used to submit runs and create Published Pipelines."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-pipeline-drafts.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and AML Basics\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the [configuration Notebook](https://aka.ms/pl-config) first if you haven't. This sets you up with a working config file that has information on your workspace, subscription id, etc.\n",
    "\n",
    "### Initialization Steps"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-pipeline-drafts.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-pipeline-drafts.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Target\n",
    "Retrieve an already attached Azure Machine Learning Compute to use in the Pipeline."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-pipeline-drafts.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "aml_compute_target = \"cpu-cluster\"\n",
    "try:\n",
    "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
    "    print(\"Found existing compute target: {}\".format(aml_compute_target))\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new compute target: {}\".format(aml_compute_target))\n",
    "    \n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\",\n",
    "                                                                min_nodes = 1, \n",
    "                                                                max_nodes = 4)    \n",
    "    aml_compute = ComputeTarget.create(ws, aml_compute_target, provisioning_config)\n",
    "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-pipeline-drafts.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Pipeline\n",
    "Build a simple pipeline to use to create a PipelineDraft."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-pipeline-drafts.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "source_directory = \"publish_run_train\"\n",
    "\n",
    "train_step = PythonScriptStep(\n",
    "    name=\"Training_Step\",\n",
    "    script_name=\"train.py\", \n",
    "    compute_target=aml_compute_target, \n",
    "    source_directory=source_directory)\n",
    "print(\"train step created\")\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[train_step])\n",
    "print (\"Pipeline is built\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-pipeline-drafts.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Pipeline Draft\n",
    "Create a PipelineDraft by specifying a name, description, experiment_name and Pipeline. You can also specify tags, properties and pipeline_parameter values.\n",
    "\n",
    "In this example we use the previously created Pipeline object to create the Pipeline Draft. You can also create a Pipeline Draft from an existing Pipeline Run, Published Pipeline, or other Pipeline Draft."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-pipeline-drafts.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineDraft\n",
    "\n",
    "pipeline_draft = PipelineDraft.create(ws, name=\"TestPipelineDraft\",\n",
    "                                      description=\"draft description\",\n",
    "                                      experiment_name=\"helloworld\",\n",
    "                                      pipeline=pipeline,\n",
    "                                      continue_on_step_failure=True,\n",
    "                                      tags={'dev': 'true'},\n",
    "                                      properties={'train': 'value'})\n",
    "\n",
    "created_pipeline_draft_id = pipeline_draft.id"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-pipeline-drafts.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Pipeline Drafts in a Workspace\n",
    "Use the PipelineDraft.list() function to list all PipelineDrafts in a Workspace. You can use the optional tags parameter to filter on specified tag values."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-pipeline-drafts.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_drafts = PipelineDraft.list(ws, tags={'dev': 'true'})\n",
    "\n",
    "for pipeline_draft in pipeline_drafts:\n",
    "    print(pipeline_draft)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-pipeline-drafts.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a Pipeline Draft by Id"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-pipeline-drafts.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_draft = PipelineDraft.get(ws, id=created_pipeline_draft_id)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-pipeline-drafts.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update a Pipeline Draft\n",
    "The update() function of a pipeline draft can be used to update the name, description, experiment name, pipeline parameter assignments, continue on step failure setting and Pipeline associated with the PipelineDraft. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-pipeline-drafts.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_step = PythonScriptStep(\n",
    "    name=\"New_Training_Step\",\n",
    "    script_name=\"train.py\", \n",
    "    compute_target=aml_compute_target, \n",
    "    source_directory=source_directory)\n",
    "\n",
    "new_pipeline = Pipeline(workspace=ws, steps=[new_train_step])\n",
    "\n",
    "pipeline_draft.update(name=\"UpdatedPipelineDraft\", description=\"has updated train step\", pipeline=new_pipeline)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-pipeline-drafts.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a Pipeline Run from a Pipeline Draft\n",
    "Use the pipeline_draft.submit() function to submit a PipelineRun. After the run is submitted, the PipelineDraft can still be edited and used to submit new runs."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-pipeline-drafts.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = pipeline_draft.submit_run()\n",
    "pipeline_run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-pipeline-drafts.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Published Pipeline from a Pipeline Draft\n",
    "Use the pipeline_draft.publish() function to create a Published Pipeline from the Pipeline Draft. After creating a Published Pipeline, the Pipeline Draft can still be edited and used to create other Published Pipelines."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-pipeline-drafts.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline_draft.publish()\n",
    "published_pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-pipeline-drafts.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-estimatorstep.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use CommandStep in Azure ML Pipelines\n",
    "\n",
    "This notebook shows how to use the CommandStep with Azure Machine Learning Pipelines for running commands in steps. The example shows running distributed TensorFlow training from within a pipeline.\n",
    "\n",
    "\n",
    "## Prerequisite:\n",
    "* Understand the [architecture and terms](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning\n",
    "* If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration notebook](https://aka.ms/pl-config) to:\n",
    "    * install the Azure ML SDK\n",
    "    * create a workspace and its configuration file (`config.json`)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started. First let's import some Python libraries."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Attach existing AmlCompute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training your model. In this tutorial, you create `AmlCompute` as your training compute resource."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we could not find the cluster with the given name, then we will create a new cluster here. We will create an `AmlCompute` cluster of `STANDARD_NC6` GPU VMs. This process is broken down into 3 steps:\n",
    "1. create the configuration (this step is local and only takes a second)\n",
    "2. create the cluster (this step will take about **20 seconds**)\n",
    "3. provision the VMs to bring the cluster to the initial size (of 1 in this case). This step will take about **3-5 minutes** and is providing only sparse output in the process. Please make sure to wait until the call returns before moving to the next cell"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"gpu-cluster\"\n",
    "\n",
    "try:\n",
    "    gpu_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    gpu_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it uses the scale settings for the cluster\n",
    "    gpu_cluster.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(gpu_cluster.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have created the compute target, let's see what the workspace's `compute_targets` property returns. You should now see one entry named 'gpu-cluster' of type `AmlCompute`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a CommandStep\n",
    "CommandStep adds a step to run a command in a Pipeline. For the full set of configurable options see the CommandStep [reference docs](https://docs.microsoft.com/python/api/azureml-pipeline-steps/azureml.pipeline.steps.commandstep?view=azure-ml-py).\n",
    "\n",
    "- **name:** Name of the step\n",
    "- **runconfig:** ScriptRunConfig object. You can configure a ScriptRunConfig object as you would for a standalone non-pipeline run and pass it in to this parameter. If using this option, you do not have to specify the `command`, `source_directory`, `compute_target` parameters of the CommandStep constructor as they are already defined in your ScriptRunConfig.\n",
    "- **runconfig_pipeline_params:** Override runconfig properties at runtime using key-value pairs each with name of the runconfig property and PipelineParameter for that property\n",
    "- **command:** The command to run or path of the executable/script relative to `source_directory`. It is required unless the `runconfig` parameter is specified. It can be specified with string arguments in a single string or with input/output/PipelineParameter in a list.\n",
    "- **source_directory:** A folder containing the script and other resources used in the step.\n",
    "- **compute_target:** Compute target to use \n",
    "- **allow_reuse:** Whether the step should reuse previous results when run with the same settings/inputs. If this is false, a new run will always be generated for this step during pipeline execution.\n",
    "- **version:** Optional version tag to denote a change in functionality for the step\n",
    "\n",
    "> The best practice is to use separate folders for scripts and its dependent files for each step and specify that folder as the `source_directory` for the step. This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted). Since changes in any files in the `source_directory` would trigger a re-upload of the snapshot, this helps keep the reuse of the step when there are no changes in the `source_directory` of the step."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define the environment that you want to step to run in. This example users a curated TensorFlow environment, but in practice you can configure any environment you want."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "tf_env = Environment.get(ws, name='AzureML-TensorFlow-2.3-GPU')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example will first create a ScriptRunConfig object that configures the training job. Since we are running a distributed job, specify the `distributed_job_config` parameter. If you are just running a single-node job, omit that parameter.\n",
    "\n",
    "> If you have an input dataset you want to use in this step, you can specify that as part of the command. For example, if you have a FileDataset object called `dataset` and a `--data-dir` script argument, you can do the following: `command=['python train.py --epochs 30 --data-dir', dataset.as_mount()]`.\n",
    "\n",
    "> For detailed guidance on how to move data in pipelines for input and output data, see the documentation [Moving data into and between ML pipelines](https://docs.microsoft.com/azure/machine-learning/how-to-move-data-in-out-of-pipelines)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.runconfig import MpiConfiguration\n",
    "\n",
    "src_dir = 'commandstep_train'\n",
    "distr_config = MpiConfiguration(node_count=2) # you can also specify the process_count_per_node parameter for multi-process-per-node training\n",
    "\n",
    "src = ScriptRunConfig(source_directory=src_dir,\n",
    "                      command=['python train.py --epochs 30'],\n",
    "                      compute_target=gpu_cluster,\n",
    "                      environment=tf_env,\n",
    "                      distributed_job_config=distr_config)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a CommandStep and pass in the ScriptRunConfig object to the `runconfig` parameter."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "estimatorstep-remarks-sample"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import CommandStep\n",
    "\n",
    "train = CommandStep(name='train-mnist', runconfig=src)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Submit the Pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.core import Experiment\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[train])\n",
    "pipeline_run = Experiment(ws, 'train-commandstep-pipeline').submit(pipeline)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Run Details"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Machine Learning Pipeline with DataTransferStep\n",
    "This notebook is used to demonstrate the use of DataTransferStep in an Azure Machine Learning Pipeline.\n",
    "\n",
    "> **Note:** In Azure Machine Learning, you can write output data directly to Azure Blob Storage, Azure Data Lake Storage Gen 1, Azure Data Lake Storage Gen 2, Azure FileShare without going through extra DataTransferStep. Learn how to use [OutputFileDatasetConfig](https://docs.microsoft.com/python/api/azureml-core/azureml.data.output_dataset_config.outputfiledatasetconfig?view=azure-ml-py) to achieve that with sample notebooks [here](https://aka.ms/pipeline-with-dataset).**\n",
    "\n",
    "In certain cases, you will need to transfer data from one data location to another. For example, your data may be in Azure SQL Database and you may want to move it to Azure Data Lake storage. Or, your data is in an ADLS account and you want to make it available in the Blob storage. The built-in **DataTransferStep** class helps you transfer data in these situations.\n",
    "\n",
    "The below examples show how to move data between different storage types supported in Azure Machine Learning.\n",
    "\n",
    "## Data transfer currently supports following storage types:\n",
    "\n",
    "| Data store | Supported as a source | Supported as a sink |\n",
    "| --- | --- | --- |\n",
    "| Azure Blob Storage | Yes | Yes |\n",
    "| Azure Data Lake Storage Gen 1 | Yes | Yes |\n",
    "| Azure Data Lake Storage Gen 2 | Yes | Yes |\n",
    "| Azure SQL Database | Yes | Yes |\n",
    "| Azure Database for PostgreSQL | Yes | Yes |\n",
    "| Azure Database for MySQL | Yes | Yes |"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Machine Learning and Pipeline SDK-specific imports"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core.compute import ComputeTarget, DataFactoryCompute\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.steps import DataTransferStep\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration. If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure the config file is present at .\\config.json\n",
    "\n",
    "If you don't have a config.json file, please go through the [configuration Notebook](https://aka.ms/pl-config) first.\n",
    "\n",
    "This sets you up with a working config file that has information on your workspace, subscription id, etc. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create workspace"
    ]
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Datastores and create DataReferences\n",
    "\n",
    "For background on registering your data store, consult this article:\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-access-data\n",
    "\n",
    "> Please make sure to update the following code examples with appropriate values."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Blob Storage\n",
    "\n",
    "> Since Blob Storage can contain a file and directory with the same name, you can use **source_reference_type** and **destination_reference_type** optional arguments in DataTransferStep constructor to explicitly specify whether you're referring to the file or the directory."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "datastore-remarks-sample"
    ]
   },
   "outputs": [],
   "source": [
    "from msrest.exceptions import HttpOperationError\n",
    "\n",
    "blob_datastore_name='MyBlobDatastore'\n",
    "account_name=os.getenv(\"BLOB_ACCOUNTNAME_62\", \"<my-account-name>\") # Storage account name\n",
    "container_name=os.getenv(\"BLOB_CONTAINER_62\", \"<my-container-name>\") # Name of Azure blob container\n",
    "account_key=os.getenv(\"BLOB_ACCOUNT_KEY_62\", \"<my-account-key>\") # Storage account key\n",
    "\n",
    "try:\n",
    "    blob_datastore = Datastore.get(ws, blob_datastore_name)\n",
    "    print(\"Found Blob Datastore with name: %s\" % blob_datastore_name)\n",
    "except HttpOperationError:\n",
    "    blob_datastore = Datastore.register_azure_blob_container(\n",
    "        workspace=ws,\n",
    "        datastore_name=blob_datastore_name,\n",
    "        account_name=account_name, # Storage account name\n",
    "        container_name=container_name, # Name of Azure blob container\n",
    "        account_key=account_key) # Storage account key\n",
    "    print(\"Registered blob datastore with name: %s\" % blob_datastore_name)\n",
    "\n",
    "blob_data_ref = DataReference(\n",
    "    datastore=blob_datastore,\n",
    "    data_reference_name=\"blob_test_data\",\n",
    "    path_on_datastore=\"testdata\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Data Lake Storage Gen1\n",
    "\n",
    "Please consult the following articles for detailed steps on setting up service principal authentication and assigning correct permissions to Data Lake Storage account:\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-service-to-service-authenticate-using-active-directory\n",
    "https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-store#use-service-principal-authentication"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_name='MyAdlsDatastore'\n",
    "subscription_id=os.getenv(\"ADL_SUBSCRIPTION_62\", \"<my-subscription-id>\") # subscription id of ADLS account\n",
    "resource_group=os.getenv(\"ADL_RESOURCE_GROUP_62\", \"<my-resource-group>\") # resource group of ADLS account\n",
    "store_name=os.getenv(\"ADL_STORENAME_62\", \"<my-datastore-name>\") # ADLS account name\n",
    "tenant_id=os.getenv(\"ADL_TENANT_62\", \"<my-tenant-id>\") # tenant id of service principal\n",
    "client_id=os.getenv(\"ADL_CLIENTID_62\", \"<my-client-id>\") # client id of service principal\n",
    "client_secret=os.getenv(\"ADL_CLIENT_SECRET_62\", \"<my-client-secret>\") # the secret of service principal\n",
    "\n",
    "try:\n",
    "    adls_datastore = Datastore.get(ws, datastore_name)\n",
    "    print(\"Found datastore with name: %s\" % datastore_name)\n",
    "except HttpOperationError:\n",
    "    adls_datastore = Datastore.register_azure_data_lake(\n",
    "        workspace=ws,\n",
    "        datastore_name=datastore_name,\n",
    "        subscription_id=subscription_id, # subscription id of ADLS account\n",
    "        resource_group=resource_group, # resource group of ADLS account\n",
    "        store_name=store_name, # ADLS account name\n",
    "        tenant_id=tenant_id, # tenant id of service principal\n",
    "        client_id=client_id, # client id of service principal\n",
    "        client_secret=client_secret) # the secret of service principal\n",
    "    print(\"Registered datastore with name: %s\" % datastore_name)\n",
    "\n",
    "adls_data_ref = DataReference(\n",
    "    datastore=adls_datastore,\n",
    "    data_reference_name=\"adls_test_data\",\n",
    "    path_on_datastore=\"testdata\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Data Lake Storage Gen2\n",
    "\n",
    "Please consult the following article for detailed steps on setting up service principal authentication and assigning correct permissions to Data lake Storage Gen2 account:\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage#service-principal-authentication"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "adlsgen2_datastore_name = 'myadlsgen2datastore'\n",
    "account_name=os.getenv(\"ADLSGEN2_ACCOUNTNAME_62\", \"<my-account-name>\") # ADLS Gen2 account name\n",
    "tenant_id=os.getenv(\"ADLSGEN2_TENANT_62\", \"<my-tenant-id>\") # tenant id of service principal\n",
    "client_id=os.getenv(\"ADLSGEN2_CLIENTID_62\", \"<my-client-id>\") # client id of service principal\n",
    "client_secret=os.getenv(\"ADLSGEN2_CLIENT_SECRET_62\", \"<my-client-secret>\") # the secret of service principal\n",
    "\n",
    "try:\n",
    "    adlsgen2_datastore = Datastore.get(ws, adlsgen2_datastore_name)\n",
    "    print(\"Found ADLS Gen2 datastore with name: %s\" % adlsgen2_datastore_name)\n",
    "except:\n",
    "    adlsgen2_datastore = Datastore.register_azure_data_lake_gen2(\n",
    "        workspace=ws,\n",
    "        datastore_name=adlsgen2_datastore_name,\n",
    "        filesystem='test', # Name of ADLS Gen2 filesystem\n",
    "        account_name=account_name, # ADLS Gen2 account name\n",
    "        tenant_id=tenant_id, # tenant id of service principal\n",
    "        client_id=client_id, # client id of service principal\n",
    "        client_secret=client_secret) # the secret of service principal\n",
    "    print(\"Registered datastore with name: %s\" % adlsgen2_datastore_name)\n",
    "\n",
    "adlsgen2_data_ref = DataReference(\n",
    "    datastore=adlsgen2_datastore,\n",
    "    data_reference_name='adlsgen2_test_data',\n",
    "    path_on_datastore='testdata')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure SQL Database\n",
    "\n",
    "For enabling service principal authentication for an Azure SQL Database, please follow this section in Azure Data Factory documentation: https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#service-principal-authentication\n",
    "\n",
    "> Note: When copying data **to** an Azure SQL Database, data will be _appended_ to an existing table. We also expect the source file to have a header row and the names should exactly match with column names in destination table."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sql_datastore_name=\"MySqlDatastore\"\n",
    "server_name=os.getenv(\"SQL_SERVERNAME_62\", \"<my-server-name>\") # Name of SQL server\n",
    "database_name=os.getenv(\"SQL_DATBASENAME_62\", \"<my-database-name>\") # Name of SQL database\n",
    "client_id=os.getenv(\"SQL_CLIENTNAME_62\", \"<my-client-id>\") # client id of service principal with permissions to access database\n",
    "client_secret=os.getenv(\"SQL_CLIENTSECRET_62\", \"<my-client-secret>\") # the secret of service principal\n",
    "tenant_id=os.getenv(\"SQL_TENANTID_62\", \"<my-tenant-id>\") # tenant id of service principal\n",
    "\n",
    "try:\n",
    "    sql_datastore = Datastore.get(ws, sql_datastore_name)\n",
    "    print(\"Found sql database datastore with name: %s\" % sql_datastore_name)\n",
    "except HttpOperationError:\n",
    "    sql_datastore = Datastore.register_azure_sql_database(\n",
    "        workspace=ws,\n",
    "        datastore_name=sql_datastore_name,\n",
    "        server_name=server_name,\n",
    "        database_name=database_name,\n",
    "        client_id=client_id,\n",
    "        client_secret=client_secret,\n",
    "        tenant_id=tenant_id)\n",
    "    print(\"Registered sql databse datastore with name: %s\" % sql_datastore_name)\n",
    "\n",
    "from azureml.data.sql_data_reference import SqlDataReference\n",
    "\n",
    "sql_query_data_ref = SqlDataReference(\n",
    "    datastore=sql_datastore,\n",
    "    data_reference_name=\"sql_query_data_ref\",\n",
    "    sql_query=\"select top 1 * from TestData\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Database for PostgreSQL"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "psql_datastore_name=\"MyPostgreSqlDatastore\"\n",
    "server_name=os.getenv(\"PSQL_SERVERNAME_62\", \"<my-server-name>\") # Name of PostgreSQL server \n",
    "database_name=os.getenv(\"PSQL_DATBASENAME_62\", \"<my-database-name>\") # Name of PostgreSQL database\n",
    "user_id=os.getenv(\"PSQL_USERID_62\", \"<my-user-id>\") # user id\n",
    "user_password=os.getenv(\"PSQL_USERPW_62\", \"<my-user-password>\") # user password\n",
    "\n",
    "try:\n",
    "    psql_datastore = Datastore.get(ws, psql_datastore_name)\n",
    "    print(\"Found PostgreSQL database datastore with name: %s\" % psql_datastore_name)\n",
    "except HttpOperationError:\n",
    "    psql_datastore = Datastore.register_azure_postgre_sql(\n",
    "        workspace=ws,\n",
    "        datastore_name=psql_datastore_name,\n",
    "        server_name=server_name,\n",
    "        database_name=database_name,\n",
    "        user_id=user_id,\n",
    "        user_password=user_password)\n",
    "    print(\"Registered PostgreSQL databse datastore with name: %s\" % psql_datastore_name)\n",
    "\n",
    "from azureml.data.sql_data_reference import SqlDataReference\n",
    "\n",
    "psql_query_data_ref = SqlDataReference(\n",
    "    datastore=psql_datastore,\n",
    "    data_reference_name=\"psql_query_data_ref\",\n",
    "    sql_query=\"SELECT * FROM testtable\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Database for MySQL"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mysql_datastore_name=\"MySqlDatastore\"\n",
    "server_name=os.getenv(\"MYSQL_SERVERNAME_62\", \"<my-server-name>\") # Name of MySQL server \n",
    "database_name=os.getenv(\"MYSQL_DATBASENAME_62\", \"<my-database-name>\") # Name of MySQL database\n",
    "user_id=os.getenv(\"MYSQL_USERID_62\", \"<my-user-id>\") # user id\n",
    "user_password=os.getenv(\"MYSQL_USERPW_62\", \"<my-user-password>\") # user password\n",
    "\n",
    "try:\n",
    "    mysql_datastore = Datastore.get(ws, mysql_datastore_name)\n",
    "    print(\"Found MySQL database datastore with name: %s\" % mysql_datastore_name)\n",
    "except HttpOperationError:\n",
    "    mysql_datastore = Datastore.register_azure_my_sql(\n",
    "        workspace=ws,\n",
    "        datastore_name=mysql_datastore_name,\n",
    "        server_name=server_name,\n",
    "        database_name=database_name,\n",
    "        user_id=user_id,\n",
    "        user_password=user_password)\n",
    "    print(\"Registered MySQL databse datastore with name: %s\" % mysql_datastore_name)\n",
    "\n",
    "from azureml.data.sql_data_reference import SqlDataReference\n",
    "\n",
    "mysql_query_data_ref = SqlDataReference(\n",
    "    datastore=mysql_datastore,\n",
    "    data_reference_name=\"mysql_query_data_ref\",\n",
    "    sql_query=\"SELECT * FROM testtable\")\n",
    "\n",
    "mysql_table_data_ref = SqlDataReference(\n",
    "    datastore=mysql_datastore,\n",
    "    data_reference_name=\"mysql_table_data_ref\",\n",
    "    sql_table=\"testtable\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Data Factory Account"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_factory_name = 'adftest'\n",
    "\n",
    "def get_or_create_data_factory(workspace, factory_name):\n",
    "    try:\n",
    "        return DataFactoryCompute(workspace, factory_name)\n",
    "    except ComputeTargetException as e:\n",
    "        if 'ComputeTargetNotFound' in e.message:\n",
    "            print('Data factory not found, creating...')\n",
    "            provisioning_config = DataFactoryCompute.provisioning_configuration()\n",
    "            data_factory = ComputeTarget.create(workspace, factory_name, provisioning_config)\n",
    "            data_factory.wait_for_completion()\n",
    "            return data_factory\n",
    "        else:\n",
    "            raise e\n",
    "            \n",
    "data_factory_compute = get_or_create_data_factory(ws, data_factory_name)\n",
    "\n",
    "print(\"Setup Azure Data Factory account complete\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DataTransferStep"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DataTransferStep** is used to transfer data between Azure Blob, Azure Data Lake Store, and Azure SQL database.\n",
    "\n",
    "- **name:** Name of module\n",
    "- **source_data_reference:** Input connection that serves as source of data transfer operation.\n",
    "- **destination_data_reference:** Input connection that serves as destination of data transfer operation.\n",
    "- **compute_target:** Azure Data Factory to use for transferring data.\n",
    "- **allow_reuse:** Whether the step should reuse results of previous DataTransferStep when run with same inputs. Set as False to force data to be transferred again.\n",
    "\n",
    "Optional arguments to explicitly specify whether a path corresponds to a file or a directory. These are useful when storage contains both file and directory with the same name or when creating a new destination path.\n",
    "\n",
    "- **source_reference_type:** An optional string specifying the type of source_data_reference. Possible values include: 'file', 'directory'. When not specified, we use the type of existing path or directory if it's a new path.\n",
    "- **destination_reference_type:** An optional string specifying the type of destination_data_reference. Possible values include: 'file', 'directory'. When not specified, we use the type of existing path or directory if it's a new path."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_adls_to_blob = DataTransferStep(\n",
    "    name=\"transfer_adls_to_blob\",\n",
    "    source_data_reference=adls_data_ref,\n",
    "    destination_data_reference=blob_data_ref,\n",
    "    compute_target=data_factory_compute)\n",
    "\n",
    "print(\"Data transfer step created\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transfer_adlsgen2_to_blob = DataTransferStep(\n",
    "    name='transfer_adlsgen2_to_blob',\n",
    "    source_data_reference=adlsgen2_data_ref,\n",
    "    destination_data_reference=blob_data_ref,\n",
    "    compute_target=data_factory_compute)\n",
    "\n",
    "transfer_sql_to_blob = DataTransferStep(\n",
    "    name=\"transfer_sql_to_blob\",\n",
    "    source_data_reference=sql_query_data_ref,\n",
    "    destination_data_reference=blob_data_ref,\n",
    "    compute_target=data_factory_compute,\n",
    "    destination_reference_type='file')\n",
    "\n",
    "transfer_psql_to_blob = DataTransferStep(\n",
    "    name=\"transfer_psql_to_blob\",\n",
    "    source_data_reference=psql_query_data_ref,\n",
    "    destination_data_reference=blob_data_ref,\n",
    "    compute_target=data_factory_compute,\n",
    "    destination_reference_type='file')\n",
    "\n",
    "transfer_mysql_to_blob = DataTransferStep(\n",
    "    name=\"transfer_mysql_to_blob\",\n",
    "    source_data_reference=mysql_query_data_ref,\n",
    "    destination_data_reference=blob_data_ref,\n",
    "    compute_target=data_factory_compute)\n",
    "print(\"Data transfer step created for Sql server, PostgreSQL and MySQL\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Submit the Experiment"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_01 = Pipeline(\n",
    "    description=\"data_transfer_01\",\n",
    "    workspace=ws,\n",
    "    steps=[transfer_adls_to_blob])\n",
    "\n",
    "pipeline_run_01 = Experiment(ws, \"Data_Transfer_example_01\").submit(pipeline_01)\n",
    "pipeline_run_01.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_02 = Pipeline(\n",
    "    description=\"data_transfer_02\",\n",
    "    workspace=ws,\n",
    "    steps=[transfer_sql_to_blob,transfer_psql_to_blob, transfer_adlsgen2_to_blob])\n",
    "\n",
    "pipeline_run_02 = Experiment(ws, \"Data_Transfer_example_02\").submit(pipeline_02)\n",
    "pipeline_run_02.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Run Details"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run_01).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run_02).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: Databricks as a Compute Target\n",
    "To use Databricks as a compute target from Azure Machine Learning Pipeline, a DatabricksStep is used. This [notebook](https://aka.ms/pl-databricks) demonstrates the use of a DatabricksStep in an Azure Machine Learning Pipeline."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-data-transfer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Setup a Schedule for a Published Pipeline or Pipeline Endpoint\n",
    "In this notebook, we will show you how you can run an already published pipeline or a pipeline endpoint on a schedule."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and AML Basics\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the [configuration Notebook](https://aka.ms/pl-config) first if you haven't. This sets you up with a working config file that has information on your workspace, subscription id, etc.\n",
    "\n",
    "### Initialization Steps"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Targets\n",
    "#### Retrieve an already attached Azure Machine Learning Compute"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "aml_compute_target = \"cpu-cluster\"\n",
    "try:\n",
    "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
    "    print(\"Found existing compute target: {}\".format(aml_compute_target))\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new compute target: {}\".format(aml_compute_target))\n",
    "    \n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\",\n",
    "                                                                min_nodes = 1, \n",
    "                                                                max_nodes = 4)    \n",
    "    aml_compute = ComputeTarget.create(ws, aml_compute_target, provisioning_config)\n",
    "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Publish Pipeline\n",
    "Build a simple pipeline, publish it and add a schedule to run it."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a pipeline step\n",
    "Define a single step pipeline for demonstration purpose. The best practice is to use separate folders for scripts and its dependent files for each step and specify that folder as the `source_directory` for the step. This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted). Since changes in any files in the `source_directory` would trigger a re-upload of the snapshot, this helps keep the reuse of the step when there are no changes in the `source_directory` of the step."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "source_directory = \"publish_run_train\"\n",
    "\n",
    "trainStep = PythonScriptStep(\n",
    "    name=\"Training_Step\",\n",
    "    script_name=\"train.py\", \n",
    "    compute_target=aml_compute_target, \n",
    "    source_directory=source_directory\n",
    ")\n",
    "print(\"TrainStep created\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline1 = Pipeline(workspace=ws, steps=[trainStep])\n",
    "print (\"Pipeline is built\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish the pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timenow = datetime.now().strftime('%m-%d-%Y-%H-%M')\n",
    "\n",
    "pipeline_name = timenow + \"-Pipeline\"\n",
    "print(pipeline_name)\n",
    "\n",
    "published_pipeline1 = pipeline1.publish(\n",
    "    name=pipeline_name, \n",
    "    description=pipeline_name)\n",
    "print(\"Newly published pipeline id: {}\".format(published_pipeline1.id))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Create a Pipeline Endpoint\n",
    "Alternatively, you can create a schedule to run a pipeline endpoint instead of a published pipeline. You will need this to create a schedule against a pipeline endpoint in the last section of this notebook. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineEndpoint\n",
    "\n",
    "pipeline_endpoint = PipelineEndpoint.publish(workspace=ws, name=\"ScheduledPipelineEndpoint\",\n",
    "                                            pipeline=pipeline1, description=\"Publish pipeline endpoint for schedule test\")\n",
    "pipeline_endpoint"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedule Operations\n",
    "Schedule operations require id of a published pipeline. You can get all published pipelines and do Schedule operations on them, or if you already know the id of the published pipeline, you can use it directly as well.\n",
    "### Get published pipeline ID"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PublishedPipeline\n",
    "\n",
    "# You could retrieve all pipelines that are published, or \n",
    "# just get the published pipeline object that you have the ID for.\n",
    "\n",
    "# Get all published pipeline objects in the workspace\n",
    "all_pub_pipelines = PublishedPipeline.list(ws)\n",
    "\n",
    "# We will iterate through the list of published pipelines and \n",
    "# use the last ID in the list for Schelue operations: \n",
    "print(\"Published pipelines found in the workspace:\")\n",
    "for pub_pipeline in all_pub_pipelines:\n",
    "    print(pub_pipeline.id)\n",
    "    pub_pipeline_id = pub_pipeline.id\n",
    "\n",
    "print(\"Published pipeline id to be used for Schedule operations: {}\".format(pub_pipeline_id))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a schedule for the published pipeline using a recurrence\n",
    "This schedule will run on a specified recurrence interval."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\n",
    "\n",
    "recurrence = ScheduleRecurrence(frequency=\"Day\", interval=2, hours=[22], minutes=[30]) # Runs every other day at 10:30pm\n",
    "\n",
    "schedule = Schedule.create(workspace=ws, name=\"My_Schedule\",\n",
    "                           pipeline_id=pub_pipeline_id, \n",
    "                           experiment_name='Schedule_Run',\n",
    "                           recurrence=recurrence,\n",
    "                           wait_for_provisioning=True,\n",
    "                           description=\"Schedule Run\")\n",
    "\n",
    "# You may want to make sure that the schedule is provisioned properly\n",
    "# before making any further changes to the schedule\n",
    "\n",
    "print(\"Created schedule with id: {}\".format(schedule.id))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Set the `wait_for_provisioning` flag to False if you do not want to wait for the call to provision the schedule in the backend."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all schedules for a given pipeline\n",
    "Once you have the published pipeline ID, then you can get all schedules for that pipeline."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedules = Schedule.list(ws, pipeline_id=pub_pipeline_id)\n",
    "\n",
    "# We will iterate through the list of schedules and \n",
    "# use the last recurrence schedule in the list for further operations: \n",
    "print(\"Found these schedules for the pipeline id {}:\".format(pub_pipeline_id))\n",
    "for schedule in schedules: \n",
    "    print(schedule.id)\n",
    "    if schedule.recurrence is not None:\n",
    "        schedule_id = schedule.id\n",
    "\n",
    "print(\"Schedule id to be used for schedule operations: {}\".format(schedule_id))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all schedules in your workspace\n",
    "You can also iterate through all schedules in your workspace if needed."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use active_only=False to get all schedules including disabled schedules\n",
    "schedules = Schedule.list(ws, active_only=True) \n",
    "print(\"Your workspace has the following schedules set up:\")\n",
    "for schedule in schedules:\n",
    "    print(\"{} (Published pipeline: {}\".format(schedule.id, schedule.pipeline_id))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the schedule"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetched_schedule = Schedule.get(ws, schedule_id)\n",
    "print(\"Using schedule with id: {}\".format(fetched_schedule.id))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disable the schedule\n",
    "It is important to note the best practice of disabling schedules when not in use.\n",
    "The number of schedule triggers allowed per month per region per subscription is 100,000.\n",
    "This is calculated using the project trigger counts for all active schedules."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1606157800044
    }
   },
   "outputs": [],
   "source": [
    "# Set the wait_for_provisioning flag to False if you do not want to wait  \n",
    "# for the call to provision the schedule in the backend.\n",
    "fetched_schedule.disable(wait_for_provisioning=True)\n",
    "fetched_schedule = Schedule.get(ws, schedule_id)\n",
    "print(\"Disabled schedule {}. New status is: {}\".format(fetched_schedule.id, fetched_schedule.status))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reenable the schedule"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the wait_for_provisioning flag to False if you do not want to wait  \n",
    "# for the call to provision the schedule in the backend.\n",
    "fetched_schedule.enable(wait_for_provisioning=True)\n",
    "fetched_schedule = Schedule.get(ws, schedule_id)\n",
    "print(\"Enabled schedule {}. New status is: {}\".format(fetched_schedule.id, fetched_schedule.status))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change recurrence of the schedule"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the wait_for_provisioning flag to False if you do not want to wait  \n",
    "# for the call to provision the schedule in the backend.\n",
    "recurrence = ScheduleRecurrence(frequency=\"Hour\", interval=2) # Runs every two hours\n",
    "\n",
    "fetched_schedule = Schedule.get(ws, schedule_id)\n",
    "\n",
    "fetched_schedule.update(name=\"My_Updated_Schedule\", \n",
    "                        description=\"Updated_Schedule_Run\", \n",
    "                        status='Active', \n",
    "                        wait_for_provisioning=True,\n",
    "                        recurrence=recurrence)\n",
    "\n",
    "fetched_schedule = Schedule.get(ws, fetched_schedule.id)\n",
    "\n",
    "print(\"Updated schedule:\", fetched_schedule.id, \n",
    "      \"\\nNew name:\", fetched_schedule.name,\n",
    "      \"\\nNew frequency:\", fetched_schedule.recurrence.frequency,\n",
    "      \"\\nNew status:\", fetched_schedule.status)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a schedule for the pipeline using a Datastore\n",
    "This schedule will run when additions or modifications are made to Blobs in the Datastore.\n",
    "By default, the Datastore container is monitored for changes. Use the path_on_datastore parameter to instead specify a path on the Datastore to monitor for changes. Note: the path_on_datastore will be under the container for the datastore, so the actual path monitored will be container/path_on_datastore. Changes made to subfolders in the container/path will not trigger the schedule.\n",
    "Note: Only Blob Datastores are supported.\n",
    "Note: Not supported for CMK workspaces. Please review these [instructions](https://docs.microsoft.com/azure/machine-learning/how-to-trigger-published-pipeline) in order to setup a blob trigger submission schedule with CMK enabled. Also see those instructions to bring your own LogicApp to avoid the schedule triggers per month limit."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.datastore import Datastore\n",
    "\n",
    "datastore = Datastore(workspace=ws, name=\"workspaceblobstore\")\n",
    "\n",
    "schedule = Schedule.create(workspace=ws, name=\"My_Schedule\",\n",
    "                           pipeline_id=pub_pipeline_id, \n",
    "                           experiment_name='Schedule_Run',\n",
    "                           datastore=datastore,\n",
    "                           wait_for_provisioning=True,\n",
    "                           description=\"Schedule Run\")\n",
    "                          #polling_interval=5, use polling_interval to specify how often to poll for blob additions/modifications. Default value is 5 minutes.\n",
    "                          #path_on_datastore=\"file/path\") use path_on_datastore to specify a specific folder to monitor for changes.\n",
    "\n",
    "# You may want to make sure that the schedule is provisioned properly\n",
    "# before making any further changes to the schedule\n",
    "\n",
    "print(\"Created schedule with id: {}\".format(schedule.id))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1606157862620
    }
   },
   "outputs": [],
   "source": [
    "# Set the wait_for_provisioning flag to False if you do not want to wait  \n",
    "# for the call to provision the schedule in the backend.\n",
    "schedule.disable(wait_for_provisioning=True)\n",
    "schedule = Schedule.get(ws, schedule_id)\n",
    "print(\"Disabled schedule {}. New status is: {}\".format(schedule.id, schedule.status))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Create a schedule for a pipeline endpoint\n",
    "Alternative to creating schedules for a published pipeline, you can also create schedules to run pipeline endpoints.\n",
    "Retrieve the pipeline endpoint id to create a schedule. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1606157888851
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "pipeline_endpoint_by_name = PipelineEndpoint.get(workspace=ws, name=\"ScheduledPipelineEndpoint\")\n",
    "published_pipeline_endpoint_id = pipeline_endpoint_by_name.id\n",
    "\n",
    "recurrence = ScheduleRecurrence(frequency=\"Day\", interval=2, hours=[22], minutes=[30]) # Runs every other day at 10:30pm\n",
    "\n",
    "schedule = Schedule.create_for_pipeline_endpoint(workspace=ws, name=\"My_Endpoint_Schedule\",\n",
    "                                                 pipeline_endpoint_id=published_pipeline_endpoint_id,\n",
    "                                                 experiment_name='Schedule_Run',\n",
    "                                                 recurrence=recurrence, description=\"Schedule_Run\",\n",
    "                                                 wait_for_provisioning=True)\n",
    "\n",
    "# You may want to make sure that the schedule is provisioned properly\n",
    "# before making any further changes to the schedule\n",
    "\n",
    "print(\"Created schedule with id: {}\".format(schedule.id))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Get all schedules for a given pipeline endpoint\n",
    "Once you have the pipeline endpoint ID, then you can get all schedules for that pipeline endopint."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "schedules_for_pipeline_endpoints = Schedule.\\\n",
    "    get_schedules_for_pipeline_endpoint_id(ws,\n",
    "                                           pipeline_endpoint_id=published_pipeline_endpoint_id)\n",
    "print('Got all schedules for pipeline endpoint:', published_pipeline_endpoint_id, 'Count:',\n",
    "      len(schedules_for_pipeline_endpoints))\n",
    "\n",
    "print('done')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Disable the schedule created for running the pipeline endpont\n",
    "Recall the best practice of disabling schedules when not in use.\n",
    "The number of schedule triggers allowed per month per region per subscription is 100,000.\n",
    "This is calculated using the project trigger counts for all active schedules."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "fetched_schedule = Schedule.get(ws, schedule_id)\n",
    "print(\"Using schedule with id: {}\".format(fetched_schedule.id))\n",
    "\n",
    "# Set the wait_for_provisioning flag to False if you do not want to wait  \n",
    "# for the call to provision the schedule in the backend.\n",
    "fetched_schedule.disable(wait_for_provisioning=True)\n",
    "fetched_schedule = Schedule.get(ws, schedule_id)\n",
    "print(\"Disabled schedule {}. New status is: {}\".format(fetched_schedule.id, fetched_schedule.status))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-adla-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-adla-as-compute-target.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-adla-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AML Pipeline with AdlaStep\n",
    "\n",
    "This notebook is used to demonstrate the use of AdlaStep in AML Pipelines. [AdlaStep](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.adla_step.adlastep?view=azure-ml-py) is used to run U-SQL scripts using Azure Data Lake Analytics service. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-adla-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AML and Pipeline SDK-specific imports"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-adla-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from msrest.exceptions import HttpOperationError\n",
    "\n",
    "import azureml.core\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.core.compute import ComputeTarget, AdlaCompute\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import AdlaStep\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-adla-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration. If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the [configuration Notebook](https://aka.ms/pl-config) first if you haven't."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-adla-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create workspace"
    ]
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-adla-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach ADLA account to workspace\n",
    "\n",
    "To submit jobs to Azure Data Lake Analytics service, you must first attach your ADLA account to the workspace. You'll need to provide the account name and resource group of ADLA account to complete this part."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-adla-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "sample-adlacompute-attach"
    ]
   },
   "outputs": [],
   "source": [
    "adla_compute_name = 'testadl' # Name to associate with new compute in workspace\n",
    "\n",
    "# ADLA account details needed to attach as compute to workspace\n",
    "adla_account_name = \"<adla_account_name>\" # Name of the Azure Data Lake Analytics account\n",
    "adla_resource_group = \"<adla_resource_group>\" # Name of the resource group which contains this account\n",
    "\n",
    "try:\n",
    "    # check if already attached\n",
    "    adla_compute = AdlaCompute(ws, adla_compute_name)\n",
    "except ComputeTargetException:\n",
    "    print('attaching adla compute...')\n",
    "    attach_config = AdlaCompute.attach_configuration(resource_group=adla_resource_group, account_name=adla_account_name)\n",
    "    adla_compute = ComputeTarget.attach(ws, adla_compute_name, attach_config)\n",
    "    adla_compute.wait_for_completion()\n",
    "\n",
    "print(\"Using ADLA compute:{}\".format(adla_compute.cluster_resource_id))\n",
    "print(\"Provisioning state:{}\".format(adla_compute.provisioning_state))\n",
    "print(\"Provisioning errors:{}\".format(adla_compute.provisioning_errors))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-adla-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Data Lake Storage as Datastore\n",
    "\n",
    "To register Data Lake Storage as Datastore in workspace, you'll need account information like account name, resource group and subscription Id. \n",
    "\n",
    "> AdlaStep can only work with data stored in the **default** Data Lake Storage of the Data Lake Analytics account provided above. If the data you need to work with is in a non-default storage, you can use a DataTransferStep to copy the data before training. You can find the default storage by opening your Data Lake Analytics account in Azure portal and then navigating to 'Data sources' item under Settings in the left pane.\n",
    "\n",
    "### Grant Azure AD application access to Data Lake Storage\n",
    "\n",
    "You'll also need to provide an Active Directory application which can access Data Lake Storage. [This document](https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-service-to-service-authenticate-using-active-directory) contains step-by-step instructions on how to create an AAD application and assign to Data Lake Storage. Couple of important notes when assigning permissions to AAD app:\n",
    "\n",
    "- Access should be provided at root folder level.\n",
    "- In 'Assign permissions' pane, select Read, Write, and Execute permissions for 'This folder and all children'. Add as 'An access permission entry and a default permission entry' to make sure application can access any new files created in the future."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-adla-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_name = 'MyAdlsDatastore' # Name to associate with data store in workspace\n",
    "\n",
    "# ADLS storage account details needed to register as a Datastore\n",
    "subscription_id = os.getenv(\"ADL_SUBSCRIPTION_62\", \"<my-subscription-id>\") # subscription id of ADLS account\n",
    "resource_group = os.getenv(\"ADL_RESOURCE_GROUP_62\", \"<my-resource-group>\") # resource group of ADLS account\n",
    "store_name = os.getenv(\"ADL_STORENAME_62\", \"<my-datastore-name>\") # ADLS account name\n",
    "tenant_id = os.getenv(\"ADL_TENANT_62\", \"<my-tenant-id>\") # tenant id of service principal\n",
    "client_id = os.getenv(\"ADL_CLIENTID_62\", \"<my-client-id>\") # client id of service principal\n",
    "client_secret = os.getenv(\"ADL_CLIENT_62_SECRET\", \"<my-client-secret>\") # the secret of service principal\n",
    "\n",
    "try:\n",
    "    adls_datastore = Datastore.get(ws, datastore_name)\n",
    "    print(\"found datastore with name: %s\" % datastore_name)\n",
    "except HttpOperationError:\n",
    "    adls_datastore = Datastore.register_azure_data_lake(\n",
    "        workspace=ws,\n",
    "        datastore_name=datastore_name,\n",
    "        subscription_id=subscription_id, # subscription id of ADLS account\n",
    "        resource_group=resource_group, # resource group of ADLS account\n",
    "        store_name=store_name, # ADLS account name\n",
    "        tenant_id=tenant_id, # tenant id of service principal\n",
    "        client_id=client_id, # client id of service principal\n",
    "        client_secret=client_secret) # the secret of service principal\n",
    "    print(\"registered datastore with name: %s\" % datastore_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-adla-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup inputs and outputs\n",
    "\n",
    "For purpose of this demo, we're going to execute a simple U-SQL script that reads a CSV file and writes portion of content to a new text file. First, let's create our sample input which contains 3 columns: employee Id, name and department Id."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-adla-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder to store files for our job\n",
    "sample_folder = \"adla_sample\"\n",
    "\n",
    "if not os.path.isdir(sample_folder):\n",
    "    os.mkdir(sample_folder)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-adla-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $sample_folder/sample_input.csv\n",
    "1, Noah, 100\n",
    "3, Liam, 100\n",
    "4, Emma, 100\n",
    "5, Jacob, 100\n",
    "7, Jennie, 100"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-adla-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload this file to Data Lake Storage at location `adla_sample/sample_input.csv` and create a DataReference to refer to this file."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-adla-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = DataReference(\n",
    "    datastore=adls_datastore,\n",
    "    data_reference_name=\"employee_data\",\n",
    "    path_on_datastore=\"adla_sample/sample_input.csv\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-adla-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create PipelineData object to store output produced by AdlaStep."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-adla-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = PipelineData(\"sample_output\", datastore=adls_datastore)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-adla-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write your U-SQL script\n",
    "\n",
    "Now let's write a U-Sql script that reads above CSV file and writes the name column to a new file.\n",
    "\n",
    "Instead of hard-coding paths in your script, you can use `@@name@@` syntax to refer to inputs, outputs, and parameters.\n",
    "\n",
    "- If `name` is the name of an input or output port binding, any occurrences of `@@name@@` in the script are replaced with actual data path of corresponding port binding.\n",
    "- If `name` matches any key in the `params` dictionary, any occurrences of `@@name@@` will be replaced with corresponding value in the dictionary.\n",
    "\n",
    "Note the use of @@ syntax in the below script. Before submitting the job to Data Lake Analytics service, `@@emplyee_data@@` will be replaced with actual path of `sample_input.csv` in Data Lake Storage. Similarly, `@@sample_output@@` will be replaced with a path in Data Lake Storage which will be used to store intermediate output produced by the step."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-adla-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $sample_folder/sample_script.usql\n",
    "\n",
    "// Read employee information from csv file\n",
    "@employees = \n",
    "    EXTRACT EmpId int, EmpName string, DeptId int\n",
    "    FROM \"@@employee_data@@\"\n",
    "    USING Extractors.Csv();\n",
    "\n",
    "// Export employee names to text file\n",
    "OUTPUT\n",
    "(\n",
    "    SELECT EmpName\n",
    "    FROM @employees\n",
    ")\n",
    "TO \"@@sample_output@@\"\n",
    "USING Outputters.Text();"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-adla-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an AdlaStep\n",
    "\n",
    "**[AdlaStep](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.adla_step.adlastep?view=azure-ml-py)** is used to run U-SQL script using Azure Data Lake Analytics.\n",
    "\n",
    "- **name:** Name of module\n",
    "- **script_name:** name of U-SQL script file\n",
    "- **inputs:** List of input port bindings\n",
    "- **outputs:** List of output port bindings\n",
    "- **compute_target:** the ADLA compute to use for this job\n",
    "- **params:** Dictionary of name-value pairs to pass to U-SQL job *(optional)*\n",
    "- **degree_of_parallelism:** the degree of parallelism to use for this job *(optional)*\n",
    "- **priority:** the priority value to use for the current job *(optional)*\n",
    "- **runtime_version:** the runtime version of the Data Lake Analytics engine *(optional)*\n",
    "- **source_directory:** folder that contains the script, assemblies etc. *(optional)*\n",
    "- **hash_paths:** list of paths to hash to detect a change (script file is always hashed) *(optional)*\n",
    "\n",
    "The best practice is to use separate folders for scripts and its dependent files for each step and specify that folder as the `source_directory` for the step. This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted). Since changes in any files in the `source_directory` would trigger a re-upload of the snapshot, this helps keep the reuse of the step when there are no changes in the `source_directory` of the step."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-adla-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "adlastep-remarks-sample"
    ]
   },
   "outputs": [],
   "source": [
    "adla_step = AdlaStep(\n",
    "    name='extract_employee_names',\n",
    "    script_name='sample_script.usql',\n",
    "    source_directory=sample_folder,\n",
    "    inputs=[sample_input],\n",
    "    outputs=[sample_output],\n",
    "    compute_target=adla_compute)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-adla-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Submit the Experiment"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-adla-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[adla_step])\n",
    "\n",
    "pipeline_run = Experiment(ws, 'adla_sample').submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-adla-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Run Details"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-adla-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-adla-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Machine Learning Pipelines: Getting Started\n",
    "\n",
    "## Overview\n",
    "\n",
    "\n",
    "A common scenario when using machine learning components is to have a data workflow that includes the following steps:\n",
    "\n",
    "- Preparing/preprocessing a given dataset for training, followed by\n",
    "- Training a machine learning model on this data, and then\n",
    "- Deploying this trained model in a separate environment, and finally\n",
    "- Running a batch scoring task on another data set, using the trained model.\n",
    "\n",
    "Azure's Machine Learning pipelines give you a way to combine multiple steps like these into one configurable workflow, so that multiple agents/users can share and/or reuse this workflow. Machine learning pipelines thus provide a consistent, reproducible mechanism for building, evaluating, deploying, and running ML systems.\n",
    "\n",
    "To get more information about Azure machine learning pipelines, please read our [Azure Machine Learning Pipelines](https://aka.ms/pl-concept) overview, or the [readme article](https://aka.ms/pl-readme).\n",
    "\n",
    "In this notebook, we provide a gentle introduction to Azure machine learning pipelines. We build a pipeline that runs jobs unattended on different compute clusters; in this notebook, you'll see how to use the basic Azure ML SDK APIs for constructing this pipeline.\n",
    " "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Azure Machine Learning Basics\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the [configuration notebook](https://aka.ms/pl-config) first if you haven't. This sets you up with a working config file that has information on your workspace, subscription id, etc. \n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Machine Learning Imports\n",
    "\n",
    "In this first code cell, we import key Azure Machine Learning modules that we will use below. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Datastore\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline-specific SDK imports\n",
    "\n",
    "Here, we import key pipeline modules, whose use will be illustrated in the examples below."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "print(\"Pipeline SDK-specific imports completed\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Workspace\n",
    "\n",
    "Initialize a [workspace](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace(class%29) object from persisted configuration."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create workspace"
    ]
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n",
    "\n",
    "# Default datastore\n",
    "def_blob_store = ws.get_default_datastore() \n",
    "# The following call GETS the Azure Blob Store associated with your workspace.\n",
    "# Note that workspaceblobstore is **the name of this store and CANNOT BE CHANGED and must be used as is** \n",
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "print(\"Blobstore's name: {}\".format(def_blob_store.name))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required data and script files for the the tutorial\n",
    "Sample files required to finish this tutorial are already copied to the corresponding source_directory locations. Even though the .py provided in the samples does not have much \"ML work\" as a data scientist, you will work on this extensively as part of your work. To complete this tutorial, the contents of these files are not very important. The one-line files are for demostration purpose only."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datastore concepts\n",
    "A [Datastore](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.datastore.datastore?view=azure-ml-py) is a place where data can be stored that is then made accessible to a compute either by means of mounting or copying the data to the compute target. \n",
    "\n",
    "A Datastore can either be backed by an Azure File Storage (default) or by an Azure Blob Storage.\n",
    "\n",
    "In this next step, we will upload the training and test set into the workspace's default storage (File storage), and another piece of data to Azure Blob Storage. When to use [Azure Blobs](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction), [Azure Files](https://docs.microsoft.com/en-us/azure/storage/files/storage-files-introduction), or [Azure Disks](https://docs.microsoft.com/en-us/azure/virtual-machines/linux/managed-disks-overview) is [detailed here](https://docs.microsoft.com/en-us/azure/storage/common/storage-decide-blobs-files-disks).\n",
    "\n",
    "**Please take good note of the concept of the datastore.**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload data to default datastore\n",
    "Default datastore on workspace is the Azure  File storage. The workspace has a Blob storage associated with it as well. Let's upload a file to each of these storages."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_default_datastore() gets the default Azure Blob Store associated with your workspace.\n",
    "# Here we are reusing the def_blob_store object we obtained earlier\n",
    "def_blob_store.upload_files([\"./20news.pkl\"], target_path=\"20newsgroups\", overwrite=True)\n",
    "print(\"Upload call completed\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) See your files using Azure Portal\n",
    "Once you successfully uploaded the files, you can browse to them (or upload more files) using [Azure Portal](https://portal.azure.com). At the portal, make sure you have selected your subscription (click *Resource Groups* and then select the subscription). Then look for your **Machine Learning Workspace** name. It has a link to your storage. Click on the storage link. It will take you to a page where you can see [Blobs](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction), [Files](https://docs.microsoft.com/en-us/azure/storage/files/storage-files-introduction), [Tables](https://docs.microsoft.com/en-us/azure/storage/tables/table-storage-overview), and [Queues](https://docs.microsoft.com/en-us/azure/storage/queues/storage-queues-introduction). We have uploaded a file each to the Blob storage and to the File storage in the above step. You should be able to see both of these files in their respective locations. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Targets\n",
    "A compute target specifies where to execute your program such as a remote Docker on a VM, or a cluster. A compute target needs to be addressable and accessible by you.\n",
    "\n",
    "**You need at least one compute target to send your payload to. We are planning to use Azure Machine Learning Compute exclusively for this tutorial for all steps. However in some cases you may require multiple compute targets as some steps may run in one compute target like Azure Machine Learning Compute, and some other steps in the same pipeline could run in a different compute target.**\n",
    "\n",
    "*The example belows show creating/retrieving/attaching to an Azure Machine Learning Compute instance.*"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of Compute Targets on the workspace"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cts = ws.compute_targets\n",
    "for ct in cts:\n",
    "    print(ct)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve or create a Azure Machine Learning compute\n",
    "Azure Machine Learning Compute is a service for provisioning and managing clusters of Azure virtual machines for running machine learning workloads. Let's create a new Azure Machine Learning Compute in the current workspace, if it doesn't already exist. We will then run the training script on this compute target.\n",
    "\n",
    "If we could not find the compute with the given name in the previous cell, then we will create a new compute here. We will create an Azure Machine Learning Compute containing **STANDARD_D2_V2 CPU VMs**. This process is broken down into the following steps:\n",
    "\n",
    "1. Create the configuration\n",
    "2. Create the Azure Machine Learning compute\n",
    "\n",
    "**This process will take about 3 minutes and is providing only sparse output in the process. Please make sure to wait until the call returns before moving to the next cell.**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "aml_compute_target = \"cpu-cluster\"\n",
    "try:\n",
    "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
    "    print(\"found existing compute target.\")\n",
    "except ComputeTargetException:\n",
    "    print(\"creating new compute target\")\n",
    "    \n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\",\n",
    "                                                                min_nodes = 1, \n",
    "                                                                max_nodes = 4)    \n",
    "    aml_compute = ComputeTarget.create(ws, aml_compute_target, provisioning_config)\n",
    "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "print(\"Azure Machine Learning Compute attached\")\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a more detailed view of current Azure Machine Learning Compute status, use get_status()\n",
    "# example: un-comment the following line.\n",
    "# print(aml_compute.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wait for this call to finish before proceeding (you will see the asterisk turning to a number).**\n",
    "\n",
    "Now that you have created the compute target, let's see what the workspace's compute_targets() function returns. You should now see one entry named 'amlcompute' of type AmlCompute."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we have completed learning the basics of Azure Machine Learning (AML), let's go ahead and start understanding the Pipeline concepts.**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Step in a Pipeline\n",
    "A Step is a unit of execution. Step typically needs a target of execution (compute target), a script to execute, and may require script arguments and inputs, and can produce outputs. The step also could take a number of other parameters. Azure Machine Learning Pipelines provides the following built-in Steps:\n",
    "\n",
    "- [**PythonScriptStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.python_script_step.pythonscriptstep?view=azure-ml-py): Adds a step to run a Python script in a Pipeline.\n",
    "- [**AdlaStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.adla_step.adlastep?view=azure-ml-py): Adds a step to run U-SQL script using Azure Data Lake Analytics.\n",
    "- [**DataTransferStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.data_transfer_step.datatransferstep?view=azure-ml-py): Transfers data between Azure Blob and Data Lake accounts.\n",
    "- [**DatabricksStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.databricks_step.databricksstep?view=azure-ml-py): Adds a DataBricks notebook as a step in a Pipeline.\n",
    "- [**HyperDriveStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.hyper_drive_step.hyperdrivestep?view=azure-ml-py): Creates a Hyper Drive step for Hyper Parameter Tuning in a Pipeline.\n",
    "- [**AzureBatchStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.azurebatch_step.azurebatchstep?view=azure-ml-py): Creates a step for submitting jobs to Azure Batch\n",
    "- [**EstimatorStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.estimator_step.estimatorstep?view=azure-ml-py): Adds a step to run Estimator in a Pipeline.\n",
    "- [**MpiStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.mpi_step.mpistep?view=azure-ml-py): Adds a step to run a MPI job in a Pipeline.\n",
    "- [**AutoMLStep**](https://docs.microsoft.com/en-us/python/api/azureml-train-automl/azureml.train.automl.automlstep?view=azure-ml-py): Creates a AutoML step in a Pipeline.\n",
    "\n",
    "The following code will create a PythonScriptStep to be executed in the Azure Machine Learning Compute we created above using train.py, one of the files already made available in the `source_directory`.\n",
    "\n",
    "A **PythonScriptStep** is a basic, built-in step to run a Python Script on a compute target. It takes a script name and optionally other parameters like arguments for the script, compute target, inputs and outputs. If no compute target is specified, default compute target for the workspace is used. You can also use a [**RunConfiguration**](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.runconfiguration?view=azure-ml-py) to specify requirements for the PythonScriptStep, such as conda dependencies and docker image.\n",
    "> The best practice is to use separate folders for scripts and its dependent files for each step and specify that folder as the `source_directory` for the step. This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted). Since changes in any files in the `source_directory` would trigger a re-upload of the snapshot, this helps keep the reuse of the step when there are no changes in the `source_directory` of the step."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses default values for PythonScriptStep construct.\n",
    "\n",
    "source_directory = './train'\n",
    "print('Source directory for the step is {}.'.format(os.path.realpath(source_directory)))\n",
    "\n",
    "# Syntax\n",
    "# PythonScriptStep(\n",
    "#     script_name, \n",
    "#     name=None, \n",
    "#     arguments=None, \n",
    "#     compute_target=None, \n",
    "#     runconfig=None, \n",
    "#     inputs=None, \n",
    "#     outputs=None, \n",
    "#     params=None, \n",
    "#     source_directory=None, \n",
    "#     allow_reuse=True, \n",
    "#     version=None, \n",
    "#     hash_paths=None)\n",
    "# This returns a Step\n",
    "step1 = PythonScriptStep(name=\"train_step\",\n",
    "                         script_name=\"train.py\", \n",
    "                         compute_target=aml_compute, \n",
    "                         source_directory=source_directory,\n",
    "                         allow_reuse=True)\n",
    "print(\"Step1 created\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In the above call to PythonScriptStep(), the flag *allow_reuse* determines whether the step should reuse previous results when run with the same settings/inputs. This flag's default value is *True*; the default is set to *True* because, when inputs and parameters have not changed, we typically do not want to re-run a given pipeline step. \n",
    "\n",
    "If *allow_reuse* is set to *False*, a new run will always be generated for this step during pipeline execution. The *allow_reuse* flag can come in handy in situations where you do *not* want to re-run a pipeline step."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a few steps in parallel\n",
    "Here we are looking at a simple scenario where we are running a few steps (all involving PythonScriptStep)  in parallel. Running nodes in **parallel** is the default behavior for steps in a pipeline.\n",
    "\n",
    "We already have one step defined earlier. Let's define few more steps. For step3, we are using customized conda-dependency, and job might fail when \"azureml-defaults\" (or other meta package) is not in pip-package list. We need to be aware if we are not using any of the meta packages (azureml-sdk, azureml-defaults, azureml-core), and we recommend installing \"azureml-defaults\"."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this step, we use a different source_directory\n",
    "source_directory = './compare'\n",
    "print('Source directory for the step is {}.'.format(os.path.realpath(source_directory)))\n",
    "\n",
    "# All steps use the same Azure Machine Learning compute target as well\n",
    "step2 = PythonScriptStep(name=\"compare_step\",\n",
    "                         script_name=\"compare.py\", \n",
    "                         compute_target=aml_compute, \n",
    "                         source_directory=source_directory)\n",
    "\n",
    "# Use a RunConfiguration to specify some additional requirements for this step.\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "\n",
    "# create a new runconfig object\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "# enable Docker \n",
    "run_config.environment.docker.enabled = True\n",
    "\n",
    "# set Docker base image to the default CPU-based image\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "\n",
    "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# specify CondaDependencies obj\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['scikit-learn'])\n",
    "\n",
    "# For this step, we use yet another source_directory\n",
    "source_directory = './extract'\n",
    "print('Source directory for the step is {}.'.format(os.path.realpath(source_directory)))\n",
    "\n",
    "step3 = PythonScriptStep(name=\"extract_step\",\n",
    "                         script_name=\"extract.py\", \n",
    "                         compute_target=aml_compute, \n",
    "                         source_directory=source_directory,\n",
    "                         runconfig=run_config)\n",
    "\n",
    "# list of steps to run\n",
    "steps = [step1, step2, step3]\n",
    "print(\"Step lists created\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the pipeline\n",
    "Once we have the steps (or steps collection), we can build the [pipeline](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py). By deafult, all these steps will run in **parallel** once we submit the pipeline for run.\n",
    "\n",
    "A pipeline is created with a list of steps and a workspace. Submit a pipeline using [submit](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment(class)?view=azure-ml-py#submit-config--tags-none----kwargs-). When submit is called, a [PipelineRun](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinerun?view=azure-ml-py) is created which in turn creates [StepRun](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.steprun?view=azure-ml-py) objects for each step in the workflow."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax\n",
    "# Pipeline(workspace, \n",
    "#          steps, \n",
    "#          description=None, \n",
    "#          default_datastore_name=None, \n",
    "#          default_source_directory=None, \n",
    "#          resolve_closure=True, \n",
    "#          _workflow_provider=None, \n",
    "#          _service_endpoint=None)\n",
    "\n",
    "pipeline1 = Pipeline(workspace=ws, steps=steps)\n",
    "print (\"Pipeline is built\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the pipeline\n",
    "You have the option to [validate](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py#validate--) the pipeline prior to submitting for run. The platform runs validation steps such as checking for circular dependencies and parameter checks etc. even if you do not explicitly call validate method."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1.validate()\n",
    "print(\"Pipeline validation complete\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the pipeline\n",
    "[Submitting](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py#submit) the pipeline involves creating an [Experiment](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment?view=azure-ml-py) object and providing the built pipeline for submission. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit syntax\n",
    "# submit(experiment_name, \n",
    "#        pipeline_parameters=None, \n",
    "#        continue_on_step_failure=False, \n",
    "#        regenerate_outputs=False)\n",
    "\n",
    "pipeline_run1 = Experiment(ws, 'Hello_World1').submit(pipeline1, regenerate_outputs=False)\n",
    "print(\"Pipeline is submitted for execution\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If regenerate_outputs is set to True, a new submit will always force generation of all step outputs, and disallow data reuse for any step of this run. Once this run is complete, however, subsequent runs may reuse the results of this run.\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the pipeline run\n",
    "\n",
    "#### Use RunDetails Widget\n",
    "We are going to use the RunDetails widget to examine the run of the pipeline. You can click each row below to get more details on the step runs."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(pipeline_run1).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Pipeline SDK objects\n",
    "You can cycle through the node_run objects and examine job logs, stdout, and stderr of each of the steps."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_runs = pipeline_run1.get_children()\n",
    "for step_run in step_runs:\n",
    "    status = step_run.get_status()\n",
    "    print('Script:', step_run.name, 'status:', status)\n",
    "    \n",
    "    # Change this if you want to see details even if the Step has succeeded.\n",
    "    if status == \"Failed\":\n",
    "        joblog = step_run.get_job_log()\n",
    "        print('job log:', joblog)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get additonal run details\n",
    "If you wait until the pipeline_run is finished, you may be able to get additional details on the run. **Since this is a blocking call, the following code is commented out.**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline_run1.wait_for_completion()\n",
    "#for step_run in pipeline_run1.get_children():\n",
    "#    print(\"{}: {}\".format(step_run.name, step_run.get_metrics()))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a few steps in sequence\n",
    "Now let's see how we run a few steps in sequence. We already have three steps defined earlier. Let's *reuse* those steps for this part.\n",
    "\n",
    "We will reuse step1, step2, step3, but build the pipeline in such a way that we chain step3 after step2 and step2 after step1. Note that there is no explicit data dependency between these steps, but still steps can be made dependent by using the [run_after](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.builder.pipelinestep?view=azure-ml-py#run-after-step-) construct."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step2.run_after(step1)\n",
    "step3.run_after(step2)\n",
    "\n",
    "# Try a loop\n",
    "#step2.run_after(step3)\n",
    "\n",
    "# Now, construct the pipeline using the steps.\n",
    "\n",
    "# We can specify the \"final step\" in the chain, \n",
    "# Pipeline will take care of \"transitive closure\" and \n",
    "# figure out the implicit or explicit dependencies\n",
    "# https://www.geeksforgeeks.org/transitive-closure-of-a-graph/\n",
    "pipeline2 = Pipeline(workspace=ws, steps=[step3])\n",
    "print (\"Pipeline is built\")\n",
    "\n",
    "pipeline2.validate()\n",
    "print(\"Simple validation complete\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run2 = Experiment(ws, 'Hello_World2').submit(pipeline2)\n",
    "print(\"Pipeline is submitted for execution\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(pipeline_run2).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: Pipelines with data dependency\n",
    "The next [notebook](https://aka.ms/pl-data-dep) demostrates how to construct a pipeline with data dependency."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Machine Learning Pipeline with AutoMLStep\n",
    "This notebook demonstrates the use of AutoMLStep in Azure Machine Learning Pipeline."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this example we showcase how you can use AzureML Dataset to load data for AutoML via AML Pipeline. \n",
    "\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you have executed the [configuration](https://aka.ms/pl-config) before running this notebook, please also take a look at the [Automated ML setup-using-a-local-conda-environment](https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/automated-machine-learning#setup-using-a-local-conda-environment) section to setup the environment.\n",
    "\n",
    "In this notebook you will learn how to:\n",
    "1. Create an `Experiment` in an existing `Workspace`.\n",
    "2. Create or Attach existing AmlCompute to a workspace.\n",
    "3. Define data loading in a `TabularDataset`.\n",
    "4. Configure AutoML using `AutoMLConfig`.\n",
    "5. Use AutoMLStep\n",
    "6. Train the model using AmlCompute\n",
    "7. Explore the results.\n",
    "8. Test the best fitted model."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Machine Learning and Pipeline SDK-specific imports"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import pkg_resources\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "from azureml.pipeline.steps import AutoMLStep\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "Initialize a workspace object from persisted configuration. Make sure the config file is present at .\\config.json"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Azure ML experiment\n",
    "Let's create an experiment named \"automlstep-classification\" and a folder to hold the training scripts. The script runs will be recorded under the experiment in Azure.\n",
    "\n",
    "The best practice is to use separate folders for scripts and its dependent files for each step and specify that folder as the `source_directory` for the step. This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted). Since changes in any files in the `source_directory` would trigger a re-upload of the snapshot, this helps keep the reuse of the step when there are no changes in the `source_directory` of the step."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a name for the run history container in the workspace.\n",
    "experiment_name = 'automlstep-classification'\n",
    "project_folder = './project'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "experiment"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Attach an AmlCompute cluster\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for your AutoML run. In this tutorial, you get the default `AmlCompute` as your training compute resource."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "amlcompute_cluster_name = \"cpu-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',# for GPU, use \"STANDARD_NC6\"\n",
    "                                                           #vm_priority = 'lowpriority', # optional\n",
    "                                                           max_nodes=4)\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True, min_node_count = 1, timeout_in_minutes = 10)\n",
    "# For a more detailed view of current AmlCompute status, use get_status()."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load the dataset from the Workspace. Otherwise, create it from the file\n",
    "found = False\n",
    "key = \"Crime-Dataset\"\n",
    "description_text = \"Crime Dataset (used in the the aml-pipelines-with-automated-machine-learning-step.ipynb notebook)\"\n",
    "\n",
    "if key in ws.datasets.keys(): \n",
    "        found = True\n",
    "        dataset = ws.datasets[key] \n",
    "\n",
    "if not found:\n",
    "        # Create AML Dataset and register it into Workspace\n",
    "        # The data referenced here was a 1MB simple random sample of the Chicago Crime data into a local temporary directory.\n",
    "        example_data = 'https://dprepdata.blob.core.windows.net/demo/crime0-random.csv'\n",
    "        dataset = Dataset.Tabular.from_delimited_files(example_data)\n",
    "        dataset = dataset.drop_columns(['FBI Code'])\n",
    "        \n",
    "        #Register Dataset in Workspace\n",
    "        dataset = dataset.register(workspace=ws,\n",
    "                                   name=key,\n",
    "                                   description=description_text)\n",
    "\n",
    "\n",
    "df = dataset.to_pandas_dataframe()\n",
    "df.describe()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the Dataset Result\n",
    "\n",
    "You can peek the result of a TabularDataset at any range using `skip(i)` and `take(j).to_pandas_dataframe()`. Doing so evaluates only `j` records for all the steps in the TabularDataset, which makes it fast even against large datasets.\n",
    "\n",
    "`TabularDataset` objects are composed of a list of transformation steps (optional)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.take(5).to_pandas_dataframe()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "This creates a general AutoML settings object."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    \"experiment_timeout_minutes\": 20,\n",
    "    \"max_concurrent_iterations\": 4,\n",
    "    \"primary_metric\" : 'AUC_weighted'\n",
    "}\n",
    "automl_config = AutoMLConfig(compute_target=compute_target,\n",
    "                             task = \"classification\",\n",
    "                             training_data=dataset,\n",
    "                             label_column_name=\"Primary Type\",   \n",
    "                             path = project_folder,\n",
    "                             enable_early_stopping= True,\n",
    "                             featurization= 'auto',\n",
    "                             debug_log = \"automl_errors.log\",\n",
    "                             **automl_settings\n",
    "                            )"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Pipeline and AutoMLStep\n",
    "\n",
    "You can define outputs for the AutoMLStep using TrainingOutput."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData, TrainingOutput\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "metrics_output_name = 'metrics_output'\n",
    "best_model_output_name = 'best_model_output'\n",
    "\n",
    "metrics_data = PipelineData(name='metrics_data',\n",
    "                           datastore=ds,\n",
    "                           pipeline_output_name=metrics_output_name,\n",
    "                           training_output=TrainingOutput(type='Metrics'))\n",
    "model_data = PipelineData(name='model_data',\n",
    "                           datastore=ds,\n",
    "                           pipeline_output_name=best_model_output_name,\n",
    "                           training_output=TrainingOutput(type='Model'))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an AutoMLStep."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "automlstep-remarks-sample1"
    ]
   },
   "outputs": [],
   "source": [
    "automl_step = AutoMLStep(\n",
    "    name='automl_module',\n",
    "    automl_config=automl_config,\n",
    "    outputs=[metrics_data, model_data],\n",
    "    allow_reuse=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "automlstep-remarks-sample2"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "pipeline = Pipeline(\n",
    "    description=\"pipeline_with_automlstep\",\n",
    "    workspace=ws,    \n",
    "    steps=[automl_step])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = experiment.submit(pipeline)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Results\n",
    "\n",
    "### Retrieve the metrics of all child runs\n",
    "Outputs of above run can be used as inputs of other steps in pipeline. In this tutorial, we will examine the outputs by retrieve output data and running some tests."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_output = pipeline_run.get_pipeline_output(metrics_output_name)\n",
    "num_file_downloaded = metrics_output.download('.', show_progress=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(metrics_output._path_on_datastore) as f:\n",
    "    metrics_output_result = f.read()\n",
    "    \n",
    "deserialized_metrics_output = json.loads(metrics_output_result)\n",
    "df = pd.DataFrame(deserialized_metrics_output)\n",
    "df"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Best Model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve best model from Pipeline Run\n",
    "best_model_output = pipeline_run.get_pipeline_output(best_model_output_name)\n",
    "num_file_downloaded = best_model_output.download('.', show_progress=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(best_model_output._path_on_datastore, \"rb\" ) as f:\n",
    "    best_model = pickle.load(f)\n",
    "best_model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.steps"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "#### Load Test Data\n",
    "For the test data, it should have the same preparation step as the train data. Otherwise it might get failed at the preprocessing step."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = Dataset.Tabular.from_delimited_files(path='https://dprepdata.blob.core.windows.net/demo/crime0-test.csv')\n",
    "df_test = dataset_test.to_pandas_dataframe()\n",
    "df_test = df_test[pd.notnull(df_test['Primary Type'])]\n",
    "\n",
    "y_test = df_test['Primary Type']\n",
    "X_test = df_test.drop(['Primary Type', 'FBI Code'], axis=1)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Our Best Fitted Model\n",
    "\n",
    "We will use confusion matrix to see how our model works."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "ypred = best_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, ypred)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the confusion matrix\n",
    "pd.DataFrame(cm).style.background_gradient(cmap='Blues', low=0, high=0.9)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep-r.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-estimatorstep.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep-r.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use CommandStep in Azure ML Pipelines\n",
    "\n",
    "This notebook shows how to use the CommandStep with Azure Machine Learning Pipelines for running R scripts in a pipeline.\n",
    "\n",
    "The example shows training a model in R to predict probability of fatality for vehicle crashes.\n",
    "\n",
    "\n",
    "## Prerequisite:\n",
    "* Understand the [architecture and terms](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning\n",
    "* If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration notebook](https://aka.ms/pl-config) to:\n",
    "    * install the Azure ML SDK\n",
    "    * create a workspace and its configuration file (`config.json`)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep-r.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started. First let's import some Python libraries."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep-r.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep-r.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep-r.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep-r.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Attach existing AmlCompute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training your model. In this tutorial, you create `AmlCompute` as your training compute resource."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep-r.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we could not find the cluster with the given name, then we will create a new cluster here. We will create an `AmlCompute` cluster of `STANDARD_D2_V2` CPU VMs. This process is broken down into 3 steps:\n",
    "1. create the configuration (this step is local and only takes a second)\n",
    "2. create the cluster (this step will take about **20 seconds**)\n",
    "3. provision the VMs to bring the cluster to the initial size (of 1 in this case). This step will take about **3-5 minutes** and is providing only sparse output in the process. Please make sure to wait until the call returns before moving to the next cell"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep-r.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"cpu-cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2', max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it uses the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep-r.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have created the compute target, let's see what the workspace's `compute_targets` property returns. You should now see one entry named 'cpu-cluster' of type `AmlCompute`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep-r.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a CommandStep\n",
    "CommandStep adds a step to run a command in a Pipeline. For the full set of configurable options see the CommandStep [reference docs](https://docs.microsoft.com/python/api/azureml-pipeline-steps/azureml.pipeline.steps.commandstep?view=azure-ml-py).\n",
    "\n",
    "- **name:** Name of the step\n",
    "- **runconfig:** ScriptRunConfig object. You can configure a ScriptRunConfig object as you would for a standalone non-pipeline run and pass it in to this parameter. If using this option, you do not have to specify the `command`, `source_directory`, `compute_target` parameters of the CommandStep constructor as they are already defined in your ScriptRunConfig.\n",
    "- **runconfig_pipeline_params:** Override runconfig properties at runtime using key-value pairs each with name of the runconfig property and PipelineParameter for that property\n",
    "- **command:** The command to run or path of the executable/script relative to `source_directory`. It is required unless the `runconfig` parameter is specified. It can be specified with string arguments in a single string or with input/output/PipelineParameter in a list.\n",
    "- **source_directory:** A folder containing the script and other resources used in the step.\n",
    "- **compute_target:** Compute target to use \n",
    "- **allow_reuse:** Whether the step should reuse previous results when run with the same settings/inputs. If this is false, a new run will always be generated for this step during pipeline execution.\n",
    "- **version:** Optional version tag to denote a change in functionality for the step\n",
    "\n",
    "> The best practice is to use separate folders for scripts and its dependent files for each step and specify that folder as the `source_directory` for the step. This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted). Since changes in any files in the `source_directory` would trigger a re-upload of the snapshot, this helps keep the reuse of the step when there are no changes in the `source_directory` of the step."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep-r.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure environment\n",
    "\n",
    "Configure the environment for the train step. In this example we will create an environment from the Dockerfile we have included.\n",
    "\n",
    "> Azure ML currently requires Python as an implicit dependency, so Python must installed in your image even if your training script does not have this dependency."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep-r.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "import os\n",
    "\n",
    "src_dir = 'commandstep_r'\n",
    "\n",
    "env = Environment.from_dockerfile(name='r_env', dockerfile=os.path.join(src_dir, 'Dockerfile'))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep-r.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure input training dataset\n",
    "\n",
    "This tutorial uses data from the US National Highway Traffic Safety Administration. This dataset includes data from over 25,000 car crashes in the US, with variables you can use to predict the likelihood of a fatality. We have included an Rdata file that includes the accidents data for analysis.\n",
    "\n",
    "Here we use the workspace's default datastore to upload the training data file (**accidents.Rd**); in practice you can use any datastore you want."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep-r.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = ws.get_default_datastore()\n",
    "data_ref = datastore.upload_files(files=[os.path.join(src_dir, 'accidents.Rd')], target_path='accidentdata')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep-r.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a FileDataset from the data, which will be used as an input to the train step."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep-r.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "dataset = Dataset.File.from_files(datastore.path('accidentdata'))\n",
    "dataset"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep-r.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a ScriptRunConfig that configures the training run. Note that in the `command` we include the input dataset for the training data.\n",
    "\n",
    "> For detailed guidance on how to move data in pipelines for input and output data, see the documentation [Moving data into and between ML pipelines](https://docs.microsoft.com/azure/machine-learning/how-to-move-data-in-out-of-pipelines)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep-r.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "train_config = ScriptRunConfig(source_directory=src_dir,\n",
    "                            command=['Rscript accidents.R --data_folder', dataset.as_mount(), '--output_folder outputs'],\n",
    "                            compute_target=compute_target,\n",
    "                            environment=env)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep-r.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a CommandStep and pass in the ScriptRunConfig object to the `runconfig` parameter."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep-r.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import CommandStep\n",
    "\n",
    "train = CommandStep(name='train', runconfig=train_config)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep-r.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Submit the Pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep-r.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.core import Experiment\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[train])\n",
    "pipeline_run = Experiment(ws, 'r-commandstep-pipeline').submit(pipeline)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep-r.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Run Details"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep-r.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-commandstep-r.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Databricks as a Compute Target from Azure Machine Learning Pipeline\n",
    "To use Databricks as a compute target from [Azure Machine Learning Pipeline](https://aka.ms/pl-concept), a [DatabricksStep](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.databricks_step.databricksstep?view=azure-ml-py) is used. This notebook demonstrates the use of DatabricksStep in Azure Machine Learning Pipeline.\n",
    "\n",
    "The notebook will show:\n",
    "1. Running an arbitrary Databricks notebook that the customer has in Databricks workspace\n",
    "2. Running an arbitrary Python script that the customer has in DBFS\n",
    "3. Running an arbitrary Python script that is available on local computer (will upload to DBFS, and then run in Databricks) \n",
    "4. Running a JAR job that the customer has in DBFS.\n",
    "\n",
    "## Before you begin:\n",
    "\n",
    "1. **Create an Azure Databricks workspace** in the same subscription where you have your Azure Machine Learning workspace. You will need details of this workspace later on to define DatabricksStep. [Click here](https://ms.portal.azure.com/#blade/HubsExtension/Resources/resourceType/Microsoft.Databricks%2Fworkspaces) for more information.\n",
    "2. **Create PAT (access token)**: Manually create a Databricks access token at the Azure Databricks portal. See [this](https://docs.databricks.com/api/latest/authentication.html#generate-a-token) for more information.\n",
    "3. **Add demo notebook to ADB**: This notebook has a sample you can use as is. Launch Azure Databricks attached to your Azure Machine Learning workspace and add a new notebook. \n",
    "4. **Create/attach a Blob storage** for use from ADB"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add demo notebook to ADB Workspace\n",
    "Copy and paste the below code to create a new notebook in your ADB workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# direct access\n",
    "dbutils.widgets.get(\"myparam\")\n",
    "p = getArgument(\"myparam\")\n",
    "print (\"Param -\\'myparam':\")\n",
    "print (p)\n",
    "\n",
    "dbutils.widgets.get(\"input\")\n",
    "i = getArgument(\"input\")\n",
    "print (\"Param -\\'input':\")\n",
    "print (i)\n",
    "\n",
    "dbutils.widgets.get(\"output\")\n",
    "o = getArgument(\"output\")\n",
    "print (\"Param -\\'output':\")\n",
    "print (o)\n",
    "\n",
    "n = i + \"/testdata.txt\"\n",
    "df = spark.read.csv(n)\n",
    "\n",
    "display (df)\n",
    "\n",
    "data = [('value1', 'value2')]\n",
    "df2 = spark.createDataFrame(data)\n",
    "\n",
    "z = o + \"/output.txt\"\n",
    "df2.write.csv(z)\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Machine Learning and Pipeline SDK-specific imports"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core.runconfig import JarLibrary\n",
    "from azureml.core.compute import ComputeTarget, DatabricksCompute\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import DatabricksStep\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration. If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the [configuration Notebook](https://aka.ms/pl-config) first if you haven't."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach Databricks compute target\n",
    "Next, you need to add your Databricks workspace to Azure Machine Learning as a compute target and give it a name. You will use this name to refer to your Databricks workspace compute target inside Azure Machine Learning.\n",
    "\n",
    "- **Resource Group** - The resource group name of your Azure Machine Learning workspace\n",
    "- **Databricks Workspace Name** - The workspace name of your Azure Databricks workspace\n",
    "- **Databricks Access Token** - The access token you created in ADB\n",
    "\n",
    "**The Databricks workspace need to be present in the same subscription as your AML workspace**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "sample-databrickscompute-attach"
    ]
   },
   "outputs": [],
   "source": [
    "# Replace with your account info before running.\n",
    " \n",
    "db_compute_name=os.getenv(\"DATABRICKS_COMPUTE_NAME\", \"<my-databricks-compute-name>\") # Databricks compute name\n",
    "db_resource_group=os.getenv(\"DATABRICKS_RESOURCE_GROUP\", \"<my-db-resource-group>\") # Databricks resource group\n",
    "db_workspace_name=os.getenv(\"DATABRICKS_WORKSPACE_NAME\", \"<my-db-workspace-name>\") # Databricks workspace name\n",
    "db_access_token=os.getenv(\"DATABRICKS_ACCESS_TOKEN\", \"<my-access-token>\") # Databricks access token\n",
    " \n",
    "try:\n",
    "    databricks_compute = DatabricksCompute(workspace=ws, name=db_compute_name)\n",
    "    print('Compute target {} already exists'.format(db_compute_name))\n",
    "except ComputeTargetException:\n",
    "    print('Compute not found, will use below parameters to attach new one')\n",
    "    print('db_compute_name {}'.format(db_compute_name))\n",
    "    print('db_resource_group {}'.format(db_resource_group))\n",
    "    print('db_workspace_name {}'.format(db_workspace_name))\n",
    "    print('db_access_token {}'.format(db_access_token))\n",
    " \n",
    "    config = DatabricksCompute.attach_configuration(\n",
    "        resource_group = db_resource_group,\n",
    "        workspace_name = db_workspace_name,\n",
    "        access_token= db_access_token)\n",
    "    databricks_compute=ComputeTarget.attach(ws, db_compute_name, config)\n",
    "    databricks_compute.wait_for_completion(True)\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Connections with Inputs and Outputs\n",
    "The DatabricksStep supports DBFS, Azure Blob and ADLS for inputs and outputs. You also will need to define a [Secrets](https://docs.azuredatabricks.net/user-guide/secrets/index.html) scope to enable authentication to external data sources such as Blob and ADLS from Databricks.\n",
    "\n",
    "- Databricks documentation on [Azure Blob](https://docs.azuredatabricks.net/spark/latest/data-sources/azure/azure-storage.html)\n",
    "- Databricks documentation on [ADLS](https://docs.databricks.com/spark/latest/data-sources/azure/azure-datalake.html)\n",
    "\n",
    "### Type of Data Access\n",
    "Databricks allows to interact with Azure Blob and ADLS in two ways.\n",
    "- **Direct Access**: Databricks allows you to interact with Azure Blob or ADLS URIs directly. The input or output URIs will be mapped to a Databricks widget param in the Databricks notebook.\n",
    "- **Mounting**: You will be supplied with additional parameters and secrets that will enable you to mount your ADLS or Azure Blob input or output location in your Databricks notebook."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Direct Access: Python sample code\n",
    "If you have a data reference named \"input\" it will represent the URI of the input and you can access it directly in the Databricks python notebook like so:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "dbutils.widgets.get(\"input\")\n",
    "y = getArgument(\"input\")\n",
    "df = spark.read.csv(y)\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mounting: Python sample code for Azure Blob\n",
    "Given an Azure Blob data reference named \"input\" the following widget params will be made available in the Databricks notebook:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# This contains the input URI\n",
    "dbutils.widgets.get(\"input\")\n",
    "myinput_uri = getArgument(\"input\")\n",
    "\n",
    "# How to get the input datastore name inside ADB notebook\n",
    "# This contains the name of a Databricks secret (in the predefined \"amlscope\" secret scope) \n",
    "# that contians an access key or sas for the Azure Blob input (this name is obtained by appending \n",
    "# the name of the input with \"_blob_secretname\". \n",
    "dbutils.widgets.get(\"input_blob_secretname\") \n",
    "myinput_blob_secretname = getArgument(\"input_blob_secretname\")\n",
    "\n",
    "# This contains the required configuration for mounting\n",
    "dbutils.widgets.get(\"input_blob_config\")\n",
    "myinput_blob_config = getArgument(\"input_blob_config\")\n",
    "\n",
    "# Usage\n",
    "dbutils.fs.mount(\n",
    "  source = myinput_uri,\n",
    "  mount_point = \"/mnt/input\",\n",
    "  extra_configs = {myinput_blob_config:dbutils.secrets.get(scope = \"amlscope\", key = myinput_blob_secretname)})\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mounting: Python sample code for ADLS\n",
    "Given an ADLS data reference named \"input\" the following widget params will be made available in the Databricks notebook:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# This contains the input URI\n",
    "dbutils.widgets.get(\"input\") \n",
    "myinput_uri = getArgument(\"input\")\n",
    "\n",
    "# This contains the client id for the service principal \n",
    "# that has access to the adls input\n",
    "dbutils.widgets.get(\"input_adls_clientid\") \n",
    "myinput_adls_clientid = getArgument(\"input_adls_clientid\")\n",
    "\n",
    "# This contains the name of a Databricks secret (in the predefined \"amlscope\" secret scope) \n",
    "# that contains the secret for the above mentioned service principal\n",
    "dbutils.widgets.get(\"input_adls_secretname\") \n",
    "myinput_adls_secretname = getArgument(\"input_adls_secretname\")\n",
    "\n",
    "# This contains the refresh url for the mounting configs\n",
    "dbutils.widgets.get(\"input_adls_refresh_url\") \n",
    "myinput_adls_refresh_url = getArgument(\"input_adls_refresh_url\")\n",
    "\n",
    "# Usage \n",
    "configs = {\"dfs.adls.oauth2.access.token.provider.type\": \"ClientCredential\",\n",
    "           \"dfs.adls.oauth2.client.id\": myinput_adls_clientid,\n",
    "           \"dfs.adls.oauth2.credential\": dbutils.secrets.get(scope = \"amlscope\", key =myinput_adls_secretname),\n",
    "           \"dfs.adls.oauth2.refresh.url\": myinput_adls_refresh_url}\n",
    "\n",
    "dbutils.fs.mount(\n",
    "  source = myinput_uri,\n",
    "  mount_point = \"/mnt/output\",\n",
    "  extra_configs = configs)\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Databricks from Azure Machine Learning Pipeline\n",
    "To use Databricks as a compute target from Azure Machine Learning Pipeline, a DatabricksStep is used. Let's define a datasource (via DataReference), intermediate data (via PipelineData) and a pipeline parameter (via PipelineParameter) to be used in DatabricksStep."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineParameter\n",
    "\n",
    "# Use the default blob storage\n",
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "print('Datastore {} will be used'.format(def_blob_store.name))\n",
    "\n",
    "pipeline_param = PipelineParameter(name=\"my_pipeline_param\", default_value=\"pipeline_param1\")\n",
    "\n",
    "# We are uploading a sample file in the local directory to be used as a datasource\n",
    "def_blob_store.upload_files(files=[\"./testdata.txt\"], target_path=\"dbtest\", overwrite=False)\n",
    "\n",
    "step_1_input = DataReference(datastore=def_blob_store, path_on_datastore=\"dbtest\",\n",
    "                                     data_reference_name=\"input\")\n",
    "\n",
    "step_1_output = PipelineData(\"output\", datastore=def_blob_store)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a DatabricksStep\n",
    "Adds a Databricks notebook as a step in a Pipeline.\n",
    "- ***name:** Name of the Module\n",
    "- **inputs:** List of input connections for data consumed by this step. Fetch this inside the notebook using dbutils.widgets.get(\"input\")\n",
    "- **outputs:** List of output port definitions for outputs produced by this step. Fetch this inside the notebook using dbutils.widgets.get(\"output\")\n",
    "- **existing_cluster_id:** Cluster ID of an existing Interactive cluster on the Databricks workspace. If you are providing this, do not provide any of the parameters below that are used to create a new cluster such as spark_version, node_type, etc.\n",
    "- **spark_version:** Version of spark for the databricks run cluster. default value: 4.0.x-scala2.11\n",
    "- **node_type:** Azure vm node types for the databricks run cluster. default value: Standard_D3_v2\n",
    "- **num_workers:** Specifies a static number of workers for the databricks run cluster\n",
    "- **min_workers:** Specifies a min number of workers to use for auto-scaling the databricks run cluster\n",
    "- **max_workers:** Specifies a max number of workers to use for auto-scaling the databricks run cluster\n",
    "- **spark_env_variables:** Spark environment variables for the databricks run cluster (dictionary of {str:str}). default value: {'PYSPARK_PYTHON': '/databricks/python3/bin/python3'}\n",
    "- **notebook_path:** Path to the notebook in the databricks instance. If you are providing this, do not provide python script related paramaters or JAR related parameters.\n",
    "- **notebook_params:** Parameters  for the databricks notebook (dictionary of {str:str}). Fetch this inside the notebook using dbutils.widgets.get(\"myparam\")\n",
    "- **python_script_path:** The path to the python script in the DBFS or S3. If you are providing this, do not provide python_script_name which is used for uploading script from local machine.\n",
    "- **python_script_params:** Parameters for the python script (list of str)\n",
    "- **main_class_name:** The name of the entry point in a JAR module. If you are providing this, do not provide any python script or notebook related parameters.\n",
    "- **jar_params:** Parameters for the JAR module (list of str)\n",
    "- **python_script_name:** name of a python script on your local machine (relative to source_directory). If you are providing this do not provide python_script_path which is used to execute a remote python script; or any of the JAR or notebook related parameters.\n",
    "- **source_directory:** folder that contains the script and other files\n",
    "- **hash_paths:** list of paths to hash to detect a change in source_directory (script file is always hashed)\n",
    "- **run_name:** Name in databricks for this run\n",
    "- **timeout_seconds:** Timeout for the databricks run\n",
    "- **runconfig:** Runconfig to use. Either pass runconfig or each library type as a separate parameter but do not mix the two\n",
    "- **maven_libraries:** maven libraries for the databricks run\n",
    "- **pypi_libraries:** pypi libraries for the databricks run\n",
    "- **egg_libraries:** egg libraries for the databricks run\n",
    "- **jar_libraries:** jar libraries for the databricks run\n",
    "- **rcran_libraries:** rcran libraries for the databricks run\n",
    "- **compute_target:** Azure Databricks compute\n",
    "- **allow_reuse:** Whether the step should reuse previous results when run with the same settings/inputs\n",
    "- **version:** Optional version tag to denote a change in functionality for the step\n",
    "\n",
    "\\* *denotes required fields*  \n",
    "*You must provide exactly one of num_workers or min_workers and max_workers paramaters*  \n",
    "*You must provide exactly one of databricks_compute or databricks_compute_name parameters*\n",
    "\n",
    "## Use runconfig to specify library dependencies\n",
    "You can use a runconfig to specify the library dependencies for your cluster in Databricks. The runconfig will contain a databricks section as follows:\n",
    "\n",
    "```yaml\n",
    "environment:\n",
    "# Databricks details\n",
    "  databricks:\n",
    "# List of maven libraries.\n",
    "    mavenLibraries:\n",
    "    - coordinates: org.jsoup:jsoup:1.7.1\n",
    "      repo: ''\n",
    "      exclusions:\n",
    "      - slf4j:slf4j\n",
    "      - '*:hadoop-client'\n",
    "# List of PyPi libraries\n",
    "    pypiLibraries:\n",
    "    - package: beautifulsoup4\n",
    "      repo: ''\n",
    "# List of RCran libraries\n",
    "    rcranLibraries:\n",
    "    -\n",
    "# Coordinates.\n",
    "      package: ada\n",
    "# Repo\n",
    "      repo: http://cran.us.r-project.org\n",
    "# List of JAR libraries\n",
    "    jarLibraries:\n",
    "    -\n",
    "# Coordinates.\n",
    "      library: dbfs:/mnt/libraries/library.jar\n",
    "# List of Egg libraries\n",
    "    eggLibraries:\n",
    "    -\n",
    "# Coordinates.\n",
    "      library: dbfs:/mnt/libraries/library.egg\n",
    "```\n",
    "\n",
    "You can then create a RunConfiguration object using this file and pass it as the runconfig parameter to DatabricksStep.\n",
    "```python\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "runconfig = RunConfiguration()\n",
    "runconfig.load(path='<directory_where_runconfig_is_stored>', name='<runconfig_file_name>')\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Running the demo notebook already added to the Databricks workspace\n",
    "Create a notebook in the Azure Databricks workspace, and provide the path to that notebook as the value associated with the environment variable \"DATABRICKS_NOTEBOOK_PATH\". This will then set the variable\u00c2\u00a0notebook_path\u00c2\u00a0when you run the code cell below:\n",
    "\n",
    "your notebook's path in Azure Databricks UI by hovering over to notebook's title. A typical path of notebook looks like this `/Users/example@databricks.com/example`. See [Databricks Workspace](https://docs.azuredatabricks.net/user-guide/workspace.html) to learn about the folder structure.\n",
    "\n",
    "Note: DataPath `PipelineParameter` should be provided in list of inputs. Such parameters can be accessed by the datapath `name`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_path=os.getenv(\"DATABRICKS_NOTEBOOK_PATH\", \"<my-databricks-notebook-path>\") # Databricks notebook path\n",
    "\n",
    "dbNbStep = DatabricksStep(\n",
    "    name=\"DBNotebookInWS\",\n",
    "    inputs=[step_1_input],\n",
    "    outputs=[step_1_output],\n",
    "    num_workers=1,\n",
    "    notebook_path=notebook_path,\n",
    "    notebook_params={'myparam': 'testparam', \n",
    "                     'myparam2': pipeline_param},\n",
    "    run_name='DB_Notebook_demo',\n",
    "    compute_target=databricks_compute,\n",
    "    allow_reuse=True\n",
    ")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and submit the Experiment\n",
    "\n",
    "Note: Default value of `pipeline_param` will be used if different value is not specified in pipeline parameters during submission"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [dbNbStep]\n",
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline_run = Experiment(ws, 'DB_Notebook_demo').submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Run Details"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Running a Python script from DBFS\n",
    "This shows how to run a Python script in DBFS. \n",
    "\n",
    "To complete this, you will need to first upload the Python script in your local machine to DBFS using the [CLI](https://docs.azuredatabricks.net/user-guide/dbfs-databricks-file-system.html). The CLI command is given below:\n",
    "\n",
    "```\n",
    "dbfs cp ./train-db-dbfs.py dbfs:/train-db-dbfs.py\n",
    "```\n",
    "\n",
    "The code in the below cell assumes that you have completed the previous step of uploading the script `train-db-dbfs.py` to the root folder in DBFS.\n",
    "\n",
    "Note: `pipeline_param` will add two values in the python_script_params, a name followed by value. the name will be in this format `--MY_PIPELINE_PARAM`. For example, in the given case, python_script_params will be `[\"arg1\", \"--MY_PIPELINE_PARAM\", \"pipeline_param1\", \"arg2\"]`"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_script_path = os.getenv(\"DATABRICKS_PYTHON_SCRIPT_PATH\", \"<my-databricks-python-script-path>\") # Databricks python script path\n",
    "\n",
    "dbPythonInDbfsStep = DatabricksStep(\n",
    "    name=\"DBPythonInDBFS\",\n",
    "    inputs=[step_1_input],\n",
    "    num_workers=1,\n",
    "    python_script_path=python_script_path,\n",
    "    python_script_params={'arg1', pipeline_param, 'arg2'},\n",
    "    run_name='DB_Python_demo',\n",
    "    compute_target=databricks_compute,\n",
    "    allow_reuse=True\n",
    ")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and submit the Experiment"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [dbPythonInDbfsStep]\n",
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline_run = Experiment(ws, 'DB_Python_demo').submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Run Details"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Running a Python script in Databricks that currenlty is in local computer\n",
    "To run a Python script that is currently in your local computer, follow the instructions below. \n",
    "\n",
    "The commented out code below code assumes that you have `train-db-local.py` in the `source_directory` subdirectory under the current working directory. \n",
    "\n",
    "The best practice is to use separate folders for scripts and its dependent files for each step and specify that folder as the `source_directory` for the step. This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted). Since changes in any files in the `source_directory` would trigger a re-upload of the snapshot, this helps keep the reuse of the step when there are no changes in the `source_directory` of the step.\n",
    "\n",
    "In this case, the Python script will be uploaded first to DBFS, and then the script will be run in Databricks."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_script_name = \"train-db-local.py\"\n",
    "source_directory = \"./databricks_train\"\n",
    "\n",
    "dbPythonInLocalMachineStep = DatabricksStep(\n",
    "    name=\"DBPythonInLocalMachine\",\n",
    "    inputs=[step_1_input],\n",
    "    num_workers=1,\n",
    "    python_script_name=python_script_name,\n",
    "    source_directory=source_directory,\n",
    "    run_name='DB_Python_Local_demo',\n",
    "    compute_target=databricks_compute,\n",
    "    allow_reuse=True\n",
    ")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and submit the Experiment"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [dbPythonInLocalMachineStep]\n",
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline_run = Experiment(ws, 'DB_Python_Local_demo').submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Run Details"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Running a JAR job that is alreay added in DBFS\n",
    "To run a JAR job that is already uploaded to DBFS, follow the instructions below. You will first upload the JAR file to DBFS using the [CLI](https://docs.azuredatabricks.net/user-guide/dbfs-databricks-file-system.html).\n",
    "\n",
    "The commented out code in the below cell assumes that you have uploaded `train-db-dbfs.jar` to the root folder in DBFS. You can upload `train-db-dbfs.jar` to the root folder in DBFS using this commandline so you can use `jar_library_dbfs_path = \"dbfs:/train-db-dbfs.jar\"`:\n",
    "\n",
    "```\n",
    "dbfs cp ./train-db-dbfs.jar dbfs:/train-db-dbfs.jar\n",
    "```\n",
    "\n",
    "Note: `pipeline_param` will add two values in the python_script_params, a name followed by value. the name will be in this format `--MY_PIPELINE_PARAM`. For example, in the given case, python_script_params will be `[\"arg1\", \"--MY_PIPELINE_PARAM\", \"pipeline_param1\", \"arg2\"]`"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_jar_class_name = \"com.microsoft.aeva.Main\"\n",
    "jar_library_dbfs_path = os.getenv(\"DATABRICKS_JAR_LIB_PATH\", \"<my-databricks-jar-lib-path>\") # Databricks jar library path\n",
    "\n",
    "dbJarInDbfsStep = DatabricksStep(\n",
    "    name=\"DBJarInDBFS\",\n",
    "    inputs=[step_1_input],\n",
    "    num_workers=1,\n",
    "    main_class_name=main_jar_class_name,\n",
    "    jar_params={'arg1', pipeline_param, 'arg2'},\n",
    "    run_name='DB_JAR_demo',\n",
    "    jar_libraries=[JarLibrary(jar_library_dbfs_path)],\n",
    "    compute_target=databricks_compute,\n",
    "    allow_reuse=True\n",
    ")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and submit the Experiment"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [dbJarInDbfsStep]\n",
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline_run = Experiment(ws, 'DB_JAR_demo').submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Run Details"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: ADLA as a Compute Target\n",
    "To use ADLA as a compute target from Azure Machine Learning Pipeline, a AdlaStep is used. This [notebook](https://aka.ms/pl-adla) demonstrates the use of AdlaStep in Azure Machine Learning Pipeline."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# How to create Module, ModuleVersion, and use them in a pipeline with ModuleStep.\n",
    "In this notebook, we introduce the concept of versioned modules and how to use them in an Azure Machine Learning Pipeline.\n",
    "\n",
    "The core idea behind introducing Module, ModuleVersion and ModuleStep is to allow the separation between reusable executable components and their actual usage. These reusable software components (such as scripts or executables) can be used in different scenarios and by different users. This follows the same idea of separating software frameworks/libraries and their actual usage in applications. Module and ModuleVersion take the role of the reusable executable components where ModuleStep is there to link them to an actual usage.\n",
    "\n",
    "A module is an elaborated container of its versions, where each version is the actual computational unit. It is up to users to define the semantics of this hierarchical structure of container and versions. For example, there could be different versions for different use cases, development progress, etc.\n",
    "\n",
    "Each ModuleVersion may have inputs, outputs and rely on parameters and its environment configuration to operate.\n",
    "\n",
    "Because Modules can now be separated from execution in a pipeline, there's a need for a mechanism to reconnect these again, and allow using Modules and their versions in a Pipeline. This is done using a new kind of Step called ModuleStep, which allows embedding a Module (and more precisely, a version of it) in a Pipeline.\n",
    " \n",
    "This notebook shows the usage of a module that computes the sum and product of two numbers. As a module can only be used as a step in a pipeline, we define two different versions for it, to be used in two different use cases:\n",
    "\n",
    "1) As the module powering the initial step of a pipeline, where the step does not receive any input from preceding steps.\n",
    "\n",
    "2) As a module powering a step in the pipeline that receives inputs from preceding steps.\n",
    "\n",
    "Once these two versions are defined, we show how to embed them as steps in the pipeline."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and AML Basics\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the [configuration Notebook](https://aka.ms/pl-config) first if you haven't. This sets you up with a working config file that has information on your workspace, subscription id, etc.\n",
    "\n",
    "### Initialization Steps"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Datastore, RunConfiguration\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PipelineParameter\n",
    "from azureml.pipeline.core.graph import InputPortDef, OutputPortDef\n",
    "from azureml.pipeline.core.module import Module\n",
    "from azureml.pipeline.steps import ModuleStep\n",
    "\n",
    "workspace = Workspace.from_config()\n",
    "print(workspace.name, workspace.resource_group, workspace.location, workspace.subscription_id, sep = '\\n')\n",
    "\n",
    "aml_compute_target = \"cpu-cluster\"\n",
    "try:\n",
    "    aml_compute = AmlCompute(workspace, aml_compute_target)\n",
    "    print(\"Found existing compute target: {}\".format(aml_compute_target))\n",
    "except:\n",
    "    print(\"Creating new compute target: {}\".format(aml_compute_target))\n",
    "    \n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\",\n",
    "                                                                min_nodes = 1, \n",
    "                                                                max_nodes = 4)    \n",
    "    aml_compute = ComputeTarget.create(workspace, aml_compute_target, provisioning_config)\n",
    "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "datastore = Datastore(workspace=workspace, name=\"workspaceblobstore\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Module"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Module is a container that manages computational units. Each such computational unit is a version of the module, and is called a ModuleVersion. We start by either creating a module or fetching an existing one by its ID or by its name."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = Module.create(workspace, name=\"AddAndMultiply\", description=\"A module that adds and multiplies\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation entry ModuleVersion"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ModuleVersion is an actual computational unit. Defining it involves defining its inputs, outputs, the computation and other configuration items. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define that this version is to be used at the beginning of the pipeline, hence does not have incoming ports, only outgoing."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "module-remarks-sample"
    ]
   },
   "outputs": [],
   "source": [
    "out_sum = OutputPortDef(name=\"out_sum\", default_datastore_name=datastore.name, default_datastore_mode=\"mount\", \n",
    "                        label=\"Sum of two numbers\")\n",
    "out_prod = OutputPortDef(name=\"out_prod\", default_datastore_name=datastore.name, default_datastore_mode=\"mount\", \n",
    "                         label=\"Product of two numbers\")\n",
    "entry_version = module.publish_python_script(\"calculate.py\", \"initial\", \n",
    "                                             inputs=[], outputs=[out_sum, out_prod], params = {\"initialNum\":12},\n",
    "                                             version=\"1\", source_directory=\"./calc\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation middle/end ModuleVersion"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another version of the module performs a computation in the middle or at the end of the pipeline. This version has both outputs and inputs, as it is to be either followed by another computation, or emits its outputs."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "moduleversion-remarks-sample"
    ]
   },
   "outputs": [],
   "source": [
    "in1_mid = InputPortDef(name=\"in1\", default_datastore_mode=\"mount\", \n",
    "                   default_data_reference_name=datastore.name, label=\"First input number\")\n",
    "in2_mid = InputPortDef(name=\"in2\", default_datastore_mode=\"mount\", \n",
    "                   default_data_reference_name=datastore.name, label=\"Second input number\")\n",
    "out_sum_mid = OutputPortDef(name=\"out_sum\", default_datastore_name=datastore.name, default_datastore_mode=\"mount\",\n",
    "                            label=\"Sum of two numbers\")\n",
    "out_prod_mid = OutputPortDef(name=\"out_prod\", default_datastore_name=datastore.name, default_datastore_mode=\"mount\",\n",
    "                             label=\"Product of two numbers\")\n",
    "module.publish_python_script(\n",
    "    \"calculate.py\", \"middle\", inputs=[in1_mid, in2_mid], outputs=[out_sum_mid, out_prod_mid], version=\"2\", is_default=True, \n",
    "    source_directory=\"./calc\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Module in a Pipeline with ModuleStep"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a Module, and more precisely, a specific version, in a pipeline is done via a specialized kind of step. This step is called ModuleStep. It is used as a step in a pipeline, one that holds enough information that allows pinpointing to a specific ModuleVersion. \n",
    "\n",
    "Another responsibility of a ModuleStep is to wire the actual data that is used in the pipeline to the inputs/outputs definitions of the ModuleVersion. This wiring is done by mapping each of the inputs and the outputs definitions to a data element in the pipeline. Defining the wiring is done using a dictionary whose keys are the name of the inputs/outputs, and the mapped value is the data element (e.g., a PipelineData object)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deciding which ModuleVersion to use - resolving"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is up to the ModuleStep to decide which ModuleVersion to use. That decision is based on the parameters given to the ModuleStep, and it follows this process:\n",
    "1. If a ModuleVersion object was provided, use it.\n",
    "2. For the given Module object, if a version was provided, use it.\n",
    "3. The given Module object resolves which is the right version:\n",
    "  1. If a default ModuleVersion was defined for the Module, use it.\n",
    "  2. If all the versions of the ModuleVersions in the Module follow semantic versioning, take the one with the highest version.\n",
    "  3. Take the ModuleVersion with the most recent update."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Step and its wires"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The first step in a pipeline does not have incoming inputs, but it does have outputs. For that we'd use the ModuleVersion that was designed for this use case.</p>\n",
    "We start off by preparing the outgoing edges as two PipelineData objects (to be later linked to another step), as well as wiring these to the moduleVersion's outputs by creating a dictionary mapping."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sum = PipelineData(\"sum_out\", datastore=datastore, output_mode=\"mount\",is_directory=False)\n",
    "first_prod = PipelineData(\"prod_out\", datastore=datastore, output_mode=\"mount\",is_directory=False)\n",
    "step_output_wiring = {\"out_sum\":first_sum, \"out_prod\":first_prod}"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial ModuleStep"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In order for the step to know which ModuleVersion to use, we provided the initial ModuleVersion object. We wire the ModuleVersion's outputs with the <i> step_output_wiring</i> map we just created. </p>\n",
    "The initial ModuleStep uses the ModuleVersion that does not have inputs from the pipeline, however, it still needs to receive two numbers to operate upon. We'll provide these numbers as arguments to the step. The first is provided as a parameter, the other one is hard coded."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_num_param = PipelineParameter(name=\"initialNum\", default_value=17)\n",
    "first_step = ModuleStep(module_version=entry_version,\n",
    "                 inputs_map={}, outputs_map=step_output_wiring, \n",
    "                 runconfig=RunConfiguration(), \n",
    "                 compute_target=aml_compute, \n",
    "                 arguments = [\"--output_sum\", first_sum, \n",
    "                              \"--output_product\", first_prod,\n",
    "                              \"--arg_num1\", first_num_param, \n",
    "                              \"--arg_num2\", \"2\"])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second step and its wires"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step in the pipeline receives its inputs from the previous step, and emits its outputs to the next step. Thus the ModuleStep here needs a different kind of ModuleVersion, one that has both inputs and outputs defined for. We have defined such ModuleVersion, and moreover, defined it to be the default version of our Module. This allows us to provide to the ModuleStep the Module object, which would resolve to that default ModuleVersion when needed."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wires"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wiring to the previous step relies on the PipelineData objects we defined before, and for them we create a new dictionary mapping to the ModuleVersion. The wiring to the next step requires us to define another pair of PipelineData objects, for which also a dictionary mapping is needed."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "modulestep-remarks-sample2"
    ]
   },
   "outputs": [],
   "source": [
    "middle_step_input_wiring = {\"in1\":first_sum, \"in2\":first_prod}\n",
    "middle_sum = PipelineData(\"middle_sum\", datastore=datastore, output_mode=\"mount\",is_directory=False)\n",
    "middle_prod = PipelineData(\"middle_prod\", datastore=datastore, output_mode=\"mount\",is_directory=False)\n",
    "middle_step_output_wiring = {\"out_sum\":middle_sum, \"out_prod\":middle_prod}"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Middle ModuleStep - resolving to the default ModuleVersion"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "modulestep-remarks-sample"
    ]
   },
   "outputs": [],
   "source": [
    "middle_step = ModuleStep(module=module,\n",
    "                         inputs_map= middle_step_input_wiring, \n",
    "                         outputs_map= middle_step_output_wiring,\n",
    "                         runconfig=RunConfiguration(), compute_target=aml_compute,\n",
    "                         arguments = [\"--file_num1\", first_sum, \"--file_num2\", first_prod,\n",
    "                                      \"--output_sum\", middle_sum, \"--output_product\", middle_prod])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End step and its wires"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step in the pipeline also has input and outputs, thus its configuration would be similar to the previous step. In this case we would still use Pipeline data as the step's outputs, even though they are not read by any following step, but rather act as the end result of the pipeline."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wires"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_step_input_wiring = {\"in1\":middle_sum, \"in2\":middle_prod}\n",
    "end_sum = PipelineData(\"end_sum\", datastore=datastore, output_mode=\"mount\",is_directory=False)\n",
    "end_prod = PipelineData(\"end_prod\", datastore=datastore, output_mode=\"mount\",is_directory=False)\n",
    "last_step_output_wiring = {\"out_sum\":end_sum, \"out_prod\":end_prod}"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last ModuleStep - specifing the exact version"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_step = ModuleStep(module=module, version=\"2\",\n",
    "                      inputs_map= last_step_input_wiring,\n",
    "                      outputs_map= last_step_output_wiring,\n",
    "                      runconfig=RunConfiguration(), compute_target=aml_compute,\n",
    "                      arguments=[\"--file_num1\", middle_sum, \"--file_num2\", middle_prod,\n",
    "                                 \"--output_sum\", end_sum, \"--output_product\", end_prod])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline, experiment, submission"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing to be done is to create a pipeline out of the previously defined steps, then create an experiment and submit the pipeline to the experiment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=workspace, steps=[first_step, middle_step, end_step])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(workspace, 'testmodulestesp')\n",
    "experiment.submit(pipeline)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-modulestep.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showcasing DataPath and PipelineParameter\n",
    "\n",
    "This notebook demonstrateas the use of [**DataPath**](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.datapath.datapath?view=azure-ml-py) and [**PipelineParameters**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelineparameter?view=azure-ml-py) in AML Pipeline. You will learn how strings and [**DataPath**](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.datapath.datapath?view=azure-ml-py) can be parameterized and submitted to AML Pipelines via [**PipelineParameters**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelineparameter?view=azure-ml-py).\n",
    "To see more about how parameters work between steps, please refer [aml-pipelines-with-data-dependency-steps](https://aka.ms/pl-data-dep).\n",
    "\n",
    "* [How to create a Pipeline with a DataPath PipelineParameter](#index1)\n",
    "* [How to submit a Pipeline with a DataPath PipelineParameter](#index2)\n",
    "* [How to submit a Pipeline and change the DataPath PipelineParameter value from the sdk](#index3)\n",
    "* [How to submit a Pipeline and change the DataPath PipelineParameter value using a REST call](#index4)\n",
    "* [How to create a datastore trigger schedule and use the data_path_parameter_name to get the path of the changed blob in the Pipeline](#index5)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Machine Learning and Pipeline SDK-specific imports"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.data.datapath import DataPath, DataPathComputeBinding\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "from azureml.pipeline.core import PipelineParameter\n",
    "from azureml.pipeline.core import Pipeline, PipelineRun\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration. If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure the config file is present at .\\config.json\n",
    "\n",
    "If you don't have a config.json file, go through the [configuration Notebook](https://aka.ms/pl-config) first.\n",
    "\n",
    "This sets you up with a working config file that has information on your workspace, subscription id, etc."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Azure ML experiment\n",
    "\n",
    "Let's create an experiment named \"showcasing-datapath\" and a folder to hold the training scripts. The script runs will be recorded under the experiment in Azure."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a name for the run history container in the workspace.\n",
    "experiment_name = 'showcasing-datapath'\n",
    "source_directory  = '.'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "experiment"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Attach an AmlCompute cluster\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for your AutoML run. In this tutorial, you get the default `AmlCompute` as your training compute resource."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a name for your cluster.\n",
    "amlcompute_cluster_name = \"cpu-cluster\"\n",
    "\n",
    "found = False\n",
    "# Check if this compute target already exists in the workspace.\n",
    "cts = ws.compute_targets\n",
    "if amlcompute_cluster_name in cts and cts[amlcompute_cluster_name].type == 'AmlCompute':\n",
    "    found = True\n",
    "    print('Found existing compute target.')\n",
    "    compute_target = cts[amlcompute_cluster_name]\n",
    "    \n",
    "if not found:\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\", # for GPU, use \"STANDARD_NC6\"\n",
    "                                                                #vm_priority = 'lowpriority', # optional\n",
    "                                                                max_nodes = 4)\n",
    "\n",
    "    # Create the cluster.\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, provisioning_config)\n",
    "    \n",
    "    # Can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # If no min_node_count is provided, it will use the scale settings for the cluster.\n",
    "    compute_target.wait_for_completion(show_output = True, timeout_in_minutes = 10)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and arguments setup \n",
    "\n",
    "We will setup a trining script to run and its arguments to be used. The sample training script below will print the two arguments to show what has been passed to pipeline."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_with_datapath.py\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "parser = argparse.ArgumentParser(\"train\")\n",
    "parser.add_argument(\"--arg1\", type=str, help=\"sample string argument\")\n",
    "parser.add_argument(\"--arg2\", type=str, help=\"sample datapath argument\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(\"Sample string argument  : %s\" % args.arg1)\n",
    "print(\"Sample datapath argument: %s\" % args.arg2)\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup string and DataPath arguments using PipelineParameter. \n",
    "\n",
    "Note that Pipeline accepts a tuple of the form ([**PipelineParameters**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelineparameter?view=azure-ml-py) , [**DataPathComputeBinding**](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.datapath.datapathcomputebinding?view=azure-ml-py)) as an input. DataPath defines the location of input data. DataPathComputeBinding defines how the data is consumed during step execution. The DataPath can be modified at pipeline submission time with a DataPath parameter, while the compute binding does not change. For static data inputs, we use [**DataReference**](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.data_reference.datareference?view=azure-ml-py) which defines both the data location and compute binding."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "datapath-remarks-sample"
    ]
   },
   "outputs": [],
   "source": [
    "def_blob_store = ws.get_default_datastore()\n",
    "print(\"Default datastore's name: {}\".format(def_blob_store.name))\n",
    "\n",
    "data_path = DataPath(datastore=def_blob_store, path_on_datastore='sample_datapath1')\n",
    "datapath1_pipeline_param = PipelineParameter(name=\"input_datapath\", default_value=data_path)\n",
    "datapath_input = (datapath1_pipeline_param, DataPathComputeBinding(mode='mount'))\n",
    "\n",
    "string_pipeline_param = PipelineParameter(name=\"input_string\", default_value='sample_string1')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='index1'></a>"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Pipeline with a DataPath PipelineParameter\n",
    "\n",
    "Note that the ```datapath_input``` is specified on both arguments and inputs to create a step."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = PythonScriptStep(\n",
    "    name='train_step',\n",
    "    script_name=\"train_with_datapath.py\",\n",
    "    arguments=[\"--arg1\", string_pipeline_param, \"--arg2\", datapath_input],\n",
    "    inputs=[datapath_input],\n",
    "    compute_target=compute_target, \n",
    "    source_directory=source_directory)\n",
    "print(\"train_step created\")\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[train_step])\n",
    "print(\"pipeline with the train_step created\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='index2'></a>"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit a Pipeline with a DataPath PipelineParameter\n",
    "\n",
    "Pipelines can be submitted with default values of PipelineParameters by not specifying any parameters."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = experiment.submit(pipeline)\n",
    "print(\"Pipeline is submitted for execution\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(pipeline_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='index3'></a>"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit a Pipeline and change the DataPath PipelineParameter value from the sdk\n",
    "\n",
    "Or Pipelines can be submitted with values other than default ones by using pipeline_parameters. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run_with_params = experiment.submit(pipeline, \\\n",
    "        pipeline_parameters={'input_datapath': DataPath(datastore=def_blob_store, path_on_datastore='sample_datapath2'),\n",
    "                         'input_string': 'sample_string2'}) "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(pipeline_run_with_params).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run_with_params.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='index4'></a>"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit a Pipeline and change the DataPath PipelineParameter value using a REST call\n",
    "\n",
    "Let's published the pipeline to use the rest endpoint of the published pipeline."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish(name=\"DataPath_Pipeline\", description=\"Pipeline to test Datapath\", continue_on_step_failure=True)\n",
    "published_pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "import requests\n",
    "\n",
    "auth = InteractiveLoginAuthentication()\n",
    "aad_token = auth.get_authentication_header()\n",
    "\n",
    "rest_endpoint = published_pipeline.endpoint\n",
    "\n",
    "print(\"You can perform HTTP POST on URL {} to trigger this pipeline\".format(rest_endpoint))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the param when running the pipeline\n",
    "response = requests.post(rest_endpoint, \n",
    "                         headers=aad_token, \n",
    "                         json={\"ExperimentName\": \"MyRestPipeline\",\n",
    "                               \"RunSource\": \"SDK\",\n",
    "                               \"DataPathAssignments\": {\n",
    "                                   \"input_datapath\": { \n",
    "                                       \"DataStoreName\": def_blob_store.name,\n",
    "                                       \"RelativePath\": 'sample_datapath3'\n",
    "                                   }\n",
    "                               },\n",
    "                               \"ParameterAssignments\": {\"input_string\": \"sample_string3\"}\n",
    "                              }\n",
    "                        )"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response.raise_for_status()\n",
    "except Exception:    \n",
    "    raise Exception('Received bad response from the endpoint: {}\\n'\n",
    "                    'Response Code: {}\\n'\n",
    "                    'Headers: {}\\n'\n",
    "                    'Content: {}'.format(rest_endpoint, response.status_code, response.headers, response.content))\n",
    "\n",
    "run_id = response.json().get('Id')\n",
    "print('Submitted pipeline run: ', run_id)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline_run_via_rest = PipelineRun(ws.experiments[\"MyRestPipeline\"], run_id)\n",
    "RunDetails(published_pipeline_run_via_rest).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline_run_via_rest.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='index5'></a>"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Datastore trigger schedule and use data path parameter\n",
    "\n",
    "When the Pipeline is scheduled with DataPath parameter, it will be triggered by the modified or added data in the DataPath. ```path_on_datastore``` should be a folder and the value of the DataPath will be replaced by the path of the modified data."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Schedule\n",
    "\n",
    "schedule = Schedule.create(workspace=ws, \n",
    "                           name=\"Datastore_trigger_schedule\",\n",
    "                           pipeline_id=published_pipeline.id, \n",
    "                           experiment_name='Scheduled_Pipeline',\n",
    "                           datastore=def_blob_store,\n",
    "                           wait_for_provisioning=True,\n",
    "                           description=\"Datastore trigger schedule demo\",\n",
    "                           path_on_datastore=\"sample_datapath_for_folder\",\n",
    "                           data_path_parameter_name=\"input_datapath\") #Same name as used above to create PipelineParameter\n",
    "\n",
    "print(\"Created schedule with id: {}\".format(schedule.id))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule.disable()\n",
    "schedule"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showcasing Dataset and PipelineParameter\n",
    "\n",
    "This notebook demonstrates how a [**FileDataset**](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.filedataset?view=azure-ml-py) or [**TabularDataset**](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabulardataset?view=azure-ml-py) can be parametrized with [**PipelineParameters**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelineparameter?view=azure-ml-py) in an AML [Pipeline](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipeline(class)?view=azure-ml-py). By parametrizing datasets, you can dynamically run pipeline experiments with different datasets without any code change.\n",
    "\n",
    "A common use case is building a training pipeline with a sample of your training data for quick iterative development. When you're ready to test and deploy your pipeline at scale, you can pass in your full training dataset to the pipeline experiment without making any changes to your training script. \n",
    " \n",
    "To see more about how parameters work between steps, please refer [aml-pipelines-with-data-dependency-steps](https://aka.ms/pl-data-dep).\n",
    "\n",
    "* [How to create a Pipeline with a Dataset PipelineParameter](#index1)\n",
    "* [How to submit a Pipeline with a Dataset PipelineParameter](#index2)\n",
    "* [How to submit a Pipeline and change the Dataset PipelineParameter value from the sdk](#index3)\n",
    "* [How to submit a Pipeline and change the Dataset PipelineParameter value using a REST call](#index4)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Machine Learning and Pipeline SDK-specific imports"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "from azureml.pipeline.core import PipelineParameter\n",
    "from azureml.pipeline.core import Pipeline, PipelineRun\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration. If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure the config file is present at .\\config.json\n",
    "\n",
    "If you don't have a config.json file, go through the [configuration Notebook](https://aka.ms/pl-config) first.\n",
    "\n",
    "This sets you up with a working config file that has information on your workspace, subscription id, etc."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Azure ML experiment\n",
    "\n",
    "Let's create an experiment named \"showcasing-dataset\" and a folder to hold the training scripts. The script runs will be recorded under the experiment in Azure."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a name for the run history container in the workspace.\n",
    "experiment_name = 'showcasing-dataset'\n",
    "source_directory  = '.'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "experiment"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Attach an AmlCompute cluster\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for your AutoML run. In this tutorial, you get the default `AmlCompute` as your training compute resource."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a name for your cluster.\n",
    "amlcompute_cluster_name = \"cpu-cluster\"\n",
    "\n",
    "found = False\n",
    "# Check if this compute target already exists in the workspace.\n",
    "cts = ws.compute_targets\n",
    "if amlcompute_cluster_name in cts and cts[amlcompute_cluster_name].type == 'AmlCompute':\n",
    "    found = True\n",
    "    print('Found existing compute target.')\n",
    "    compute_target = cts[amlcompute_cluster_name]\n",
    "    \n",
    "if not found:\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\", # for GPU, use \"STANDARD_NC6\"\n",
    "                                                                #vm_priority = 'lowpriority', # optional\n",
    "                                                                max_nodes = 4)\n",
    "\n",
    "    # Create the cluster.\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, provisioning_config)\n",
    "    \n",
    "    # Can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # If no min_node_count is provided, it will use the scale settings for the cluster.\n",
    "    compute_target.wait_for_completion(show_output = True, timeout_in_minutes = 10)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Configuration\n",
    "\n",
    "The following steps detail how to create a [FileDataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.filedataset?view=azure-ml-py) and [TabularDataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabulardataset?view=azure-ml-py) from an external CSV file, and configure them to be used by a [Pipeline](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipeline(class)?view=azure-ml-py):\n",
    "\n",
    "1. Create a dataset from a csv file\n",
    "2. Create a [PipelineParameter](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelineparameter?view=azure-ml-py) object and set the `default_value` to the dataset. [PipelineParameter](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelineparameter?view=azure-ml-py) objects enabled arguments to be passed into Pipelines when they are resubmitted after creation. The `name` is referenced later on when we submit additional pipeline runs with different input datasets. \n",
    "3. Create a [DatasetConsumptionConfig](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.dataset_consumption_config.datasetconsumptionconfig?view=azure-ml-py) object from the [PiepelineParameter](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelineparameter?view=azure-ml-py). The [DatasetConsumptionConfig](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.dataset_consumption_config.datasetconsumptionconfig?view=azure-ml-py) object specifies how the dataset should be used by the remote compute where the pipeline is run. **NOTE** only [DatasetConsumptionConfig](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.dataset_consumption_config.datasetconsumptionconfig?view=azure-ml-py) objects built on [FileDataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.filedataset?view=azure-ml-py) can be set `as_mount()` or `as_download()` on the remote compute."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "datapath-remarks-sample"
    ]
   },
   "outputs": [],
   "source": [
    "file_dataset = Dataset.File.from_files('https://dprepdata.blob.core.windows.net/demo/Titanic.csv')\n",
    "file_pipeline_param = PipelineParameter(name=\"file_ds_param\", default_value=file_dataset)\n",
    "file_ds_consumption = DatasetConsumptionConfig(\"file_dataset\", file_pipeline_param).as_mount()\n",
    "\n",
    "tabular_dataset = Dataset.Tabular.from_delimited_files('https://dprepdata.blob.core.windows.net/demo/Titanic.csv')\n",
    "tabular_pipeline_param = PipelineParameter(name=\"tabular_ds_param\", default_value=tabular_dataset)\n",
    "tabular_ds_consumption = DatasetConsumptionConfig(\"tabular_dataset\", tabular_pipeline_param)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will setup a training script to ingest our passed-in datasets and print their contents. **NOTE** the names of the datasets referenced inside the training script correspond to the `name` of their respective [DatasetConsumptionConfig](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.dataset_consumption_config.datasetconsumptionconfig?view=azure-ml-py) objects we defined above."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_with_dataset.py\n",
    "from azureml.core import Run\n",
    "\n",
    "input_file_ds_path = Run.get_context().input_datasets['file_dataset']\n",
    "with open(input_file_ds_path, 'r') as f:\n",
    "    content = f.read()\n",
    "    print(content)\n",
    "\n",
    "input_tabular_ds = Run.get_context().input_datasets['tabular_dataset']\n",
    "tabular_df = input_tabular_ds.to_pandas_dataframe()\n",
    "print(tabular_df)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='index1'></a>"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Pipeline with a Dataset PipelineParameter\n",
    "\n",
    "Note that the ```file_ds_consumption``` and ```tabular_ds_consumption``` are specified as both arguments and inputs to create a step."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = PythonScriptStep(\n",
    "    name=\"train_step\",\n",
    "    script_name=\"train_with_dataset.py\",\n",
    "    arguments=[\"--param1\", file_ds_consumption, \"--param2\", tabular_ds_consumption],\n",
    "    inputs=[file_ds_consumption, tabular_ds_consumption],\n",
    "    compute_target=compute_target,\n",
    "    source_directory=source_directory)\n",
    "\n",
    "print(\"train_step created\")\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[train_step])\n",
    "print(\"pipeline with the train_step created\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='index2'></a>"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit a Pipeline with a Dataset PipelineParameter\n",
    "\n",
    "Pipelines can be submitted with default values of PipelineParameters by not specifying any parameters."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline will run with default file_ds and tabular_ds\n",
    "pipeline_run = experiment.submit(pipeline)\n",
    "print(\"Pipeline is submitted for execution\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(pipeline_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='index3'></a>"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit a Pipeline with a different Dataset PipelineParameter value from the SDK\n",
    "\n",
    "The training pipeline can be reused with different input datasets by passing them in as PipelineParameters"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_file_ds = Dataset.File.from_files('https://raw.githubusercontent.com/Azure/MachineLearningNotebooks/'\n",
    "                                        '4e7b3784d50e81c313c62bcdf9a330194153d9cd/how-to-use-azureml/work-with-data/'\n",
    "                                        'datasets-tutorial/train-with-datasets/train-dataset/iris.csv')\n",
    "\n",
    "iris_tabular_ds = Dataset.Tabular.from_delimited_files('https://raw.githubusercontent.com/Azure/MachineLearningNotebooks/'\n",
    "                                                       '4e7b3784d50e81c313c62bcdf9a330194153d9cd/how-to-use-azureml/work-with-data/'\n",
    "                                                       'datasets-tutorial/train-with-datasets/train-dataset/iris.csv')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run_with_params = experiment.submit(pipeline, pipeline_parameters={'file_ds_param': iris_file_ds, 'tabular_ds_param': iris_tabular_ds}) "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(pipeline_run_with_params).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run_with_params.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='index4'></a>"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamically Set the Dataset PipelineParameter Values using a REST Call\n",
    "\n",
    "Let's publish the pipeline we created previously, so we can generate a pipeline endpoint. We can then submit the iris datasets to the pipeline REST endpoint by passing in their IDs. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish(name=\"Dataset_Pipeline\", description=\"Pipeline to test Dataset PipelineParameter\", continue_on_step_failure=True)\n",
    "published_pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline.submit(ws, experiment_name=\"publishedexperiment\", pipeline_parameters={'file_ds_param': iris_file_ds, 'tabular_ds_param': iris_tabular_ds})"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "import requests\n",
    "\n",
    "auth = InteractiveLoginAuthentication()\n",
    "aad_token = auth.get_authentication_header()\n",
    "\n",
    "rest_endpoint = published_pipeline.endpoint\n",
    "\n",
    "print(\"You can perform HTTP POST on URL {} to trigger this pipeline\".format(rest_endpoint))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the param when running the pipeline\n",
    "response = requests.post(rest_endpoint, \n",
    "                         headers=aad_token, \n",
    "                         json={\"ExperimentName\": \"MyRestPipeline\",\n",
    "                               \"RunSource\": \"SDK\",\n",
    "                               \"DataSetDefinitionValueAssignments\": {\"file_ds_param\": {\"SavedDataSetReference\": {\"Id\": iris_file_ds.id}},\n",
    "                                                                     \"tabular_ds_param\": {\"SavedDataSetReference\": {\"Id\": iris_tabular_ds.id}}}\n",
    "                              }\n",
    "                        )"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response.raise_for_status()\n",
    "except Exception:    \n",
    "    raise Exception('Received bad response from the endpoint: {}\\n'\n",
    "                    'Response Code: {}\\n'\n",
    "                    'Headers: {}\\n'\n",
    "                    'Content: {}'.format(rest_endpoint, response.status_code, response.headers, response.content))\n",
    "\n",
    "run_id = response.json().get('Id')\n",
    "print('Submitted pipeline run: ', run_id)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline_run_via_rest = PipelineRun(ws.experiments[\"MyRestPipeline\"], run_id)\n",
    "RunDetails(published_pipeline_run_via_rest).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline_run_via_rest.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='index5'></a>"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-kusto-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-kusto-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Machine Learning Pipeline with KustoStep\n",
    "To use Kusto as a compute target from [Azure Machine Learning Pipeline](https://aka.ms/pl-concept), a KustoStep is used. A KustoStep enables the functionality of running Kusto queries on a target Kusto cluster in Azure ML Pipelines. Each KustoStep can target one Kusto cluster and perform multiple queries on them. This notebook demonstrates the use of KustoStep in Azure Machine Learning (AML) Pipeline.\n",
    "\n",
    "## Before you begin:\n",
    "\n",
    "1. **Have an Azure Machine Learning workspace**: You will need details of this workspace later on to define KustoStep.\n",
    "2. **Have a Service Principal**: You will need a service principal and use its credentials to access your cluster. See [this](https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal) for more information.\n",
    "3. **Have a Blob storage**: You will need a Azure Blob storage for uploading the output of your Kusto query."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-kusto-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Machine Learning and Pipeline SDK-specific imports"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-kusto-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core.runconfig import JarLibrary\n",
    "from azureml.core.compute import ComputeTarget, KustoCompute\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import KustoStep\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-kusto-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration. If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the [configuration Notebook](https://aka.ms/pl-config) first if you haven't."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-kusto-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-kusto-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach Kusto compute target\n",
    "Next, you need to create a Kusto compute target and give it a name. You will use this name to refer to your Kusto compute target inside Azure Machine Learning. Your workspace will be associated to this Kusto compute target. You will also need to provide some credentials that will be used to enable access to your target Kusto cluster and database.\n",
    "\n",
    "- **Resource Group** - The resource group name of your Azure Machine Learning workspace\n",
    "- **Workspace Name** - The workspace name of your Azure Machine Learning workspace\n",
    "- **Resource ID** - The resource ID of your Kusto cluster\n",
    "- **Tenant ID** - The tenant ID associated to your Kusto cluster\n",
    "- **Application ID** - The Application ID associated to your Kusto cluster\n",
    "- **Application Key** - The Application key associated to your Kusto cluster\n",
    "- **Kusto Connection String** - The connection string of your Kusto cluster\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-kusto-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "sample-databrickscompute-attach"
    ]
   },
   "outputs": [],
   "source": [
    "compute_name = \"<compute_name>\" # Name to associate with new compute in workspace\n",
    "\n",
    "# Account details associated to the target Kusto cluster\n",
    "resource_id = \"<resource_id>\" # Resource ID of the Kusto cluster\n",
    "kusto_connection_string = \"<kusto_connection_string>\" # Connection string of the Kusto cluster\n",
    "application_id = \"<application_id>\" # Application ID associated to the Kusto cluster\n",
    "application_key = \"<application_key>\" # Application Key associated to the Kusto cluster\n",
    "tenant_id = \"<tenant_id>\" # Tenant ID associated to the Kusto cluster\n",
    "\n",
    "try:\n",
    "    kusto_compute = KustoCompute(workspace=ws, name=compute_name)\n",
    "    print('Compute target {} already exists'.format(compute_name))\n",
    "except ComputeTargetException:\n",
    "    print('Compute not found, will use provided parameters to attach new one')\n",
    "    config = KustoCompute.attach_configuration(resource_group=ws.resource_group, workspace_name=ws.name, \n",
    "                                               resource_id=resource_id, tenant_id=tenant_id, \n",
    "                                               kusto_connection_string=kusto_connection_string, \n",
    "                                               application_id=application_id, application_key=application_key)\n",
    "    kusto_compute=ComputeTarget.attach(ws, compute_name, config)\n",
    "    kusto_compute.wait_for_completion(True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-kusto-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup output\n",
    "To use Kusto as a compute target for Azure Machine Learning Pipeline, a KustoStep is used. Currently KustoStep only supports uploading results to Azure Blob store. Let's define an output datastore via PipelineData to be used in KustoStep."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-kusto-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineParameter\n",
    "\n",
    "# Use the default blob storage\n",
    "def_blob_store = Datastore.get(ws, \"workspaceblobstore\")\n",
    "print('Datastore {} will be used'.format(def_blob_store.name))\n",
    "\n",
    "step_1_output = PipelineData(\"output\", datastore=def_blob_store)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-kusto-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add a KustoStep to Pipeline\n",
    "Adds a Kusto query as a step in a Pipeline.\n",
    "- **name:** Name of the Module\n",
    "- **compute_target:** Name of Kusto compute target\n",
    "- **database_name:** Name of the database to perform Kusto query on\n",
    "- **query_directory:** Path to folder that contains only a text file with Kusto queries (see [here](https://docs.microsoft.com/en-us/azure/data-explorer/kusto/query/) for more details on Kusto queries). \n",
    "    - If the query is parameterized, then the text file must also include any declaration of query parameters (see [here](https://docs.microsoft.com/en-us/azure/data-explorer/kusto/query/queryparametersstatement?pivots=azuredataexplorer) for more details on query parameters declaration statements). \n",
    "    - An example of the query text file could just contain the query \"StormEvents | count | as HowManyRecords;\", where StormEvents is the table name. \n",
    "    - Note. the text file should just contain the declarations and queries without quotation marks around them.\n",
    "- **outputs:** Output binding to an Azure Blob Store.\n",
    "- **parameter_dict (optional):** Dictionary that contains the values of parameters declared in the query text file in the **query_directory** mentioned above.\n",
    "    - Dictionary key is the parameter name, and dictionary value is the parameter value.\n",
    "    - For example, parameter_dict = {\"paramName1\": \"paramValue1\", \"paramName2\": \"paramValue2\"}\n",
    "- **allow_reuse (optional):** Whether the step should reuse previous results when run with the same settings/inputs (default to False)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-kusto-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = \"<database_name>\" # Name of the database to perform Kusto queries on\n",
    "query_directory = \"<query_directory>\" # Path to folder that contains a text file with Kusto queries\n",
    "\n",
    "kustoStep = KustoStep(\n",
    "    name='KustoNotebook',\n",
    "    compute_target=compute_name,\n",
    "    database_name=database_name,\n",
    "    query_directory=query_directory,\n",
    "    output=step_1_output,\n",
    ")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-kusto-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and submit the Experiment"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-kusto-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [kustoStep]\n",
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline_run = Experiment(ws, 'Notebook_demo').submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-kusto-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Run Details"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-kusto-as-compute-target.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-kusto-as-compute-target.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Machine Learning Pipeline with AzureBatchStep"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to demonstrate the use of AzureBatchStep in Azure Machine Learning Pipeline.\n",
    "An AzureBatchStep will submit a job to an AzureBatch Compute to run a simple windows executable."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Machine Learning and Pipeline SDK-specific Imports"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.core.compute import ComputeTarget, BatchCompute\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import AzureBatchStep\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "from tempfile import mkdtemp\n",
    "\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a workspace object from persisted configuration. Make sure the config file is present at .\\config.json\n",
    "\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, If you don't have a config.json file, please go through the [configuration Notebook](https://aka.ms/pl-config) located [here](https://github.com/Azure/MachineLearningNotebooks).  \n",
    "\n",
    "This sets you up with a working config file that has information on your workspace, subscription id, etc. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "print('Workspace Name: ' + ws.name, \n",
    "      'Azure Region: ' + ws.location, \n",
    "      'Subscription Id: ' + ws.subscription_id, \n",
    "      'Resource Group: ' + ws.resource_group, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach Batch Compute to Workspace"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit jobs to Azure Batch service, you must attach your Azure Batch account to the workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "sample-batchcompute-attach"
    ]
   },
   "outputs": [],
   "source": [
    "batch_compute_name = 'mybatchcompute' # Name to associate with new compute in workspace\n",
    "\n",
    "# Batch account details needed to attach as compute to workspace\n",
    "batch_account_name = \"<batch_account_name>\" # Name of the Batch account\n",
    "batch_resource_group = \"<batch_resource_group>\" # Name of the resource group which contains this account\n",
    "\n",
    "try:\n",
    "    # check if already attached\n",
    "    batch_compute = BatchCompute(ws, batch_compute_name)\n",
    "except ComputeTargetException:\n",
    "    print('Attaching Batch compute...')\n",
    "    provisioning_config = BatchCompute.attach_configuration(resource_group=batch_resource_group, \n",
    "                                                            account_name=batch_account_name)\n",
    "    batch_compute = ComputeTarget.attach(ws, batch_compute_name, provisioning_config)\n",
    "    batch_compute.wait_for_completion()\n",
    "    print(\"Provisioning state:{}\".format(batch_compute.provisioning_state))\n",
    "    print(\"Provisioning errors:{}\".format(batch_compute.provisioning_errors))\n",
    "\n",
    "print(\"Using Batch compute:{}\".format(batch_compute.cluster_resource_id))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Datastore"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the Blob storage associated with the workspace.  \n",
    "The following call retrieves the Azure Blob Store associated with your workspace.  \n",
    "Note that workspaceblobstore is **the name of this store and CANNOT BE CHANGED and must be used as is**.  \n",
    "  \n",
    "If you want to register another Datastore, please follow the instructions from here:\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-access-data#register-a-datastore"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = Datastore(ws, \"workspaceblobstore\")\n",
    "\n",
    "print('Datastore details:')\n",
    "print('Datastore Account Name: ' + datastore.account_name)\n",
    "print('Datastore Workspace Name: ' + datastore.workspace.name)\n",
    "print('Datastore Container Name: ' + datastore.container_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Input and Output"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we will upload a file in the provided Datastore. These are some helper methods to achieve that."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_local_file(content, file_name):\n",
    "    # create a file in a local temporary directory\n",
    "    temp_dir = mkdtemp()\n",
    "    with open(path.join(temp_dir, file_name), 'w') as f:\n",
    "        f.write(content)\n",
    "    return temp_dir\n",
    "\n",
    "\n",
    "def upload_file_to_datastore(datastore, file_name, content):\n",
    "    src_dir = create_local_file(content=content, file_name=file_name)\n",
    "    datastore.upload(src_dir=src_dir, overwrite=True, show_progress=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we associate the input DataReference with an existing file in the provided Datastore. Feel free to upload the file of your choice manually or use the *upload_file_to_datastore* method. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name=\"input.txt\"\n",
    "\n",
    "upload_file_to_datastore(datastore=datastore, \n",
    "                         file_name=file_name, \n",
    "                         content=\"this is the content of the file\")\n",
    "\n",
    "testdata = DataReference(datastore=datastore, \n",
    "                         path_on_datastore=file_name, \n",
    "                         data_reference_name=\"input\")\n",
    "\n",
    "outputdata = PipelineData(name=\"output\", datastore=datastore)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup AzureBatch Job Binaries"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AzureBatch can run a task within the job and here we put a simple .cmd file to be executed. Feel free to put any binaries in the folder, or modify the .cmd file as needed, they will be uploaded once we create the AzureBatch Step."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binaries_folder = \"azurebatch/job_binaries\"\n",
    "if not os.path.isdir(binaries_folder):\n",
    "    os.makedirs(binaries_folder)\n",
    "\n",
    "file_name=\"azurebatch.cmd\"\n",
    "with open(path.join(binaries_folder, file_name), 'w') as f:\n",
    "    f.write(\"copy \\\"%1\\\" \\\"%2\\\"\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an AzureBatchStep"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AzureBatchStep is used to submit a job to the attached Azure Batch compute.\n",
    "- **name:** Name of the step\n",
    "- **pool_id:** Name of the pool, it can be an existing pool, or one that will be created when the job is submitted\n",
    "- **inputs:** List of inputs that will be processed by the job\n",
    "- **outputs:** List of outputs the job will create\n",
    "- **executable:** The executable that will run as part of the job\n",
    "- **arguments:** Arguments for the executable. They can be plain string format, inputs, outputs or parameters\n",
    "- **compute_target:** The compute target where the job will run.\n",
    "- **source_directory:** The local directory with binaries to be executed by the job\n",
    "\n",
    "Optional parameters:\n",
    "\n",
    "- **create_pool:** Boolean flag to indicate whether create the pool before running the jobs\n",
    "- **delete_batch_job_after_finish:** Boolean flag to indicate whether to delete the job from Batch account after it's finished\n",
    "- **delete_batch_pool_after_finish:** Boolean flag to indicate whether to delete the pool after the job finishes\n",
    "- **is_positive_exit_code_failure:** Boolean flag to indicate if the job fails if the task exists with a positive code\n",
    "- **vm_image_urn:** If create_pool is true and VM uses VirtualMachineConfiguration.  \n",
    " Value format: 'urn:publisher:offer:sku'.  \n",
    " Example: urn:MicrosoftWindowsServer:WindowsServer:2012-R2-Datacenter  \n",
    " For more details:  \n",
    " https://docs.microsoft.com/en-us/azure/virtual-machines/windows/cli-ps-findimage#table-of-commonly-used-windows-images and  \n",
    " https://docs.microsoft.com/en-us/azure/virtual-machines/linux/cli-ps-findimage#find-specific-images\n",
    "- **run_task_as_admin:** Boolean flag to indicate if the task should run with Admin privileges\n",
    "- **target_compute_nodes:** Assumes create_pool is true, indicates how many compute nodes will be added to the pool\n",
    "- **source_directory:** Local folder that contains the module binaries, executable, assemblies etc.\n",
    "- **executable:** Name of the command/executable that will be executed as part of the job\n",
    "- **arguments:** Arguments for the command/executable\n",
    "- **inputs:** List of input port bindings\n",
    "- **outputs:** List of output port bindings\n",
    "- **vm_size:** If create_pool is true, indicating Virtual machine size of the compute nodes\n",
    "- **compute_target:** BatchCompute compute\n",
    "- **allow_reuse:** Whether the module should reuse previous results when run with the same settings/inputs\n",
    "- **version:** A version tag to denote a change in functionality for the module"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "azurebatchstep-remarks-sample"
    ]
   },
   "outputs": [],
   "source": [
    "step = AzureBatchStep(\n",
    "            name=\"Azure Batch Job\",\n",
    "            pool_id=\"MyPoolName\", # Replace this with the pool name of your choice\n",
    "            inputs=[testdata],\n",
    "            outputs=[outputdata],\n",
    "            executable=\"azurebatch.cmd\",\n",
    "            arguments=[testdata, outputdata],\n",
    "            compute_target=batch_compute,\n",
    "            source_directory=binaries_folder,\n",
    ")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Submit the Pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[step])\n",
    "pipeline_run = Experiment(ws, 'azurebatch_experiment').submit(pipeline)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Running Pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-azurebatch-to-run-a-windows-executable.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Machine Learning Pipeline with NotebookRunnerStep\n",
    "This notebook demonstrates the use of `NotebookRunnerStep`. It allows you to run a local notebook as a step in Azure Machine Learning Pipeline."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this example we showcase how you can run another notebook `notebook_runner/training_notebook.ipynb` as a step in Azure Machine Learning Pipeline.\n",
    "\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you have executed the [configuration](https://aka.ms/pl-config) before running this notebook.\n",
    "\n",
    "In this notebook you will learn how to:\n",
    "1. Create an `Experiment` in an existing `Workspace`.\n",
    "2. Create or Attach existing AmlCompute to a workspace.\n",
    "3. Configure NotebookRun using `NotebokRunConfig`.\n",
    "5. Use NotebookRunnerStep.\n",
    "6. Run the notebook on `AmlCompute` as a pipeline step consuming the output of a python script step.\n",
    "\n",
    "Advantages of running your notebook as a step in pipeline:\n",
    "1. Run your notebook like a python script without converting into .py files, leveraging complete end to end experience of Azure Machine Learning Pipelines.\n",
    "2. Use pipeline intermediate data to and from the notebook along with other steps in pipeline.\n",
    "3. Parameterize your notebook with [Pipeline Parameters](./aml-pipelines-publish-and-run-using-rest-endpoint.ipynb).\n",
    "\n",
    "Try some more [quick start notebooks](https://github.com/microsoft/recommenders/tree/master/notebooks/00_quick_start) with `NotebookRunnerStep`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Machine Learning and Pipeline SDK-specific imports"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import azureml.core\n",
    "\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.core.datastore import Datastore\n",
    "\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.contrib.notebook import NotebookRunConfig, AzureMLNotebookHandler\n",
    "\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.contrib.notebook import NotebookRunnerStep\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Workspace\n",
    "\n",
    "Initialize a [workspace](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace(class%29) object from persisted configuration."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n",
    "ws.set_default_datastore(\"workspaceblobstore\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to datastore"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datastore.get(ws, \"workspaceblobstore\").upload_files([\"./20news.pkl\"], target_path=\"20newsgroups\", overwrite=True)\n",
    "print(\"Upload call completed\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Azure ML experiment\n",
    "Let's create an experiment named \"notebook-step-run-example\" and a folder to holding the notebook and other scripts. The script runs will be recorded under the experiment in Azure.\n",
    "\n",
    "The best practice is to use separate folders for scripts and its dependent files for each step and specify that folder as the `source_directory` for the step. This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted). Since changes in any files in the `source_directory` would trigger a re-upload of the snapshot, this helps keep the reuse of the step when there are no changes in the `source_directory` of the step."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a name for the run history container in the workspace.\n",
    "experiment_name = 'notebook-step-run-example'\n",
    "source_directory = 'notebook_runner'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "experiment"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Attach an AmlCompute cluster\n",
    "You will need to create a [compute target](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.computetarget?view=azure-ml-py) for your remote run. In this tutorial, you get the default `AmlCompute` as your training compute resource."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a name for your cluster.\n",
    "amlcompute_cluster_name = \"cpu-cluster\"\n",
    "\n",
    "found = False\n",
    "# Check if this compute target already exists in the workspace.\n",
    "cts = ws.compute_targets\n",
    "if amlcompute_cluster_name in cts and cts[amlcompute_cluster_name].type == 'AmlCompute':\n",
    "    found = True\n",
    "    print('Found existing compute target.')\n",
    "    compute_target = cts[amlcompute_cluster_name]\n",
    "    \n",
    "if not found:\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\", # for GPU, use \"STANDARD_NC6\"\n",
    "                                                                #vm_priority = 'lowpriority', # optional\n",
    "                                                                max_nodes = 4)\n",
    "\n",
    "    # Create the cluster.\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, provisioning_config)\n",
    "    \n",
    "    # Can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # If no min_node_count is provided, it will use the scale settings for the cluster.\n",
    "    compute_target.wait_for_completion(show_output = True, min_node_count = 1, timeout_in_minutes = 10)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new RunConfig object"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "conda_run_config = RunConfiguration(framework=\"python\")\n",
    "\n",
    "conda_run_config.environment.docker.enabled = True\n",
    "conda_run_config.environment.docker.base_image = azureml.core.runconfig.DEFAULT_CPU_IMAGE\n",
    "\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-sdk'])\n",
    "conda_run_config.environment.python.conda_dependencies = cd\n",
    "\n",
    "print('run config is ready')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define input and outputs"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = DataReference(\n",
    "    datastore=Datastore.get(ws, \"workspaceblobstore\"),\n",
    "    data_reference_name=\"blob_test_data\",\n",
    "    path_on_datastore=\"20newsgroups/20news.pkl\")\n",
    "\n",
    "output_data = PipelineData(name=\"processed_data\",\n",
    "                           datastore=Datastore.get(ws, \"workspaceblobstore\"))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create notebook run configuration and set parameters values"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = AzureMLNotebookHandler(timeout=600, progress_bar=False, log_output=True)\n",
    "\n",
    "cfg = NotebookRunConfig(source_directory=source_directory, notebook=\"training_notebook.ipynb\",\n",
    "                        handler = handler,\n",
    "                        parameters={\"arg1\": \"Machine Learning\"},\n",
    "                        run_config=conda_run_config)\n",
    "\n",
    "print(\"Notebook Run Config is created.\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define PythonScriptStep"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Source directory for the step is {}.'.format(os.path.realpath('./train')))\n",
    "python_script_step = PythonScriptStep(\n",
    "                         script_name=\"train.py\",\n",
    "                         arguments=[\"--input_data\", input_data],\n",
    "                         inputs=[input_data],\n",
    "                         outputs=[output_data],\n",
    "                         compute_target=compute_target, \n",
    "                         source_directory=\"./train\",\n",
    "                         allow_reuse=True)\n",
    "print(\"python_script_step created\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define NotebookRunnerStep\n",
    "\n",
    "This step will consume intermediate output produced by `python_script_step` as an input.\n",
    "\n",
    "Optionally, a output of type `output_notebook_pipeline_data_name` can be added to the `NotebookRunnerStep` to redirect the `output_notebook` of notebook run to `NotebookRunnerStep`'s step output produced as `PipelineData` and can be further passed along the pipeline."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineParameter\n",
    "\n",
    "output_from_notebook = PipelineData(name=\"notebook_processed_data\",\n",
    "                                    datastore=Datastore.get(ws, \"workspaceblobstore\"))\n",
    "\n",
    "my_pipeline_param = PipelineParameter(name=\"pipeline_param\", default_value=\"my_param\")\n",
    "\n",
    "print('Source directory for the step is {}.'.format(os.path.realpath(source_directory)))\n",
    "notebook_runner_step = NotebookRunnerStep(name=\"training_notebook_step\",\n",
    "                                          notebook_run_config=cfg,\n",
    "                                          params={\"my_pipeline_param\": my_pipeline_param},\n",
    "                                          inputs=[output_data],\n",
    "                                          outputs=[output_from_notebook],\n",
    "                                          allow_reuse=True,\n",
    "                                          compute_target=compute_target,\n",
    "                                          output_notebook_pipeline_data_name=\"notebook_result\")\n",
    "\n",
    "print(\"Notebook Runner Step is Created.\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Pipeline\n",
    "\n",
    "Once we have the steps (or steps collection), we can build the [pipeline](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py). By deafult, all these steps will run in **parallel** once we submit the pipeline for run.\n",
    "\n",
    "A pipeline is created with a list of steps and a workspace. Submit a pipeline using [submit](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py#submit-experiment-name--pipeline-parameters-none--continue-on-step-failure-false--regenerate-outputs-false--parent-run-id-none----kwargs-). When submit is called, a [PipelineRun](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinerun?view=azure-ml-py) is created which in turn creates [StepRun](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.steprun?view=azure-ml-py) objects for each step in the workflow."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1 = Pipeline(workspace=ws, steps=[notebook_runner_step])\n",
    "print(\"Pipeline creation complete\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run1 = experiment.submit(pipeline1)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run1).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download output notebook\n",
    "\n",
    "`output_notebook` can be retrieved via pipeline step output if `output_notebook_pipeline_data_name` is provided to the `NotebookRunnerStep`"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run1.wait_for_completion()\n",
    "train_step = pipeline_run1.find_step_run('training_notebook_step') # Retrieve the step runs by name `train.py`\n",
    "\n",
    "if train_step:\n",
    "    train_step_obj = train_step[0] # since we have only one step by name `training_notebook_step`\n",
    "    train_step_obj.get_output_data('notebook_result').download(source_directory) # download the output to source_directory"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-notebook-runner-step.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# How to Setup a PipelineEndpoint and Submit a Pipeline Using the PipelineEndpoint.\n",
    "In this notebook, we will see how to setup a PipelineEndpoint and run a specific pipeline version.\n",
    "\n",
    "PipelineEndpoint can be used to update a published pipeline while maintaining the same endpoint.\n",
    "PipelineEndpoint provides a way to keep track of [PublishedPipelines](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.publishedpipeline) using versions. PipelineEndpoint uses endpoint with version information to trigger an underlying published pipeline. Pipeline endpoints are uniquely named within a workspace.  \n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites and AML Basics\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the [configuration Notebook](https://aka.ms/pl-config) first if you haven't. This sets you up with a working config file that has information on your workspace, subscription id, etc.\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Overview\n",
    "In this notebook, we provide an introduction to Azure machine learning PipelineEndpoints. It covers:\n",
    "* [Create PipelineEndpoint](#Create-PipelineEndpoint), How to create PipelineEndpoint.\n",
    "* [Retrieving PipelineEndpoint](#Retrieving-PipelineEndpoint), How to get specific PipelineEndpoint from worskpace by name/Id and get all [PipelineEndpoints](#Get-all-PipelineEndpoints-in-workspace) within workspace.\n",
    "* [PipelineEndpoint Properties](#PipelineEndpoint-properties). How to get and set PipelineEndpoint properties, such as default version of PipelineEndpoint.\n",
    "* [PipelineEndpoint Submission](#PipelineEndpoint-Submission). How to run a Pipeline using PipelineEndpoint."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create PipelineEndpoint\n",
    "Following are required input parameters to create PipelineEndpoint:\n",
    "\n",
    "* *workspace*: AML workspace.\n",
    "* *name*: name of PipelineEndpoint, it is unique within workspace.\n",
    "* *description*: description details for PipelineEndpoint.\n",
    "* *pipeline*: A [Pipeline](#Steps-to-create-simple-Pipeline) or [PublishedPipeline](#Publish-Pipeline), to set default version of PipelineEndpoint.                                                       "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Initialization, Steps to create a Pipeline\n",
    "\n",
    "The best practice is to use separate folders for scripts and its dependent files for each step and specify that folder as the `source_directory` for the step. This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted). Since changes in any files in the `source_directory` would trigger a re-upload of the snapshot, this helps keep the reuse of the step when there are no changes in the `source_directory` of the step."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "#Retrieve an already attached Azure Machine Learning Compute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "aml_compute_target = \"cpu-cluster\"\n",
    "try:\n",
    "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
    "    print(\"Found existing compute target: {}\".format(aml_compute_target))\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new compute target: {}\".format(aml_compute_target))\n",
    "    \n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\",\n",
    "                                                                min_nodes = 1, \n",
    "                                                                max_nodes = 4)    \n",
    "    aml_compute = ComputeTarget.create(ws, aml_compute_target, provisioning_config)\n",
    "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "# source_directory\n",
    "source_directory = 'publish_run_train'\n",
    "# define a single step pipeline for demonstration purpose.\n",
    "trainStep = PythonScriptStep(\n",
    "    name=\"Training_Step\",\n",
    "    script_name=\"train.py\", \n",
    "    compute_target=aml_compute_target, \n",
    "    source_directory=source_directory\n",
    ")\n",
    "print(\"TrainStep created\")\n",
    "# build and validate Pipeline\n",
    "pipeline = Pipeline(workspace=ws, steps=[trainStep])\n",
    "print(\"Pipeline is built\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Publish Pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timenow = datetime.now().strftime('%m-%d-%Y-%H-%M')\n",
    "\n",
    "pipeline_name = timenow + \"-Pipeline\"\n",
    "print(pipeline_name)\n",
    "\n",
    "published_pipeline = pipeline.publish(\n",
    "    name=pipeline_name, \n",
    "    description=pipeline_name)\n",
    "print(\"Newly published pipeline id: {}\".format(published_pipeline.id))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Publishing PipelineEndpoint\n",
    "Create PipelineEndpoint with required parameters: workspace, name, description and pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineEndpoint\n",
    "\n",
    "pipeline_endpoint = PipelineEndpoint.publish(workspace=ws, name=\"PipelineEndpointTest\",\n",
    "                                            pipeline=pipeline, description=\"Test description Notebook\")\n",
    "pipeline_endpoint"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving PipelineEndpoint\n",
    "\n",
    "PipelineEndpoint is uniquely defined by name and id within workspace. PipelineEndpoint in workspace can be retrived by Id or by name."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get PipelineEndpoint by Name\n",
    "\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_endpoint_by_name = PipelineEndpoint.get(workspace=ws, name=\"PipelineEndpointTest\")\n",
    "pipeline_endpoint_by_name"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get PipelineEndpoint by Id\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the PipelineEndpoint Id\n",
    "pipeline_endpoint_by_name = PipelineEndpoint.get(workspace=ws, name=\"PipelineEndpointTest\")\n",
    "endpoint_id = pipeline_endpoint_by_name.id\n",
    "\n",
    "pipeline_endpoint_by_id = PipelineEndpoint.get(workspace=ws, id=endpoint_id)\n",
    "pipeline_endpoint_by_id"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all PipelineEndpoints in workspace\n",
    "Returns all PipelineEndpoints within workspace"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_list = PipelineEndpoint.list(workspace=ws, active_only=True)\n",
    "endpoint_list"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PipelineEndpoint properties"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default Version of PipelineEndpoint\n",
    "Default version of PipelineEndpoint starts from \"0\" and increments on addition of pipelines.\n",
    "\n",
    "##### Get the Default Version"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_version = pipeline_endpoint_by_name.get_default_version()\n",
    "default_version"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Set default version \n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_endpoint_by_name.set_default_version(\"0\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Published Pipeline corresponds to specific version of PipelineEndpoint"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipeline_endpoint_by_name.get_pipeline(\"0\")\n",
    "pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get default version Published Pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipeline_endpoint_by_name.get_pipeline()\n",
    "pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Published Pipeline to PipelineEndpoint, \n",
    "Adds a published pipeline (if its not present) using add() and if you want to add and set to default use add_default()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_endpoint_by_name.add(published_pipeline)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Published pipeline to PipelineEndpoint and set it to default version\n",
    "Adding published pipeline to PipelineEndpoint if not present and set it to default"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Published Pipeline to PipelineEndpoint, if exists\n",
    "pipeline_endpoint_by_name.set_default(published_pipeline)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all Versions in PipelineEndpoint\n",
    "Returns list of published pipelines and its versions"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versions = pipeline_endpoint_by_name.list_versions()\n",
    "\n",
    "for ve in versions:\n",
    "    print(ve.version)\n",
    "    print(ve.pipeline.id)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all Published Pipelines in PipelineEndpoint\n",
    "Returns all active pipelines in PipelineEnpoint, if active_only flag is set to True."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = pipeline_endpoint_by_name.list_pipelines(active_only=True)\n",
    "pipelines"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name property of PipelineEndpoint\n",
    "PipelineEndpoint is uniquely identified by name"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set Name PipelineEndpoint"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_endpoint_by_name.set_name(name=\"NewName\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PipelineEndpoint Submission\n",
    "PipelineEndpoint triggers specific versioned pipeline or default pipeline by:\n",
    "* Rest Endpoint \n",
    "* Submit call."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Pipeline by endpoint property of PipelineEndpoint\n",
    "Run specific pipeline using endpoint property of PipelineEndpoint and executing http post."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_endpoint_by_name = PipelineEndpoint.get(workspace=ws, name=\"NewName\")\n",
    "\n",
    "# endpoint with id \n",
    "rest_endpoint_id =  pipeline_endpoint_by_name.endpoint\n",
    "\n",
    "# for default version pipeline\n",
    "rest_endpoint_id_without_version_with_id = rest_endpoint_id\n",
    "\n",
    "# for specific version pipeline just append version info\n",
    "version=\"0\"\n",
    "rest_endpoint_id_with_version = rest_endpoint_id_without_version_with_id+\"/\"+ version\n",
    "print(rest_endpoint_id_with_version)\n",
    "pipeline_endpoint_by_name"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint with name\n",
    "rest_endpoint_name = rest_endpoint_id.split(\"Id\", 1)[0] + \"Name?name=\" + pipeline_endpoint_by_name.name\n",
    "\n",
    "# for default version pipeline\n",
    "rest_endpoint_name_without_version = rest_endpoint_name\n",
    "\n",
    "# for specific version pipeline just append version info\n",
    "version=\"0\"\n",
    "rest_endpoint_name_with_version = rest_endpoint_name_without_version+\"&pipelineVersion=\"+ version\n",
    "print(rest_endpoint_name_with_version)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This notebook](https://aka.ms/pl-restep-auth) shows how to authenticate to AML workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "import requests\n",
    "\n",
    "auth = InteractiveLoginAuthentication()\n",
    "aad_token = auth.get_authentication_header()\n",
    "\n",
    "#endpoint = pipeline_endpoint_by_name.url\n",
    "\n",
    "print(\"You can perform HTTP POST on URL {} to trigger this pipeline\".format(rest_endpoint_name_with_version))\n",
    "\n",
    "# specify the param when running the pipeline\n",
    "response = requests.post(rest_endpoint_name_with_version, \n",
    "                         headers=aad_token, \n",
    "                         json={\"ExperimentName\": \"default_pipeline\",\n",
    "                               \"RunSource\": \"SDK\",\n",
    "                               \"ParameterAssignments\": {\"1\": \"united\", \"2\":\"city\"}})"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response.raise_for_status()\n",
    "except Exception:    \n",
    "    raise Exception('Received bad response from the endpoint: {}\\n'\n",
    "                    'Response Code: {}\\n'\n",
    "                    'Headers: {}\\n'\n",
    "                    'Content: {}'.format(rest_endpoint, response.status_code, response.headers, response.content))\n",
    "\n",
    "run_id = response.json().get('Id')\n",
    "print('Submitted pipeline run: ', run_id)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Pipeline by Submit call of PipelineEndpoint \n",
    "Run specific pipeline using Submit api of PipelineEndpoint"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit pipeline with specific version\n",
    "run_id = pipeline_endpoint_by_name.submit(\"NewName\", pipeline_version=\"0\")\n",
    "print(run_id)\n",
    "\n",
    "# submit pipeline with default version\n",
    "run_id = pipeline_endpoint_by_name.submit(\"NewName\")\n",
    "print(run_id)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Experiment.Submit() to Submit Pipeline\n",
    "Run specific pipeline using Experiment submit api"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "pipeline_run = Experiment(ws, name=\"submit_from_endpoint\").submit(pipeline_endpoint_by_name, tags={'endpoint_tag': \"1\"}, pipeline_version=\"0\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/notebook_runner/training_notebook.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/notebook_runner/training_notebook.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/notebook_runner/training_notebook.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/notebook_runner/training_notebook.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"In training_notebook.ipynb\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/notebook_runner/training_notebook.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# declaring parameters to override\n",
    "\n",
    "arg1 = \"Azure\"\n",
    "processed_data = None\n",
    "notebook_processed_data = None\n",
    "my_pipeline_param = None"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/notebook_runner/training_notebook.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final parameter values\n",
    "\n",
    "print(\"arg1: %s\" % arg1)\n",
    "print(\"input from previous step: %s\" % processed_data)\n",
    "print(\"output from notebook: %s\" % notebook_processed_data)\n",
    "print(\"pipeline_parameter: %s\" % my_pipeline_param)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/notebook_runner/training_notebook.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (notebook_processed_data is None):\n",
    "    os.makedirs(notebook_processed_data, exist_ok=True)\n",
    "    print(\"%s created\" % notebook_processed_data)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/notebook_runner/training_notebook.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural style transfer on video\n",
    "Using modified code from `pytorch`'s neural style [example](https://pytorch.org/tutorials/advanced/neural_style_tutorial.html), we show how to setup a pipeline for doing style transfer on video. The pipeline has following steps:\n",
    "1. Split a video into images\n",
    "2. Run neural style on each image using one of the provided models (from `pytorch` pretrained models for this example).\n",
    "3. Stitch the image back into a video.\n",
    "\n",
    "> **Tip**\n",
    "If your system requires low-latency processing (to process a single document or small set of documents quickly), use [real-time scoring](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-consume-web-service) instead of batch prediction."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the configuration Notebook located at https://github.com/Azure/MachineLearningNotebooks first if you haven't. This sets you up with a working config file that has information on your workspace, subscription id, etc. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.core.runconfig import CondaDependencies, RunConfiguration\n",
    "from azureml.core.compute_target import ComputeTargetException"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download models"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# create directory for model\n",
    "model_dir = 'models'\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.mkdir(model_dir)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "def download_model(model_name):\n",
    "    # downloaded models from https://pytorch.org/tutorials/advanced/neural_style_tutorial.html are kept here\n",
    "    url = \"https://pipelinedata.blob.core.windows.net/styletransfer/saved_models/\" + model_name\n",
    "    local_path = os.path.join(model_dir, model_name)\n",
    "    urllib.request.urlretrieve(url, local_path)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register all Models"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "mosaic_model = None\n",
    "candy_model = None\n",
    "\n",
    "models = Model.list(workspace=ws, tags=['scenario'])\n",
    "for m in models:\n",
    "    print(\"Name:\", m.name,\"\\tVersion:\", m.version, \"\\tDescription:\", m.description, m.tags)\n",
    "    if m.name == 'mosaic' and mosaic_model is None:\n",
    "        mosaic_model = m\n",
    "    elif m.name == 'candy' and candy_model is None:\n",
    "        candy_model = m\n",
    "\n",
    "if mosaic_model is None:\n",
    "    print('Mosaic model does not exist, registering it')\n",
    "    download_model('mosaic.pth')\n",
    "    mosaic_model = Model.register(model_path = os.path.join(model_dir, \"mosaic.pth\"),\n",
    "                       model_name = \"mosaic\",\n",
    "                       tags = {'type': \"mosaic\", 'scenario': \"Style transfer using batch inference\"},\n",
    "                       description = \"Style transfer - Mosaic\",\n",
    "                       workspace = ws)\n",
    "else:\n",
    "    print('Reusing existing mosaic model')\n",
    "    \n",
    "\n",
    "if candy_model is None:\n",
    "    print('Candy model does not exist, registering it')\n",
    "    download_model('candy.pth')\n",
    "    candy_model = Model.register(model_path = os.path.join(model_dir, \"candy.pth\"),\n",
    "                       model_name = \"candy\",\n",
    "                       tags = {'type': \"candy\", 'scenario': \"Style transfer using batch inference\"},\n",
    "                       description = \"Style transfer - Candy\",\n",
    "                       workspace = ws)\n",
    "else:\n",
    "    print('Reusing existing candy model')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create or use existing compute"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AmlCompute\n",
    "cpu_cluster_name = \"cpu-cluster\"\n",
    "try:\n",
    "    cpu_cluster = AmlCompute(ws, cpu_cluster_name)\n",
    "    print(\"found existing cluster.\")\n",
    "except ComputeTargetException:\n",
    "    print(\"creating new cluster\")\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_v2\",\n",
    "                                                                    max_nodes = 1)\n",
    "\n",
    "    # create the cluster\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, provisioning_config)\n",
    "    cpu_cluster.wait_for_completion(show_output=True)\n",
    "    \n",
    "# AmlCompute\n",
    "gpu_cluster_name = \"gpu-cluster\"\n",
    "try:\n",
    "    gpu_cluster = AmlCompute(ws, gpu_cluster_name)\n",
    "    print(\"found existing cluster.\")\n",
    "except ComputeTargetException:\n",
    "    print(\"creating new cluster\")\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_NC6\",\n",
    "                                                                max_nodes = 3)\n",
    "\n",
    "    # create the cluster\n",
    "    gpu_cluster = ComputeTarget.create(ws, gpu_cluster_name, provisioning_config)\n",
    "    gpu_cluster.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Scripts\n",
    "We use an edited version of `neural_style_mpi.py` (original is [here](https://github.com/pytorch/examples/blob/master/fast_neural_style/neural_style/neural_style.py)). Scripts to split and stitch the video are thin wrappers to calls to `ffmpeg`. \n",
    "\n",
    "We install `ffmpeg` through conda dependencies."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts_folder = \"scripts\""
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_video_script_file = \"process_video.py\"\n",
    "\n",
    "# peek at contents\n",
    "with open(os.path.join(scripts_folder, process_video_script_file)) as process_video_file:\n",
    "    print(process_video_file.read())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitch_video_script_file = \"stitch_video.py\"\n",
    "\n",
    "# peek at contents\n",
    "with open(os.path.join(scripts_folder, stitch_video_script_file)) as stitch_video_file:\n",
    "    print(stitch_video_file.read())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample video **organutan.mp4** is stored at a publicly shared datastore. We are registering the datastore below. If you want to take a look at the original video, click here. (https://pipelinedata.blob.core.windows.net/sample-videos/orangutan.mp4)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datastore for input video\n",
    "account_name = \"pipelinedata\"\n",
    "video_ds = Datastore.register_azure_blob_container(ws, \"videos\", \"sample-videos\",\n",
    "                                            account_name=account_name, overwrite=True)\n",
    "\n",
    "# the default blob store attached to a workspace\n",
    "default_datastore = ws.get_default_datastore()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample video"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name=os.getenv(\"STYLE_TRANSFER_VIDEO_NAME\", \"orangutan.mp4\") \n",
    "orangutan_video = DataReference(datastore=video_ds,\n",
    "                            data_reference_name=\"video\",\n",
    "                            path_on_datastore=video_name, mode=\"download\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = CondaDependencies()\n",
    "\n",
    "cd.add_channel(\"conda-forge\")\n",
    "cd.add_conda_package(\"ffmpeg==4.0.2\")\n",
    "\n",
    "# Runconfig\n",
    "amlcompute_run_config = RunConfiguration(conda_dependencies=cd)\n",
    "amlcompute_run_config.environment.docker.base_image = \"pytorch/pytorch\"\n",
    "amlcompute_run_config.environment.spark.precache_packages = False"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffmpeg_audio = PipelineData(name=\"ffmpeg_audio\", datastore=default_datastore)\n",
    "processed_images = PipelineData(name=\"processed_images\", datastore=default_datastore)\n",
    "output_video = PipelineData(name=\"output_video\", datastore=default_datastore)\n",
    "\n",
    "ffmpeg_images_ds_name = \"ffmpeg_images_data\"\n",
    "ffmpeg_images = PipelineData(name=\"ffmpeg_images\", datastore=default_datastore)\n",
    "ffmpeg_images_file_dataset = ffmpeg_images.as_dataset()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define tweakable parameters to pipeline\n",
    "These parameters can be changed when the pipeline is published and rerun from a REST call.\n",
    "As part of ParallelRunStep following 2 pipeline parameters will be created which can be used to override values.\n",
    "    node_count\n",
    "    process_count_per_node"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core.graph import PipelineParameter\n",
    "# create a parameter for style (one of \"candy\", \"mosaic\") to transfer the images to\n",
    "style_param = PipelineParameter(name=\"style\", default_value=\"mosaic\")\n",
    "# create a parameter for the number of nodes to use in step no. 2 (style transfer)\n",
    "nodecount_param = PipelineParameter(name=\"nodecount\", default_value=2)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_video_step = PythonScriptStep(\n",
    "    name=\"split video\",\n",
    "    script_name=\"process_video.py\",\n",
    "    arguments=[\"--input_video\", orangutan_video,\n",
    "               \"--output_audio\", ffmpeg_audio,\n",
    "               \"--output_images\", ffmpeg_images_file_dataset,\n",
    "              ],\n",
    "    compute_target=cpu_cluster,\n",
    "    inputs=[orangutan_video],\n",
    "    outputs=[ffmpeg_images_file_dataset, ffmpeg_audio],\n",
    "    runconfig=amlcompute_run_config,\n",
    "    source_directory=scripts_folder\n",
    ")\n",
    "\n",
    "stitch_video_step = PythonScriptStep(\n",
    "    name=\"stitch\",\n",
    "    script_name=\"stitch_video.py\",\n",
    "    arguments=[\"--images_dir\", processed_images, \n",
    "               \"--input_audio\", ffmpeg_audio, \n",
    "               \"--output_dir\", output_video],\n",
    "    compute_target=cpu_cluster,\n",
    "    inputs=[processed_images, ffmpeg_audio],\n",
    "    outputs=[output_video],\n",
    "    runconfig=amlcompute_run_config,\n",
    "    source_directory=scripts_folder\n",
    ")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environment, parallel step run config and parallel run step"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import DEFAULT_GPU_IMAGE\n",
    "\n",
    "parallel_cd = CondaDependencies()\n",
    "\n",
    "parallel_cd.add_channel(\"pytorch\")\n",
    "parallel_cd.add_conda_package(\"pytorch\")\n",
    "parallel_cd.add_conda_package(\"torchvision\")\n",
    "parallel_cd.add_conda_package(\"pillow<7\") # needed for torchvision==0.4.0\n",
    "parallel_cd.add_pip_package(\"azureml-core\")\n",
    "parallel_cd.add_pip_package(\"azureml-dataset-runtime[fuse]\")\n",
    "\n",
    "styleenvironment = Environment(name=\"styleenvironment\")\n",
    "styleenvironment.python.conda_dependencies=parallel_cd\n",
    "styleenvironment.docker.base_image = DEFAULT_GPU_IMAGE"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineParameter\n",
    "from azureml.pipeline.steps import ParallelRunConfig\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    environment=styleenvironment,\n",
    "    entry_script='transform.py',\n",
    "    output_action='summary_only',\n",
    "    mini_batch_size=\"1\",\n",
    "    error_threshold=1,\n",
    "    source_directory=scripts_folder,\n",
    "    compute_target=gpu_cluster, \n",
    "    node_count=nodecount_param,\n",
    "    process_count_per_node=2\n",
    ")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import ParallelRunStep\n",
    "from datetime import datetime\n",
    "\n",
    "parallel_step_name = 'styletransfer-' + datetime.now().strftime('%Y%m%d%H%M')\n",
    "\n",
    "distributed_style_transfer_step = ParallelRunStep(\n",
    "    name=parallel_step_name,\n",
    "    inputs=[ffmpeg_images_file_dataset], # Input file share/blob container/file dataset\n",
    "    output=processed_images,  # Output file share/blob container\n",
    "    arguments=[\"--style\", style_param],\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    allow_reuse=False #[optional - default value True]\n",
    ")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[stitch_video_step])\n",
    "\n",
    "pipeline.validate()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the pipeline and provide values for the PipelineParameters used in the pipeline\n",
    "pipeline_run = Experiment(ws, 'styletransfer_parallel_mosaic').submit(pipeline)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitor pipeline run\n",
    "\n",
    "The pipeline run status could be checked in Azure Machine Learning portal (https://ml.azure.com). The link to the pipeline run could be retrieved by inspecting the `pipeline_run` object.\n",
    "\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will output information of the pipeline run, including the link to the details page of portal.\n",
    "pipeline_run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: View detailed logs (streaming) "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait the run for completion and show output log to console\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download output video"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloads the video in `output_video` folder"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video(run, target_dir=None):\n",
    "    stitch_run = run.find_step_run(stitch_video_step.name)[0]\n",
    "    port_data = stitch_run.get_output_data(output_video.name)\n",
    "    port_data.download(target_dir, show_progress=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run.wait_for_completion()\n",
    "download_video(pipeline_run, \"output_video_mosaic\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publish pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = \"style-transfer-batch-inference\"\n",
    "print(pipeline_name)\n",
    "\n",
    "published_pipeline = pipeline.publish(\n",
    "    name=pipeline_name, \n",
    "    description=pipeline_name)\n",
    "print(\"Newly published pipeline id: {}\".format(published_pipeline.id))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get published pipeline\n",
    "This is another way to get the published pipeline."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PublishedPipeline\n",
    "\n",
    "# You could retrieve all pipelines that are published, or \n",
    "# just get the published pipeline object that you have the ID for.\n",
    "\n",
    "# Get all published pipeline objects in the workspace\n",
    "all_pub_pipelines = PublishedPipeline.list(ws)\n",
    "\n",
    "# We will iterate through the list of published pipelines and \n",
    "# use the last ID in the list for Schelue operations: \n",
    "print(\"Published pipelines found in the workspace:\")\n",
    "for pub_pipeline in all_pub_pipelines:\n",
    "    print(\"Name:\", pub_pipeline.name,\"\\tDescription:\", pub_pipeline.description, \"\\tId:\", pub_pipeline.id, \"\\tStatus:\", pub_pipeline.status)\n",
    "    if(pub_pipeline.name == pipeline_name):\n",
    "        published_pipeline = pub_pipeline\n",
    "\n",
    "print(\"Published pipeline id: {}\".format(published_pipeline.id))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run pipeline through REST calls for other styles\n",
    "\n",
    "# Get AAD token"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "import requests\n",
    "\n",
    "auth = InteractiveLoginAuthentication()\n",
    "aad_token = auth.get_authentication_header()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get endpoint URL"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_endpoint = published_pipeline.endpoint\n",
    "print(\"Pipeline REST endpoing: {}\".format(rest_endpoint))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send request and monitor"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'styletransfer_parallel_candy'\n",
    "response = requests.post(rest_endpoint, \n",
    "                         headers=aad_token,\n",
    "                         json={\"ExperimentName\": experiment_name,\n",
    "                               \"ParameterAssignments\": {\"style\": \"candy\", \"NodeCount\": 3}})\n",
    "\n",
    "run_id = response.json()[\"Id\"]\n",
    "\n",
    "from azureml.pipeline.core.run import PipelineRun\n",
    "published_pipeline_run_candy = PipelineRun(ws.experiments[experiment_name], run_id)\n",
    "\n",
    "# Show detail information of run\n",
    "published_pipeline_run_candy"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download output from re-run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline_run_candy.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_video(published_pipeline_run_candy, target_dir=\"output_video_candy\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess Fairness, Explore Interpretability, and Mitigate Fairness Issues \n",
    "\n",
    "This notebook demonstrates how to use [InterpretML](interpret.ml), [Fairlearn](fairlearn.org), and the [Responsible AI Widget's](https://github.com/microsoft/responsible-ai-widgets/) Fairness and Interpretability dashboards to understand a model trained on the Census dataset. This dataset is a classification problem - given a range of data about 32,000 individuals, predict whether their annual income is above or below fifty thousand dollars per year.\n",
    "\n",
    "For the purposes of this notebook, we shall treat this as a loan decision problem. We will pretend that the label indicates whether or not each individual repaid a loan in the past. We will use the data to train a predictor to predict whether previously unseen individuals will repay a loan or not. The assumption is that the model predictions are used to decide whether an individual should be offered a loan.\n",
    "\n",
    "We will first train a fairness-unaware predictor, load its global and local explanations, and use the interpretability and fairness dashboards to demonstrate how this model leads to unfair decisions (under a specific notion of fairness called *demographic parity*). We then mitigate unfairness by applying the `GridSearch` algorithm from `Fairlearn` package.\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade fairlearn\n",
    "%pip install --upgrade interpret-community\n",
    "%pip install --upgrade raiwidgets"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing packages, you must close and reopen the notebook as well as restarting the kernel."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess the dataset\n",
    "\n",
    "For simplicity, we import the dataset from the `shap` package, which contains the data in a cleaned format. We start by importing the various modules we're going to use:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import GridSearch\n",
    "from fairlearn.reductions import DemographicParity, ErrorRate\n",
    "from fairlearn.datasets import fetch_adult\n",
    "from fairlearn.metrics import MetricFrame, selection_rate\n",
    "\n",
    "from sklearn import svm, neighbors, tree\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# SHAP Tabular Explainer\n",
    "from interpret.ext.blackbox import KernelExplainer\n",
    "from interpret.ext.blackbox import MimicExplainer\n",
    "from interpret.ext.glassbox import LGBMExplainableModel"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load and inspect the data:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_adult(as_frame=True)\n",
    "X_raw, y = dataset['data'], dataset['target']\n",
    "X_raw[\"race\"].value_counts().to_dict()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to treat the sex of each individual as a protected attribute (where 0 indicates female and 1 indicates male), and in this particular case we are going separate this attribute out and drop it from the main data. We then perform some standard data preprocessing steps to convert the data into a format suitable for the ML algorithms"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_features = X_raw[['sex','race']]\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we split the data into training and test sets:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, sensitive_features_train, sensitive_features_test = \\\n",
    "    train_test_split(X_raw, y, sensitive_features,\n",
    "                     test_size = 0.2, random_state=0, stratify=y)\n",
    "\n",
    "# Work around indexing bug\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "sensitive_features_train = sensitive_features_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "sensitive_features_test = sensitive_features_test.reset_index(drop=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a fairness-unaware predictor\n",
    "\n",
    "To show the effect of `Fairlearn` we will first train a standard ML predictor that does not incorporate fairness. For speed of demonstration, we use a simple logistic regression estimator from `sklearn`:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"impute\", SimpleImputer()),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "categorical_transformer = Pipeline(\n",
    "    [\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, make_column_selector(dtype_exclude=\"category\")),\n",
    "        (\"cat\", categorical_transformer, make_column_selector(dtype_include=\"category\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            LogisticRegression(solver=\"liblinear\", fit_intercept=True),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate model explanations"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SHAP KernelExplainer\n",
    "# clf.steps[-1][1] returns the trained classification model\n",
    "explainer = MimicExplainer(model.steps[-1][1], \n",
    "                           X_train,\n",
    "                           LGBMExplainableModel,\n",
    "                           features=X_raw.columns, \n",
    "                           classes=['Rejected', 'Approved'],\n",
    "                           transformations=preprocessor)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate global explanations\n",
    "Explain overall model predictions (global explanation)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the model based on a subset of 1000 rows\n",
    "global_explanation = explainer.explain_global(X_test[:1000])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_explanation.get_feature_importance_dict()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate local explanations\n",
    "Explain local data points (individual instances)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can pass a specific data point or a group of data points to the explain_local function\n",
    "# E.g., Explain the first data point in the test set\n",
    "instance_num = 1\n",
    "local_explanation = explainer.explain_local(X_test[:instance_num])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prediction for the first member of the test set and explain why model made that prediction\n",
    "prediction_value = model.predict(X_test)[instance_num]\n",
    "\n",
    "sorted_local_importance_values = local_explanation.get_ranked_local_values()[prediction_value]\n",
    "sorted_local_importance_names = local_explanation.get_ranked_local_names()[prediction_value]"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('local importance values: {}'.format(sorted_local_importance_values))\n",
    "print('local importance names: {}'.format(sorted_local_importance_names))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize model explanations\n",
    "Load the interpretability visualization dashboard"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raiwidgets import ExplanationDashboard"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExplanationDashboard(global_explanation, model, dataset=X_test[:1000], true_y=y_test[:1000])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load this predictor into the Fairness dashboard, and examine how it is unfair:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess model fairness \n",
    "Load the fairness visualization dashboard"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raiwidgets import FairnessDashboard\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "FairnessDashboard(sensitive_features=sensitive_features_test,\n",
    "                  y_true=y_test,\n",
    "                  y_pred=y_pred)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the disparity in accuracy, we see that males have an error rate about three times greater than the females. More interesting is the disparity in opportunitiy - males are offered loans at three times the rate of females.\n",
    "\n",
    "Despite the fact that we removed the feature from the training data, our predictor still discriminates based on sex. This demonstrates that simply ignoring a protected attribute when fitting a predictor rarely eliminates unfairness. There will generally be enough other features correlated with the removed attribute to lead to disparate impact."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mitigation with Fairlearn (GridSearch)\n",
    "\n",
    "The `GridSearch` class in `Fairlearn` implements a simplified version of the exponentiated gradient reduction of [Agarwal et al. 2018](https://arxiv.org/abs/1803.02453). The user supplies a standard ML estimator, which is treated as a blackbox. `GridSearch` works by generating a sequence of relabellings and reweightings, and trains a predictor for each.\n",
    "\n",
    "For this example, we specify demographic parity (on the protected attribute of sex) as the fairness metric. Demographic parity requires that individuals are offered the opportunity (are approved for a loan in this example) independent of membership in the protected class (i.e., females and males should be offered loans at the same rate). We are using this metric for the sake of simplicity; in general, the appropriate fairness metric will not be obvious."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fairlearn is not yet fully compatible with Pipelines, so we have to pass the estimator only\n",
    "X_train_prep = preprocessor.transform(X_train).toarray()\n",
    "X_test_prep = preprocessor.transform(X_test).toarray()\n",
    "\n",
    "sweep = GridSearch(LogisticRegression(solver=\"liblinear\", fit_intercept=True),\n",
    "                   constraints=DemographicParity(),\n",
    "                   grid_size=70)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our algorithms provide `fit()` and `predict()` methods, so they behave in a similar manner to other ML packages in Python. We do however have to specify two extra arguments to `fit()` - the column of protected attribute labels, and also the number of predictors to generate in our sweep.\n",
    "\n",
    "After `fit()` completes, we extract the full set of predictors from the `GridSearch` object."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep.fit(X_train_prep, y_train,\n",
    "          sensitive_features=sensitive_features_train.sex)\n",
    "\n",
    "predictors = sweep.predictors_"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could load these predictors into the Fairness dashboard now. However, the plot would be somewhat confusing due to their number. In this case, we are going to remove the predictors which are dominated in the error-disparity space by others from the sweep (note that the disparity will only be calculated for the sensitive feature). In general, one might not want to do this, since there may be other considerations beyond the strict optimization of error and disparity (of the given protected attribute)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies, disparities = [], []\n",
    "\n",
    "for predictor in predictors:\n",
    "    accuracy_metric_frame = MetricFrame(accuracy_score, y_train, predictor.predict(X_train_prep), sensitive_features=sensitive_features_train.sex)\n",
    "    selection_rate_metric_frame = MetricFrame(selection_rate, y_train, predictor.predict(X_train_prep), sensitive_features=sensitive_features_train.sex)\n",
    "    accuracies.append(accuracy_metric_frame.overall)\n",
    "    disparities.append(selection_rate_metric_frame.difference())\n",
    "    \n",
    "all_results = pd.DataFrame({\"predictor\": predictors, \"accuracy\": accuracies, \"disparity\": disparities})\n",
    "\n",
    "all_models_dict = {\"unmitigated\": model.steps[-1][1]}\n",
    "dominant_models_dict = {\"unmitigated\": model.steps[-1][1]}\n",
    "base_name_format = \"grid_{0}\"\n",
    "row_id = 0\n",
    "for row in all_results.itertuples():\n",
    "    model_name = base_name_format.format(row_id)\n",
    "    all_models_dict[model_name] = row.predictor\n",
    "    accuracy_for_lower_or_eq_disparity = all_results[\"accuracy\"][all_results[\"disparity\"] <= row.disparity]\n",
    "    if row.accuracy >= accuracy_for_lower_or_eq_disparity.max():\n",
    "        dominant_models_dict[model_name] = row.predictor\n",
    "    row_id = row_id + 1"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct predictions for all the models, and also for the dominant models:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raiwidgets import FairnessDashboard\n",
    "\n",
    "dashboard_all = {}\n",
    "for name, predictor in all_models_dict.items():\n",
    "    value = predictor.predict(X_test_prep)\n",
    "    dashboard_all[name] = value\n",
    "    \n",
    "dominant_all = {}\n",
    "for name, predictor in dominant_models_dict.items():\n",
    "    dominant_all[name] = predictor.predict(X_test_prep)\n",
    "\n",
    "FairnessDashboard(sensitive_features=sensitive_features_test, \n",
    "                  y_true=y_test,\n",
    "                  y_pred=dominant_all)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at just the dominant models in the dashboard:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a Pareto front forming - the set of predictors which represent optimal tradeoffs between accuracy and disparity in predictions. In the ideal case, we would have a predictor at (1,0) - perfectly accurate and without any unfairness under demographic parity (with respect to the protected attribute \"sex\"). The Pareto front represents the closest we can come to this ideal based on our data and choice of estimator. Note the range of the axes - the disparity axis covers more values than the accuracy, so we can reduce disparity substantially for a small loss in accuracy.\n",
    "\n",
    "By clicking on individual models on the plot, we can inspect their metrics for disparity and accuracy in greater detail. In a real example, we would then pick the model which represented the best trade-off between accuracy and disparity given the relevant business constraints."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AzureML integration\n",
    "\n",
    "We will now go through a brief example of the AzureML integration.\n",
    "\n",
    "The required package can be installed via:\n",
    "\n",
    "```\n",
    "pip install azureml-contrib-fairness\n",
    "pip install azureml-interpret\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to workspace\n",
    "\n",
    "Just like in the previous tutorials, we will need to connect to a [workspace](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace(class)?view=azure-ml-py).\n",
    "\n",
    "The following code will allow you to create a workspace if you don't already have one created. You must have an Azure subscription to create a workspace:\n",
    "\n",
    "```python\n",
    "from azureml.core import Workspace\n",
    "ws = Workspace.create(name='myworkspace',\n",
    "                      subscription_id='<azure-subscription-id>',\n",
    "                      resource_group='myresourcegroup',\n",
    "                      create_resource_group=True,\n",
    "                      location='eastus2')\n",
    "```\n",
    "\n",
    "**If you are running this on a Notebook VM, you can import the existing workspace.**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering models\n",
    "\n",
    "The fairness dashboard is designed to integrate with registered models, so we need to do this for the models we want in the Studio portal. The assumption is that the names of the models specified in the dashboard dictionary correspond to the `id`s (i.e. `<name>:<version>` pairs) of registered models in the workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we register each of the models in the `dashboard_predicted` dictionary into the workspace. For this, we have to save each model to a file, and then register that file:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "from azureml.core import Model, Experiment, Run\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "def register_model(name, model):\n",
    "    print(\"Registering \", name)\n",
    "    model_path = \"models/{0}.pkl\".format(name)\n",
    "    joblib.dump(value=model, filename=model_path)\n",
    "    registered_model = Model.register(model_path=model_path,\n",
    "                                      model_name=name,\n",
    "                                      workspace=ws)\n",
    "    print(\"Registered \", registered_model.id)\n",
    "    return registered_model.id\n",
    "\n",
    "model_name_id_mapping = dict()\n",
    "for name, model in dashboard_all.items():\n",
    "    m_id = register_model(name, model)\n",
    "    model_name_id_mapping[name] = m_id"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, produce new predictions dictionaries, with the updated names:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_all_ids = dict()\n",
    "for name, y_pred in dashboard_all.items():\n",
    "    dashboard_all_ids[model_name_id_mapping[name]] = y_pred"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading a dashboard\n",
    "\n",
    "We create a _dashboard dictionary_ using Fairlearn's `metrics` package. The `_create_group_metric_set` method has arguments similar to the Dashboard constructor, except that the sensitive features are passed as a dictionary (to ensure that names are available), and we must specify the type of prediction. Note that we use the `dashboard_registered` dictionary we just created:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = { 'sex': sensitive_features_test.sex, 'race': sensitive_features_test.race }\n",
    "\n",
    "from fairlearn.metrics._group_metric_set import _create_group_metric_set\n",
    "\n",
    "dash_dict_all = _create_group_metric_set(y_true=y_test,\n",
    "                                         predictions=dashboard_all_ids,\n",
    "                                         sensitive_features=sf,\n",
    "                                         prediction_type='binary_classification')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we import our `contrib` package which contains the routine to perform the upload:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.fairness import upload_dashboard_dictionary, download_dashboard_by_upload_id"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create an Experiment, then a Run, and upload our dashboard to it:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(ws, 'responsible-ai-loan-decision')\n",
    "print(exp)\n",
    "\n",
    "run = exp.start_logging()\n",
    "try:\n",
    "    dashboard_title = \"Upload MultiAsset from Grid Search with Census Data Notebook\"\n",
    "    upload_id = upload_dashboard_dictionary(run,\n",
    "                                            dash_dict_all,\n",
    "                                            dashboard_name=dashboard_title)\n",
    "    print(\"\\nUploaded to id: {0}\\n\".format(upload_id))\n",
    "\n",
    "    downloaded_dict = download_dashboard_by_upload_id(run, upload_id)\n",
    "finally:\n",
    "    run.complete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading  explanations\n",
    "\n",
    "\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.interpret import ExplanationClient\n",
    "\n",
    "client = ExplanationClient.from_run(run)\n",
    "client.upload_model_explanation(global_explanation, comment = \"census data global explanation\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/responsible-ai/visualize-upload-loan-decision/rai-loan-decision.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging\n",
    "\n",
    "_**This notebook showcases various ways to use the Azure Machine Learning service run logging APIs, and view the results in the Azure portal.**_\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "    1. Validate Azure ML SDK installation\n",
    "    1. Initialize workspace\n",
    "    1. Set experiment\n",
    "1. [Logging](#Logging)\n",
    "    1. Starting a run\n",
    "        1. Viewing a run in the portal\n",
    "        1. Viewing the experiment in the portal\n",
    "    1. Logging metrics\n",
    "        1. Logging string metrics\n",
    "        1. Logging numeric metrics\n",
    "        1. Logging vectors\n",
    "        1. Logging tables\n",
    "        1. Uploading files\n",
    "1. [Analyzing results](#Analyzing-results)\n",
    "    1. Tagging a run\n",
    "1. [Next steps](#Next-steps)\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Logging metrics from runs in your experiments allows you to track results from one run to another, determining trends in your outputs and understand how your inputs correspond to your model and script performance.  Azure Machine Learning services (AzureML) allows you to track various types of metrics including images and arbitrary files in order to understand, analyze, and audit your experimental progress. \n",
    "\n",
    "Typically you should log all parameters for your experiment and all numerical and string outputs of your experiment.  This will allow you to analyze the performance of your experiments across multiple runs, correlate inputs to outputs, and filter runs based on interesting criteria.\n",
    "\n",
    "The experiment's Run History report page automatically creates a report that can be customized to show the KPI's, charts, and column sets that are interesting to you. \n",
    "\n",
    "| ![Run Details](./img/run_details.PNG) | ![Run History](./img/run_history.PNG) |\n",
    "|:--:|:--:|\n",
    "| *Run Details* | *Run History* |\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set.  Otherwise, go through the [configuration](../../../configuration.ipynb) Notebook first if you haven't already to establish your connection to the AzureML Workspace. Also make sure you have tqdm and matplotlib installed in the current kernel.\n",
    "\n",
    "```\n",
    "(myenv) $ conda install -y tqdm matplotlib\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Azure ML SDK installation and get version number for debugging purposes"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "install"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment, Workspace, Run\n",
    "import azureml.core\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check core SDK version number\n",
    "\n",
    "print(\"This notebook was created using SDK version 1.22.0, you are currently running version\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create workspace"
    ]
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set experiment\n",
    "Create a new experiment (or get the one with the specified name).  An *experiment* is a container for an arbitrary set of *runs*. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(workspace=ws, name='logging-api-test')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Logging\n",
    "In this section we will explore the various logging mechanisms.\n",
    "\n",
    "### Starting a run\n",
    "\n",
    "A *run* is a singular experimental trial.  In this notebook we will create a run directly on the experiment  by calling `run = exp.start_logging()`.  If you were experimenting by submitting a script file as an experiment using ``experiment.submit()``, you would call `run = Run.get_context()` in your script to access the run context of your code.  In either case, the logging methods on the returned run object work the same.\n",
    "\n",
    "This cell also stores the run id for use later in this notebook.  The run_id is not necessary for logging."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start logging for the run\n",
    "run = experiment.start_logging()\n",
    "\n",
    "# access the run id for use later\n",
    "run_id = run.id\n",
    "\n",
    "# change the scale factor on different runs to see how you can compare multiple runs\n",
    "scale_factor = 2\n",
    "\n",
    "# change the category on different runs to see how to organize data in reports\n",
    "category = 'Red'"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Viewing a run in the Portal\n",
    "Once a run is started you can see the run in the portal by simply typing ``run``.  Clicking on the \"Link to Portal\" link will take you to the Run Details page that shows the metrics you have logged and other run properties.  You can refresh this page after each logging statement to see the updated results."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing an experiment in the portal\n",
    "You can also view an experiement similarly by typing `experiment`.  The portal link will take you to the experiment's Run History page that shows all runs and allows you to analyze trends across multiple runs."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging metrics\n",
    "Metrics are visible in the run details page in the AzureML portal and also can be analyzed in experiment reports.  The run details page looks as below and contains tabs for Details, Outputs, Logs, and Snapshot.  \n",
    "* The Details page displays attributes about the run, plus logged metrics and images.  Metrics that are vectors appear as charts.  \n",
    "* The Outputs page contains any files, such as models, you uploaded into the \"outputs\" directory from your run into storage.  If you place files in the \"outputs\" directory locally, the files are automatically uploaded on your behald when the run is completed.\n",
    "* The Logs page allows you to view any log files created by your run.  Logging runs created in notebooks typically do not generate log files.\n",
    "* The Snapshot page contains a snapshot of the directory specified in the ''start_logging'' statement, plus the notebook at the time of the ''start_logging'' call.  This snapshot and notebook can be downloaded from the Run Details page to continue or reproduce an experiment.\n",
    "\n",
    "### Logging string metrics\n",
    "The following cell logs a string metric.  A string metric is simply a string value associated with a name.  A string metric String metrics are useful for labelling runs and to organize your data.  Typically you should log all string parameters as metrics for later analysis - even information such as paths can help to understand how individual experiements perform differently.\n",
    "\n",
    "String metrics can be used in the following ways:\n",
    "* Plot in hitograms\n",
    "* Group by indicators for numerical plots\n",
    "* Filtering runs\n",
    "\n",
    "String metrics appear in the **Tracked Metrics** section of the Run Details page and can be added as a column in Run History reports."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log a string metric\n",
    "run.log(name='Category', value=category)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging numerical metrics\n",
    "The following cell logs some numerical metrics.  Numerical metrics can include metrics such as AUC or MSE.  You should log any parameter or significant output measure in order to understand trends across multiple experiments.  Numerical metrics appear in the **Tracked Metrics** section of the Run Details page, and can be used in charts or KPI's in experiment Run History reports."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log numerical values\n",
    "run.log(name=\"scale factor\", value = scale_factor)\n",
    "run.log(name='Magic Number', value=42 * scale_factor)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging vectors\n",
    "Vectors are good for recording information such as loss curves. You can log a vector by creating a list of numbers, calling ``log_list()`` and supplying a name and the list, or by repeatedly logging a value using the same name.\n",
    "\n",
    "Vectors are presented in Run Details as a chart, and are directly comparable in experiment reports when placed in a chart. \n",
    "\n",
    "**Note:** vectors logged into the run are expected to be relatively small. Logging very large vectors into Azure ML can result in reduced performance. If you need to store large amounts of data associated with the run, you can write the data to file that will be uploaded."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fibonacci_values = [0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89]\n",
    "scaled_values = (i * scale_factor for i in fibonacci_values)\n",
    "\n",
    "# Log a list of values. Note this will generate a single-variable line chart.\n",
    "run.log_list(name='Fibonacci', value=scaled_values)\n",
    "\n",
    "for i in tqdm(range(-10, 10)):\n",
    "    # log a metric value repeatedly, this will generate a single-variable line chart.\n",
    "    run.log(name='Sigmoid', value=1 / (1 + np.exp(-i)))\n",
    "    "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging tables\n",
    "Tables are good for recording related sets of information such as accuracy tables, confusion matrices, etc.  \n",
    "You can log a table in two ways:\n",
    "* Create a dictionary of lists where each list represents a column in the table and call ``log_table()``\n",
    "* Repeatedly call ``log_row()`` providing the same table name with a consistent set of named args as the column values\n",
    "\n",
    "Tables are presented in Run Details as a chart using the first two columns of the table  \n",
    "\n",
    "**Note:** tables logged into the run are expected to be relatively small.  Logging very large tables into Azure ML can result in reduced performance.  If you need to store large amounts of data associated with the run, you can write the data to file that will be uploaded."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to hold a table of values\n",
    "sines = {}\n",
    "sines['angle'] = []\n",
    "sines['sine'] = []\n",
    "\n",
    "for i in tqdm(range(-10, 10)):\n",
    "    angle = i / 2.0 * scale_factor\n",
    "    \n",
    "    # log a 2 (or more) values as a metric repeatedly. This will generate a 2-variable line chart if you have 2 numerical columns.\n",
    "    run.log_row(name='Cosine Wave', angle=angle, cos=np.cos(angle))\n",
    "        \n",
    "    sines['angle'].append(angle)\n",
    "    sines['sine'].append(np.sin(angle))\n",
    "\n",
    "# log a dictionary as a table, this will generate a 2-variable chart if you have 2 numerical columns\n",
    "run.log_table(name='Sine Wave', value=sines)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging images\n",
    "You can directly log _matplotlib_ plots and arbitrary images to your run record.  This code logs a _matplotlib_ pyplot object.  Images show up in the run details page in the Azure ML Portal."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Create a plot\n",
    "import matplotlib.pyplot as plt\n",
    "angle = np.linspace(-3, 3, 50) * scale_factor\n",
    "plt.plot(angle,np.tanh(angle), label='tanh')\n",
    "plt.legend(fontsize=12)\n",
    "plt.title('Hyperbolic Tangent', fontsize=16)\n",
    "plt.grid(True)\n",
    "\n",
    "# Log the plot to the run.  To log an arbitrary image, use the form run.log_image(name, path='./image_path.png')\n",
    "run.log_image(name='Hyperbolic Tangent', plot=plt)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading files\n",
    "\n",
    "Files can also be uploaded explicitly and stored as artifacts along with the run record. These files are also visible in the *Outputs* tab of the Run Details page.\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "directory = 'logging-api'\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.mkdir(directory)\n",
    "\n",
    "file_name = os.path.join(directory, \"myfile.txt\")\n",
    "\n",
    "with open(file_name, \"w\") as f:\n",
    "    f.write('This is an output file that will be uploaded.\\n')\n",
    "\n",
    "# Upload the file explicitly into artifacts \n",
    "run.upload_file(name = file_name, path_or_stream = file_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completing the run\n",
    "\n",
    "Calling `run.complete()` marks the run as completed and triggers the output file collection.  If for any reason you need to indicate the run failed or simply need to cancel the run you can call `run.fail()` or `run.cancel()`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.complete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Analyzing results"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can refresh the run in the Azure portal to see all of your results.  In many cases you will want to analyze runs that were performed previously to inspect the contents or compare results.  Runs can be fetched from their parent Experiment object using the ``Run()`` constructor or the ``experiment.get_runs()`` method. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetched_run = Run(experiment, run_id)\n",
    "fetched_run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call ``run.get_metrics()`` to retrieve all the metrics from a run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetched_run.get_metrics()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call ``run.get_metrics(name = <metric name>)`` to retrieve a metric value by name. Retrieving a single metric can be faster, especially if the run contains many metrics."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetched_run.get_metrics(name = \"scale factor\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the files uploaded for this run by calling ``run.get_file_names()``"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetched_run.get_file_names()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you know the file names in a run, you can download the files using the ``run.download_file()`` method"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('files', exist_ok=True)\n",
    "\n",
    "for f in run.get_file_names():\n",
    "    dest = os.path.join('files', f.split('/')[-1])\n",
    "    print('Downloading file {} to {}...'.format(f, dest))\n",
    "    fetched_run.download_file(f, dest)   "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging a run\n",
    "Often when you analyze the results of a run, you may need to tag that run with important personal or external information.  You can add a tag to a run using the ``run.tag()`` method.  AzureML supports valueless and valued tags."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetched_run.tag(\"My Favorite Run\")\n",
    "fetched_run.tag(\"Competition Rank\", 1)\n",
    "\n",
    "fetched_run.get_tags()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "To experiment more with logging and to understand how metrics can be visualized, go back to the *Start a run* section, try changing the category and scale_factor values and going through the notebook several times.  Play with the KPI, charting, and column selection options on the experiment's Run History reports page to see how the various metrics can be combined and visualized.\n",
    "\n",
    "After learning about all of the logging options, go to the [train on remote vm](..\\train-on-remote-vm\\train-on-remote-vm.ipynb) notebook and experiment with logging from remote compute contexts."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage runs\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Start, monitor and complete a run](#Start,-monitor-and-complete-a-run)\n",
    "1. [Add properties and tags](#Add-properties-and-tags)\n",
    "1. [Query properties and tags](#Query-properties-and-tags)\n",
    "1. [Start and query child runs](#Start-and-query-child-runs)\n",
    "1. [Cancel or fail runs](#Cancel-or-fail-runs)\n",
    "1. [Reproduce a run](#Reproduce-a-run)\n",
    "1. [Next steps](#Next-steps)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "When you're building enterprise-grade machine learning models, it is important to track, organize, monitor and reproduce your training runs. For example, you might want to trace the lineage behind a model deployed to production, and re-run the training experiment to troubleshoot issues. \n",
    "\n",
    "This notebooks shows examples how to use Azure Machine Learning services to manage your training runs."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set.  Otherwise, go through the [configuration](../../../configuration.ipynb) Notebook first if you haven't already to establish your connection to the AzureML Workspace. Also, if you're new to Azure ML, we recommend that you go through [the tutorial](https://docs.microsoft.com/en-us/azure/machine-learning/service/tutorial-train-models-with-aml) first to learn the basic concepts.\n",
    "\n",
    "Let's first import required packages, check Azure ML SDK version, connect to your workspace and create an Experiment to hold the runs."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Run\n",
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "print(azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(workspace=ws, name=\"explore-runs\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start, monitor and complete a run\n",
    "\n",
    "A run is an unit of execution, typically to train a model, but for other purposes as well, such as loading or transforming data. Runs are tracked by Azure ML service, and can be instrumented with metrics and artifact logging.\n",
    "\n",
    "A simplest way to start a run in your interactive Python session is to call *Experiment.start_logging* method. You can then log metrics from within the run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_run = exp.start_logging()\n",
    "\n",
    "notebook_run.log(name=\"message\", value=\"Hello from run!\")\n",
    "\n",
    "print(notebook_run.get_status())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use *get_status method* to get the status of the run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(notebook_run.get_status())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, you can simply enter the run to get a link to Azure Portal details"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method *get_details* gives you more details on the run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_run.get_details()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use *complete* method to end the run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_run.complete()\n",
    "print(notebook_run.get_status())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use Python's *with...as* pattern. The run will automatically complete when moving out of scope. This way you don't need to manually complete the run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with exp.start_logging() as notebook_run:\n",
    "    notebook_run.log(name=\"message\", value=\"Hello from run!\")\n",
    "    print(\"Is it still running?\",notebook_run.get_status())\n",
    "    \n",
    "print(\"Has it completed?\",notebook_run.get_status())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at submitting a run as a separate Python process. To keep the example simple, we submit the run on local computer. Other targets could include remote VMs and Machine Learning Compute clusters in your Azure ML Workspace.\n",
    "\n",
    "We use *hello.py* script as an example. To perform logging, we need to get a reference to the Run instance from within the scope of the script. We do this using *Run.get_context* method."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!more hello.py"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submitted runs take a snapshot of the *source_directory* to use when executing. You can control which files are available to the run by using an *.amlignore* file."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .amlignore\n",
    "# Exclude the outputs directory automatically created by our earlier runs.\n",
    "/outputs"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's submit the run on a local computer. A standard pattern in Azure ML SDK is to create run configuration, and then use *Experiment.submit* method."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = ScriptRunConfig(source_directory='.', script='hello.py')\n",
    "\n",
    "local_script_run = exp.submit(run_config)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view the status of the run as before"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(local_script_run.get_status())\n",
    "local_script_run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submitted runs have additional log files you can inspect using *get_details_with_logs*."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_script_run.get_details_with_logs()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use *wait_for_completion* method to block the local execution until remote run is complete."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_script_run.wait_for_completion(show_output=True)\n",
    "print(local_script_run.get_status())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add properties and tags\n",
    "\n",
    "Properties and tags help you organize your runs. You can use them to describe, for example, who authored the run, what the results were, and what machine learning approach was used. And as you'll later learn, properties and tags can be used to query the history of your runs to find the important ones.\n",
    "\n",
    "For example, let's add \"author\" property to the run:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_script_run.add_properties({\"author\":\"azureml-user\"})\n",
    "print(local_script_run.get_properties())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properties are immutable. Once you assign a value it cannot be changed, making them useful as a permanent record for auditing purposes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    local_script_run.add_properties({\"author\":\"different-user\"})\n",
    "except Exception as e:\n",
    "    print(e)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tags on the other hand can be changed:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_script_run.tag(\"quality\", \"great run\")\n",
    "print(local_script_run.get_tags())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_script_run.tag(\"quality\", \"fantastic run\")\n",
    "print(local_script_run.get_tags())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add a simple string tag. It appears in the tag dictionary with value of None"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_script_run.tag(\"worth another look\")\n",
    "print(local_script_run.get_tags())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query properties and tags\n",
    "\n",
    "You can query runs within an experiment that match specific properties and tags."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(exp.get_runs(properties={\"author\":\"azureml-user\"},tags={\"quality\":\"fantastic run\"}))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(exp.get_runs(properties={\"author\":\"azureml-user\"},tags=\"worth another look\"))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start and query child runs"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use child runs to group together related runs, for example different hyperparameter tuning iterations.\n",
    "\n",
    "Let's use *hello_with_children* script to create a batch of 5 child runs from within a submitted run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!more hello_with_children.py"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = ScriptRunConfig(source_directory='.', script='hello_with_children.py')\n",
    "\n",
    "local_script_run = exp.submit(run_config)\n",
    "local_script_run.wait_for_completion(show_output=True)\n",
    "print(local_script_run.get_status())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start child runs one by one. Note that this is less efficient than submitting a batch of runs, because each creation results in a network call.\n",
    "\n",
    "Child runs too complete automatically as they move out of scope."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with exp.start_logging() as parent_run:\n",
    "    for c,count in enumerate(range(5)):\n",
    "        with parent_run.child_run() as child:\n",
    "            child.log(name=\"Hello from child run\", value=c)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query the child runs belonging to specific parent, use *get_children* method."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(parent_run.get_children())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancel or fail runs\n",
    "\n",
    "Sometimes, you realize that the run is not performing as intended, and you want to cancel it instead of waiting for it to complete.\n",
    "\n",
    "As an example, let's create a Python script with a delay in the middle."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!more hello_with_delay.py"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use *cancel* method to cancel a run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = ScriptRunConfig(source_directory='.', script='hello_with_delay.py')\n",
    "\n",
    "local_script_run = exp.submit(run_config)\n",
    "print(\"Did the run start?\",local_script_run.get_status())\n",
    "local_script_run.cancel()\n",
    "print(\"Did the run cancel?\",local_script_run.get_status())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also mark an unsuccessful run as failed."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_script_run = exp.submit(run_config)\n",
    "local_script_run.fail()\n",
    "print(local_script_run.get_status())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce a run\n",
    "\n",
    "When updating or troubleshooting on a model deployed to production, you sometimes need to revisit the original training run that produced the model. To help you with this, Azure ML service by default creates snapshots of your scripts a the time of run submission:\n",
    "\n",
    "You can use *restore_snapshot* to obtain a zip package of the latest snapshot of the script folder. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_script_run.restore_snapshot(path=\"snapshots\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then extract the zip package, examine the code, and submit your run again."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    " * To learn more about logging APIs, see [logging API notebook](./logging-api/logging-api.ipynb)\n",
    " * To learn more about remote runs, see [train on AML compute notebook](./train-on-amlcompute/train-on-amlcompute.ipynb)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard Integration with Run History\n",
    "\n",
    "1. Run a TensorFlow job locally and view its TB output live.\n",
    "2. The same, for a DSVM.\n",
    "3. And once more, with an AmlCompute cluster.\n",
    "4. Finally, we'll collect all of these historical runs together into a single Tensorboard graph."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* Understand the [architecture and terms](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning\n",
    "* If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration notebook](../../../configuration.ipynb) notebook to:\n",
    "    * install the AML SDK\n",
    "    * create a workspace and its configuration file (`config.json`)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "Opt-in diagnostics for better experience, quality, and security of future releases."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Diagnostics"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.telemetry import set_diagnostics_collection\n",
    "\n",
    "set_diagnostics_collection(send_diagnostics=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set experiment name and create project\n",
    "Choose a name for your run history container in the workspace, and create a folder for the project."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, makedirs\n",
    "experiment_name = 'tensorboard-demo'\n",
    "\n",
    "# experiment folder\n",
    "exp_dir = './sample_projects/' + experiment_name\n",
    "\n",
    "if not path.exists(exp_dir):\n",
    "    makedirs(exp_dir)\n",
    "\n",
    "# runs we started in this session, for the finale\n",
    "runs = []"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Tensorflow Tensorboard demo code\n",
    "\n",
    "Tensorflow's repository has an MNIST demo with extensive Tensorboard instrumentation. We'll use it here for our purposes.\n",
    "\n",
    "Note that we don't need to make any code changes at all - the code works without modification from the Tensorflow repository."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "tf_code = requests.get(\"https://raw.githubusercontent.com/tensorflow/tensorflow/r2.1/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py\")\n",
    "input_code = requests.get(\"https://raw.githubusercontent.com/tensorflow/tensorflow/r2.1/tensorflow/examples/tutorials/mnist/input_data.py\")\n",
    "with open(os.path.join(exp_dir, \"mnist_with_summaries.py\"), \"w\") as file:\n",
    "    file.write(tf_code.text.replace(\"from tensorflow.examples.tutorials.mnist import input_data\", \"import input_data\"))\n",
    "with open(os.path.join(exp_dir, \"input_data.py\"), \"w\") as file:\n",
    "    file.write(input_code.text)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and run locally\n",
    "\n",
    "We'll start by running this locally. While it might not initially seem that useful to use this for a local run - why not just run TB against the files generated locally? - even in this case there is some value to using this feature. Your local run will be registered in the run history, and your Tensorboard logs will be uploaded to the artifact store associated with this run. Later, you'll be able to restore the logs from any run, regardless of where it happened.\n",
    "\n",
    "Note that for this run, you will need to install Tensorflow on your local machine by yourself. Further, the Tensorboard module (that is, the one included with Tensorflow) must be accessible to this notebook's kernel, as the local machine is what runs Tensorboard. In addition, you will also need to have the `azureml-tensorboard` package installed."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "myenv = Environment(\"myenv\")\n",
    "myenv.python.user_managed_dependencies = True\n",
    "\n",
    "# You can choose a specific Python environment by pointing to a Python path \n",
    "#myenv.python.interpreter_path = '/home/ninghai/miniconda3/envs/sdk2/bin/python'"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.core.script_run_config import ScriptRunConfig\n",
    "\n",
    "logs_dir = os.path.join(os.curdir, os.path.join(\"logs\", \"tb-logs\"))\n",
    "data_dir = os.path.abspath(os.path.join(os.curdir, \"mnist_data\"))\n",
    "\n",
    "if not path.exists(data_dir):\n",
    "    makedirs(data_dir)\n",
    "\n",
    "os.environ[\"TEST_TMPDIR\"] = data_dir\n",
    "\n",
    "# Writing logs to ./logs results in their being uploaded to Artifact Service,\n",
    "# and thus, made accessible to our Tensorboard instance.\n",
    "arguments = [\"--log_dir\", logs_dir]\n",
    "\n",
    "# Create an experiment\n",
    "exp = Experiment(ws, experiment_name)\n",
    "\n",
    "# If you would like the run to go for longer, add --max_steps 5000 to the arguments list:\n",
    "# arguments += [\"--max_steps\", \"5000\"]\n",
    "\n",
    "src = ScriptRunConfig(exp_dir,\n",
    "                      script=\"mnist_with_summaries.py\",\n",
    "                      arguments=arguments,\n",
    "                      environment=myenv)\n",
    "\n",
    "run = exp.submit(src)\n",
    "# You can also wait for the run to complete\n",
    "# run.wait_for_completion(show_output=True)\n",
    "runs.append(run)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Tensorboard\n",
    "\n",
    "Now, while the run is in progress, we just need to start Tensorboard with the run as its target, and it will begin streaming logs."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "tensorboard-sample"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.tensorboard import Tensorboard\n",
    "\n",
    "# The Tensorboard constructor takes an array of runs, so be sure and pass it in as a single-element array here\n",
    "tb = Tensorboard([run])\n",
    "\n",
    "# If successful, start() returns a string with the URI of the instance.\n",
    "tb.start()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Tensorboard\n",
    "\n",
    "When you're done, make sure to call the `stop()` method of the Tensorboard object, or it will stay running even after your job completes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.stop()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, with a DSVM\n",
    "\n",
    "Tensorboard uploading works with all compute targets. Here we demonstrate it from a DSVM.\n",
    "Note that the Tensorboard instance itself will be run by the notebook kernel. Again, this means this notebook's kernel must have access to the Tensorboard module.\n",
    "\n",
    "If you are unfamiliar with DSVM configuration, check [Train in a remote VM](../../training/train-on-remote-vm/train-on-remote-vm.ipynb) for a more detailed breakdown.\n",
    "\n",
    "**Note**: To streamline the compute that Azure Machine Learning creates, we are making updates to support creating only single to multi-node `AmlCompute`. The `DSVMCompute` class will be deprecated in a later release, but the DSVM can be created using the below single line command and then attached(like any VM) using the sample code below. Also note, that we only support Linux VMs for remote execution from AML and the commands below will spin a Linux VM only.\n",
    "\n",
    "```shell\n",
    "# create a DSVM in your resource group\n",
    "# note you need to be at least a contributor to the resource group in order to execute this command successfully.\n",
    "(myenv) $ az vm create --resource-group <resource_group_name> --name <some_vm_name> --image microsoft-dsvm:linux-data-science-vm-ubuntu:linuxdsvmubuntu:latest --admin-username <username> --admin-password <password> --generate-ssh-keys --authentication-type password\n",
    "```\n",
    "You can also use [this url](https://portal.azure.com/#create/microsoft-dsvm.linux-data-science-vm-ubuntulinuxdsvmubuntu) to create the VM using the Azure Portal."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, RemoteCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "username = os.getenv('AZUREML_DSVM_USERNAME', default='<my_username>')\n",
    "address = os.getenv('AZUREML_DSVM_ADDRESS', default='<ip_address_or_fqdn>')\n",
    "\n",
    "compute_target_name = 'cpudsvm'\n",
    "# if you want to connect using SSH key instead of username/password you can provide parameters private_key_file and private_key_passphrase \n",
    "try:\n",
    "    attached_dsvm_compute = RemoteCompute(workspace=ws, name=compute_target_name)\n",
    "    print('found existing:', attached_dsvm_compute.name)\n",
    "except ComputeTargetException:\n",
    "    config = RemoteCompute.attach_configuration(username=username,\n",
    "                                                address=address,\n",
    "                                                ssh_port=22,\n",
    "                                                private_key_file='./.ssh/id_rsa')\n",
    "    attached_dsvm_compute = ComputeTarget.attach(ws, compute_target_name, config)\n",
    "    \n",
    "    attached_dsvm_compute.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit run using TensorFlow estimator\n",
    "\n",
    "Instead of manually configuring the DSVM environment, we can use the TensorFlow estimator and everything is set up automatically."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "script_params = {\"--log_dir\": \"./logs\"}\n",
    "\n",
    "# If you want the run to go longer, set --max-steps to a higher number.\n",
    "# script_params[\"--max_steps\"] = \"5000\"\n",
    "\n",
    "tf_estimator = TensorFlow(source_directory=exp_dir,\n",
    "                          compute_target=attached_dsvm_compute,\n",
    "                          entry_script='mnist_with_summaries.py',\n",
    "                          script_params=script_params,\n",
    "                          framework_version=\"2.0\")\n",
    "\n",
    "run = exp.submit(tf_estimator)\n",
    "\n",
    "runs.append(run)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Tensorboard with this run\n",
    "\n",
    "Just like before."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Tensorboard constructor takes an array of runs, so be sure and pass it in as a single-element array here\n",
    "tb = Tensorboard([run])\n",
    "\n",
    "# If successful, start() returns a string with the URI of the instance.\n",
    "tb.start()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Tensorboard\n",
    "\n",
    "When you're done, make sure to call the `stop()` method of the Tensorboard object, or it will stay running even after your job completes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.stop()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once more, with an AmlCompute cluster\n",
    "\n",
    "Just to prove we can, let's create an AmlCompute CPU cluster, and run our demo there, as well."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"cpu-cluster\"\n",
    "\n",
    "cts = ws.compute_targets\n",
    "found = False\n",
    "if cluster_name in cts and cts[cluster_name].type == 'AmlCompute':\n",
    "    found = True\n",
    "    print('Found existing compute target.')\n",
    "    compute_target = cts[cluster_name]\n",
    "if not found:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2', \n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True, min_node_count=None)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "# print(compute_target.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit run using TensorFlow estimator\n",
    "\n",
    "Again, we can use the TensorFlow estimator and everything is set up automatically."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "script_params = {\"--log_dir\": \"./logs\"}\n",
    "\n",
    "# If you want the run to go longer, set --max-steps to a higher number.\n",
    "# script_params[\"--max_steps\"] = \"5000\"\n",
    "\n",
    "tf_estimator = TensorFlow(source_directory=exp_dir,\n",
    "                          compute_target=compute_target,\n",
    "                          entry_script='mnist_with_summaries.py',\n",
    "                          script_params=script_params,\n",
    "                          framework_version=\"2.0\")\n",
    "\n",
    "run = exp.submit(tf_estimator)\n",
    "\n",
    "runs.append(run)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Tensorboard with this run\n",
    "\n",
    "Once more..."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Tensorboard constructor takes an array of runs, so be sure and pass it in as a single-element array here\n",
    "tb = Tensorboard([run])\n",
    "\n",
    "# If successful, start() returns a string with the URI of the instance.\n",
    "tb.start()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Tensorboard\n",
    "\n",
    "When you're done, make sure to call the `stop()` method of the Tensorboard object, or it will stay running even after your job completes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.stop()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finale\n",
    "\n",
    "If you've paid close attention, you'll have noticed that we've been saving the run objects in an array as we went along. We can start a Tensorboard instance that combines all of these run objects into a single process. This way, you can compare historical runs. You can even do this with live runs; if you made some of those previous runs longer via the `--max_steps` parameter, they might still be running, and you'll see them live in this instance as well."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Tensorboard constructor takes an array of runs...\n",
    "# and it turns out that we have been building one of those all along.\n",
    "tb = Tensorboard(runs)\n",
    "\n",
    "# If successful, start() returns a string with the URI of the instance.\n",
    "tb.start()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Tensorboard\n",
    "\n",
    "As you might already know, make sure to call the `stop()` method of the Tensorboard object, or it will stay running (until you kill the kernel associated with this notebook, at least)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.stop()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/tensorboard/tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/export-run-history-to-tensorboard/export-run-history-to-tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* Understand the [architecture and terms](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning\n",
    "* If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration notebook](../../../configuration.ipynb) notebook to:\n",
    "    * install the AML SDK\n",
    "    * create a workspace and its configuration file (`config.json`)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/export-run-history-to-tensorboard/export-run-history-to-tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/export-run-history-to-tensorboard/export-run-history-to-tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/export-run-history-to-tensorboard/export-run-history-to-tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/export-run-history-to-tensorboard/export-run-history-to-tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set experiment name and start the run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/export-run-history-to-tensorboard/export-run-history-to-tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'export-to-tensorboard'\n",
    "exp = Experiment(ws, experiment_name)\n",
    "root_run = exp.start_logging()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/export-run-history-to-tensorboard/export-run-history-to-tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load diabetes dataset, a well-known built-in small dataset that comes with scikit-learn\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "\n",
    "columns = ['age', 'gender', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "data = {\n",
    "    \"train\":{\"x\":x_train, \"y\":y_train},        \n",
    "    \"test\":{\"x\":x_test, \"y\":y_test}\n",
    "}"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/export-run-history-to-tensorboard/export-run-history-to-tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example experiment\n",
    "from tqdm import tqdm\n",
    "\n",
    "alphas = [.1, .2, .3, .4, .5, .6 , .7]\n",
    "\n",
    "# try a bunch of alpha values in a Linear Regression (Ridge) model\n",
    "for alpha in tqdm(alphas):\n",
    "    # create a bunch of child runs\n",
    "    with root_run.child_run(\"alpha\" + str(alpha)) as run:\n",
    "        # More data science stuff\n",
    "        reg = Ridge(alpha=alpha)\n",
    "        reg.fit(data[\"train\"][\"x\"], data[\"train\"][\"y\"])\n",
    "        \n",
    "        preds = reg.predict(data[\"test\"][\"x\"])\n",
    "        mse = mean_squared_error(preds, data[\"test\"][\"y\"])\n",
    "        # End train and eval\n",
    "\n",
    "        # log alpha, mean_squared_error and feature names in run history\n",
    "        root_run.log(\"alpha\", alpha)\n",
    "        root_run.log(\"mse\", mse)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/export-run-history-to-tensorboard/export-run-history-to-tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Run History to Tensorboard logs"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/export-run-history-to-tensorboard/export-run-history-to-tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "tensorboard-export-sample"
    ]
   },
   "outputs": [],
   "source": [
    "# Export Run History to Tensorboard logs\n",
    "from azureml.tensorboard.export import export_to_tensorboard\n",
    "import os\n",
    "\n",
    "logdir = 'exportedTBlogs'\n",
    "log_path = os.path.join(os.getcwd(), logdir)\n",
    "try:\n",
    "    os.stat(log_path)\n",
    "except os.error:\n",
    "    os.mkdir(log_path)\n",
    "print(logdir)\n",
    "\n",
    "# export run history for the project\n",
    "export_to_tensorboard(root_run, logdir)\n",
    "\n",
    "# or export a particular run\n",
    "# export_to_tensorboard(run, logdir)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/export-run-history-to-tensorboard/export-run-history-to-tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_run.complete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/export-run-history-to-tensorboard/export-run-history-to-tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Tensorboard\n",
    "\n",
    "Or you can start the Tensorboard outside this notebook to view the result"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/export-run-history-to-tensorboard/export-run-history-to-tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.tensorboard import Tensorboard\n",
    "\n",
    "# The Tensorboard constructor takes an array of runs, so be sure and pass it in as a single-element array here\n",
    "tb = Tensorboard([], local_root=logdir, port=6006)\n",
    "\n",
    "# If successful, start() returns a string with the URI of the instance.\n",
    "tb.start()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/export-run-history-to-tensorboard/export-run-history-to-tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Tensorboard\n",
    "\n",
    "When you're done, make sure to call the `stop()` method of the Tensorboard object."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/export-run-history-to-tensorboard/export-run-history-to-tensorboard.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.stop()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/tensorboard/export-run-history-to-tensorboard/export-run-history-to-tensorboard.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use MLflow with Azure Machine Learning for Remote Training Run\n",
    "\n",
    "This example shows you how to use MLflow tracking APIs together with Azure Machine Learning services for storing your metrics and artifacts, from local Notebook run. You'll learn how to:\n",
    "\n",
    " 1. Set up MLflow tracking URI so as to use Azure ML\n",
    " 2. Create experiment\n",
    " 3. Train a model on Machine Learning Compute while logging metrics and artifacts\n",
    " 4. View your experiment within your Azure ML Workspace in Azure Portal."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Make sure you have completed the [Configuration](../../../configuration.ipnyb) notebook to set up your Azure Machine Learning workspace and ensure other common prerequisites are met."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "\n",
    "Check Azure ML SDK version installed on your computer, and then connect to your Workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n",
    "\n",
    "ws = Workspace.from_config()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create a Machine Learning Compute cluster for submitting the remote run. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cluster_name = \"cpu-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print(\"Found existing cpu-cluster\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new cpu-cluster\")\n",
    "    \n",
    "    # Specify the configuration for the new cluster\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_D2_V2\",\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=2)\n",
    "\n",
    "    # Create the cluster with the specified name and configuration\n",
    "    cpu_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "    \n",
    "    # Wait for the cluster to complete, show the output log\n",
    "    cpu_cluster.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Azure ML Experiment"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps show how to submit a training Python script to a cluster as an Azure ML run, while logging happens through MLflow APIs to your Azure ML Workspace. Let's first create an experiment to hold the training runs."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = \"experiment-with-mlflow\"\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrument remote training script using MLflow"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use [*train_diabetes.py*](train_diabetes.py) to train a regression model against diabetes dataset as the example. Note that the training script uses mlflow.start_run() to start logging, and then logs metrics, saves the trained scikit-learn model, and saves a plot as an artifact.\n",
    "\n",
    "Run following command to view the script file. Notice the mlflow logging statements, and also notice that the script doesn't have explicit dependencies on azureml library."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_script = 'train_diabetes.py'\n",
    "with open(training_script, 'r') as f:\n",
    "    print(f.read())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit Run to Cluster \n",
    "\n",
    "Let's submit the run to cluster. When running on the remote cluster as submitted run, Azure ML sets the MLflow tracking URI to point to your Azure ML Workspace, so that the metrics and artifacts are automatically logged there.\n",
    "\n",
    "Note that you have to specify the packages your script depends on, including *azureml-mlflow* that implicitly enables the MLflow logging to Azure ML. \n",
    "\n",
    "First, create a environment with Docker enable and required package dependencies specified."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "mlflow"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "env = Environment(name=\"mlflow-env\")\n",
    "\n",
    "env.docker.enabled = True\n",
    "\n",
    "# Specify conda dependencies with scikit-learn and temporary pointers to mlflow extensions\n",
    "cd = CondaDependencies.create(\n",
    "    conda_packages=[\"scikit-learn\", \"matplotlib\"],\n",
    "    pip_packages=[\"azureml-mlflow\", \"numpy\"]\n",
    "    )\n",
    "\n",
    "env.python.conda_dependencies = cd"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, specify a script run configuration that includes the training script, environment and CPU cluster created earlier."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "src = ScriptRunConfig(source_directory=\".\",\n",
    "                      script=training_script,\n",
    "                      compute_target=cpu_cluster,\n",
    "                      environment=env)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, submit the run. Note that the first instance of the run typically takes longer as the Docker-based environment is created, several minutes. Subsequent runs reuse the image and are faster."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(src)\n",
    "run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can navigate to your Azure ML Workspace at Azure Portal to view the run metrics and artifacts. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get the metrics and bring them to your local notebook, and view the details of the run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_metrics()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_details()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "\n",
    " * [Deploy the model as a web service](../deploy-model/deploy-model.ipynb)\n",
    " * [Learn more about Azure Machine Learning compute options](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-remote/train-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-remote/train-projects-remote.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-remote/train-projects-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-remote/train-projects-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with MLflow Projects on AML Compute\n",
    "\n",
    "Train MLflow Projects on Azure Machine Learning Compute.\n",
    "\n",
    "Train MLflow Projects on your machine with AzureML compute and tracking. In this notebook you will:\n",
    "\n",
    "1. Set up MLflow tracking URI to track experiments and metrics in AzureML\n",
    "2. Create experiment\n",
    "3. Set up an MLflow project to run on AzureML compute\n",
    "4. Submit an MLflow project run and view it in an AzureML workspace \n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-remote/train-projects-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites \n",
    "\n",
    "If you are using a Notebook VM, you are all set. Otherwise, go through the [Configuration](../../../../configuration.ipnyb) notebook to set up your Azure Machine Learning workspace and ensure other common prerequisites are met.\n",
    "\n",
    "Make sure you have the following before staring the notebook: \n",
    "- Connected to an AML Workspace \n",
    "- Have an existing [Azure ML Compute cluster](https://docs.microsoft.com/azure/machine-learning/how-to-create-attach-compute-sdk#amlcompute) in that Workspace \n",
    "- Have an MLproject file with a modified environment specification \n",
    "\n",
    "Add the azureml-mlflow package as a pip dependency to your environment configuration file (conda.yaml). The project can run without this addition, but key artifacts and metrics will not be logged to your Workspace. An example conda.yaml is included in this tutorial folder with the necessary packages. \n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-remote/train-projects-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up \n",
    "\n",
    "Check the Azure ML and MLflow SDK version installed on your computer and connect to your workspace"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-remote/train-projects-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import mlflow\n",
    "import mlflow.azureml\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n",
    "print(\"MLflow version:\", mlflow.version.VERSION)\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-remote/train-projects-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Tracking Store and Experiment\n",
    "\n",
    "### Set Tracking URI \n",
    "\n",
    "Set the MLflow tracking URI to point to your Azure ML Workspace. The subsequent logging calls from MLflow APIs will go to Azure ML services and will be tracked under your Workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-remote/train-projects-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-remote/train-projects-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Experiment\n",
    "\n",
    "Create an Mlflow Experiment to organize your runs. It can be set either by passing the name as a **parameter** in the mlflow.projects.run call or by the following,"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-remote/train-projects-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_name = \"train-project-amlcompute\"\n",
    "mlflow.set_experiment(experiment_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-remote/train-projects-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Backend Configuration Object\n",
    "\n",
    "The backend configuration object will store necesary information for the integration such as the compute target and whether to use your local managed environment or a system managed environment. \n",
    "\n",
    "The integration will accept \"COMPUTE\" and \"USE_CONDA\" as parameters where \"COMPUTE\" is set to the name of your remote compute cluster and \"USE_CONDA\" which creates a new environment for the project from the environment configuration file. If \"COMPUTE\" is present in the object, the project will be automatically submitted to the remote compute and ignore \"USE_CONDA\". MLflow accepts a dictionary object or a JSON file."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-remote/train-projects-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary\n",
    "backend_config = {\"COMPUTE\": \"cpu-cluster\", \"USE_CONDA\": False}\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-remote/train-projects-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify your Environment specification\n",
    "\n",
    "Add the azureml-mlflow package as a pip dependency to your environment configuration file (conda.yaml). The project can run without this addition, but key artifacts and metrics will not be logged to your Workspace. An example conda.yaml is included in the notebook folder. Adding it to to the file will look like this,\n",
    "\n",
    "```\n",
    "name: mlflow-example\n",
    "channels:\n",
    "  - defaults\n",
    "  - anaconda\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.6\n",
    "  - scikit-learn=0.19.1\n",
    "  - pip\n",
    "  - pip:\n",
    "    - mlflow\n",
    "    - azureml-mlflow\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-remote/train-projects-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Run \n",
    "\n",
    "Submit the mlflow project run using aml compute and ensure the **backened** parameter is set to azureml.\n",
    "\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-remote/train-projects-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "remote_mlflow_run = mlflow.projects.run(uri=\".\", \n",
    "                                    parameters={\"alpha\":0.3},\n",
    "                                    backend = \"azureml\",\n",
    "                                    backend_config = backend_config,\n",
    "                                    synchronous=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-remote/train-projects-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-remote/train-projects-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View run \n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-remote/train-projects-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "remote_mlflow_run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-remote/train-projects-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Try out these notebooks to learn more about MLflow-Azure Machine Learning integration:\n",
    "\n",
    " * [Train a model using remote compute on Azure Cloud](../train-on-remote/train-on-remote.ipynb)\n",
    " * [Deploy the model as a web service](../deploy-model/deploy-model.ipynb)\n",
    " * [Train a model using Pytorch and MLflow](../../ml-frameworks/using-mlflow/train-and-deploy-pytorch)\n",
    "\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-remote/train-projects-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-local/train-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/using-mlflow/train-local/train-local.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-local/train-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use MLflow with Azure Machine Learning for Local Training Run\n",
    "\n",
    "This example shows you how to use mlflow tracking APIs together with Azure Machine Learning services for storing your metrics and artifacts, from local Notebook run. You'll learn how to:\n",
    "\n",
    " 1. Set up MLflow tracking URI so as to use Azure ML\n",
    " 2. Create experiment\n",
    " 3. Train a model on your local computer while logging metrics and artifacts\n",
    " 4. View your experiment within your Azure ML Workspace in Azure Portal.\n",
    "\n",
    "## Prerequisites and Set-up\n",
    "\n",
    "Make sure you have completed the [Configuration](../../../configuration.ipnyb) notebook to set up your Azure Machine Learning workspace and ensure other common prerequisites are met.\n",
    "\n",
    "Install azureml-mlflow package before running this notebook. Note that mlflow itself gets installed as dependency if you haven't installed it yet.\n",
    "\n",
    "```\n",
    "pip install azureml-mlflow\n",
    "```\n",
    "\n",
    "This example also uses scikit-learn and matplotlib packages. Install them:\n",
    "```\n",
    "pip install scikit-learn matplotlib\n",
    "```\n",
    "\n",
    "Then, import necessary packages"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-local/train-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-local/train-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set tracking URI\n",
    "\n",
    "Set the MLflow tracking URI to point to your Azure ML Workspace. The subsequent logging calls from MLflow APIs will go to Azure ML services and will be tracked under your Workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-local/train-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-local/train-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experiment\n",
    "\n",
    "In both MLflow and Azure ML, training runs are grouped into experiments. Let's create one for our experimentation."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-local/train-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"experiment-with-mlflow\"\n",
    "mlflow.set_experiment(experiment_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-local/train-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and test data set\n",
    "\n",
    "This example uses diabetes dataset to build a simple regression model. Let's load the dataset and split it into training and test sets."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-local/train-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_diabetes(return_X_y = True)\n",
    "columns = ['age', 'gender', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "data = {\n",
    "    \"train\":{\"X\": X_train, \"y\": y_train},        \n",
    "    \"test\":{\"X\": X_test, \"y\": y_test}\n",
    "}\n",
    "\n",
    "print (\"Data contains\", len(data['train']['X']), \"training samples and\",len(data['test']['X']), \"test samples\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-local/train-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train while logging metrics and artifacts\n",
    "\n",
    "Next, start a mlflow run to train a scikit-learn regression model. Note that the training script has been instrumented using MLflow to:\n",
    " * Log model hyperparameter alpha value\n",
    " * Log mean squared error against test set\n",
    " * Save the scikit-learn based regression model produced by training\n",
    " * Save an image that shows actuals vs predictions against test set.\n",
    " \n",
    "These metrics and artifacts have been recorded to your Azure ML Workspace; in the next step you'll learn how to view them."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-local/train-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a run object in the experiment\n",
    "model_save_path = \"model\"\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    # Log the algorithm parameter alpha to the run\n",
    "    mlflow.log_metric('alpha', 0.03)\n",
    "    # Create, fit, and test the scikit-learn Ridge regression model\n",
    "    regression_model = Ridge(alpha=0.03)\n",
    "    regression_model.fit(data['train']['X'], data['train']['y'])\n",
    "    preds = regression_model.predict(data['test']['X'])\n",
    "\n",
    "    # Log mean squared error\n",
    "    print('Mean Squared Error is', mean_squared_error(data['test']['y'], preds))\n",
    "    mlflow.log_metric('mse', mean_squared_error(data['test']['y'], preds))\n",
    "    \n",
    "    # Save the model to the outputs directory for capture\n",
    "    mlflow.sklearn.log_model(regression_model,model_save_path)\n",
    "    \n",
    "    # Plot actuals vs predictions and save the plot within the run\n",
    "    fig = plt.figure(1)\n",
    "    idx = np.argsort(data['test']['y'])\n",
    "    plt.plot(data['test']['y'][idx],preds[idx])\n",
    "    fig.savefig(\"actuals_vs_predictions.png\")\n",
    "    mlflow.log_artifact(\"actuals_vs_predictions.png\") "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-local/train-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can open the report page for your experiment and runs within it from Azure Portal.\n",
    "\n",
    "Select one of the runs to view the metrics, and the plot you saved. The saved scikit-learn model appears under **outputs** tab."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-local/train-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws.experiments[experiment_name]"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-local/train-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "\n",
    "Try out these notebooks to learn more about MLflow-Azure Machine Learning integration:\n",
    "\n",
    " * [Train a model using remote compute on Azure Cloud](../train-on-remote/train-on-remote.ipynb)\n",
    " * [Deploy the model as a web service](../deploy-model/deploy-model.ipynb)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-local/train-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-local/train-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-local/train-projects-local.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-local/train-projects-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-local/train-projects-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with MLflow Projects on local compute\n",
    "\n",
    "Train MLflow Projects on your machine with local compute and AzureML tracking. In this notebook you will:\n",
    "\n",
    "1. Set up MLflow tracking URI to track experiments and metrics in AzureML\n",
    "2. Create experiment\n",
    "3. Set up an MLflow project to run on AzureML compute\n",
    "4. Submit an MLflow project run and view it in an AzureML workspace "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-local/train-projects-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites \n",
    "\n",
    "If you are using a Notebook VM, you're all set. Otherwise, go through the [Configuration](../../../../configuration.ipnyb) notebook to set up your Azure Machine Learning workspace and ensure other common prerequisites are met.\n",
    "\n",
    "Install azureml-mlflow package before running this notebook. Note that mlflow itself gets installed as dependency if you haven't installed it yet.\n",
    "\n",
    "```\n",
    "pip install azureml-mlflow\n",
    "```\n",
    "\n",
    "This example also uses scikit-learn. Install them using the following:\n",
    "```\n",
    "pip install scikit-learn matplotlib\n",
    "```\n",
    "\n",
    "Make sure you have the following before starting the notebook: \n",
    "- Connected to an AzureML Workspace \n",
    "- Your local conda environment has the necessary packages needed to run this project\n",
    "\n",
    "\n",
    "\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-local/train-projects-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "\n",
    "Configure your workspace and check package versions"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-local/train-projects-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import mlflow\n",
    "import mlflow.azureml\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n",
    "print(\"MLflow version:\", mlflow.version.VERSION)\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-local/train-projects-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Tracking Store and Experiment\n",
    "\n",
    "### Set the Tracking Store\n",
    "Set the MLflow tracking URI to point to your Azure ML Workspace. The subsequent logging calls from MLflow APIs will go to Azure ML services and will be tracked under your Workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-local/train-projects-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-local/train-projects-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Experiment\n",
    "Create an Mlflow Experiment to organize your runs. It can be set either by passing the name as a parameter in the mlflow.projects.run call or by the following,"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-local/train-projects-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_name = \"train-project-local\"\n",
    "mlflow.set_experiment(experiment_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-local/train-projects-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Backend Configuration Object\n",
    "\n",
    "The backend configuration object will store necesary information for the integration such as the compute target and whether to use your local managed environment or a system managed environment. \n",
    "\n",
    "The integration will accept \"COMPUTE\" and \"USE_CONDA\" as parameters where \"COMPUTE\" is set to the name of a remote target (not applicable for this training example) and \"USE_CONDA\" which creates a new environment for the project from the environment configuration file. You must set this to \"False\" and include it in the backend configuration object if you want to use your local environment for the project run. Mlflow accepts a dictionary object or a JSON file."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-local/train-projects-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary\n",
    "backend_config = {\"USE_CONDA\": False}\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-local/train-projects-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the Integration to your Environment Configuration\n",
    "\n",
    "Add the azureml-mlflow package as a pip dependency to your environment configuration file (conda.yaml). The project can run without this addition, but key artifacts and metrics will not be logged to your Workspace. An example conda.yaml file is included in this notebook folder. Adding it to to the file will look like this,\n",
    "\n",
    "```\n",
    "name: mlflow-example\n",
    "channels:\n",
    "  - defaults\n",
    "  - anaconda\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.6\n",
    "  - scikit-learn=0.19.1\n",
    "  - pip\n",
    "  - pip:\n",
    "    - mlflow\n",
    "    - azureml-mlflow\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-local/train-projects-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Managed environment\n",
    "For using your local conda environment, set `use_conda = False` in the backend_config object. Ensure your local environment has all the necessary packages for running the project and you are specifying the **backend parameter** in any run call to be **\"azureml\"**."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-local/train-projects-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_env_run = mlflow.projects.run(uri=\".\", \n",
    "                                    parameters={\"alpha\":0.3},\n",
    "                                    backend = \"azureml\",\n",
    "                                    use_conda=False,\n",
    "                                    backend_config = backend_config)\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-local/train-projects-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_env_run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-local/train-projects-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: All these calculations were run on your local machine, in the conda environment you defined above. You can find the results in:\n",
    "- Your AzureML Experiments (a link to the run will be provided in the console)\n",
    "- ~/.azureml/envs/azureml_xxxx for the conda environment you just created\n",
    "- ~/AppData/Local/Temp/azureml_runs/train-on-local_xxxx for the machine learning models you trained (this path may differ depending on the platform you use). This folder also contains\n",
    "    - Logs (under azureml_logs/)\n",
    "    - Output pickled files (under outputs/)\n",
    "    - The configuration files (credentials, local and docker image setups)\n",
    "    - The train.py and mylib.py scripts\n",
    "    - The current notebook"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-local/train-projects-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Mananged Environment\n",
    "\n",
    "Now, instead of managing the setup of the environment yourself, you can ask the system to build a new conda environment for you using the environment configuration file in this project. If a backend configuration object is not provided in the call, the integration will default to creating a new conda environment. The environment is built once, and will be reused in subsequent executions as long as the conda dependencies remain unchanged.\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-local/train-projects-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "backend_config = {\"USE_CONDA\": True}\n",
    "local_mlproject_run = mlflow.projects.run(uri=\".\", \n",
    "                                    parameters={\"alpha\":0.3},\n",
    "                                    backend = \"azureml\",\n",
    "                                    backend_config = backend_config)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-local/train-projects-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps \n",
    "\n",
    "Try out these notebooks to learn more about MLflow-Azure Machine Learning integration:\n",
    "\n",
    " * [Train a model using remote compute on Azure Cloud](../train-on-remote/train-on-remote.ipynb)\n",
    " * [Deploy the model as a web service](../deploy-model/deploy-model.ipynb)\n",
    " * [Train a model using Pytorch and MLflow](../../ml-frameworks/using-mlflow/train-and-deploy-pytorch)\n",
    "\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/track-and-monitor-experiments/using-mlflow/train-projects-local/train-projects-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/reinforcement-learning/cartpole_on_single_compute/cartpole_sc.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning in Azure Machine Learning - Cartpole Problem on Single Compute\n",
    "\n",
    "Reinforcement Learning in Azure Machine Learning is a managed service for running reinforcement learning training and simulation. With Reinforcement Learning in Azure Machine Learning, data scientists can start developing reinforcement learning systems on one machine, and scale to compute targets with 100s of nodes if needed.\n",
    "\n",
    "This example shows how to use Reinforcement Learning in Azure Machine Learning to train a Cartpole playing agent on a single compute. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cartpole problem\n",
    "\n",
    "Cartpole, also known as [Inverted Pendulum](https://en.wikipedia.org/wiki/Inverted_pendulum), is a pendulum with a center of mass above its pivot point. This formation is essentially unstable and will easily fall over but can be kept balanced by applying appropriate horizontal forces to the pivot point.\n",
    "\n",
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "    <th>\n",
    "      <img src=\"./images/cartpole.png\" alt=\"Cartpole image\" /> \n",
    "    </th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <th><p>Fig 1. Cartpole problem schematic description (from <a href=\"https://towardsdatascience.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288\">towardsdatascience.com</a>).</p></th>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "The goal here is to train an agent to keep the cartpole balanced by applying appropriate forces to the pivot point.\n",
    "\n",
    "See [this video](https://www.youtube.com/watch?v=XiigTGKZfks) for a real-world demonstration of cartpole problem."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisite\n",
    "The user should have completed the Azure Machine Learning Tutorial: [Get started creating your first ML experiment with the Python SDK](https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-1st-experiment-sdk-setup). You will need to make sure that you have a valid subscription ID, a resource group, and an Azure Machine Learning workspace. All datastores and datasets you use should be associated with your workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Development Environment\n",
    "The following subsections show typical steps to setup your development environment. Setup includes:\n",
    "\n",
    "* Connecting to a workspace to enable communication between your local machine and remote resources\n",
    "* Creating an experiment to track all your runs\n",
    "* Creating a remote compute target to use for training"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Machine Learning SDK \n",
    "Display the Azure Machine Learning SDK version."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "\n",
    "print(\"Azure Machine Learning SDK Version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Azure Machine Learning workspace\n",
    "Get a reference to an existing Azure Machine Learning workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, sep = ' | ')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new compute resource or attach an existing one\n",
    "\n",
    "A compute target is a designated compute resource where you run your training and simulation scripts. This location may be your local machine or a cloud-based compute resource. The code below shows how to create a cloud-based compute target. For more information see [What are compute targets in Azure Machine Learning?](https://docs.microsoft.com/en-us/azure/machine-learning/concept-compute-target)\n",
    "\n",
    "**Note: Creation of a compute resource can take several minutes**. Please make sure to change `STANDARD_D2_V2` to a [size available in your region](https://azure.microsoft.com/en-us/global-infrastructure/services/?products=virtual-machines)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "import os\n",
    "\n",
    "# Choose a name and maximum size for your cluster\n",
    "compute_name = \"cpu-cluster-d2\"\n",
    "compute_min_nodes = 0\n",
    "compute_max_nodes = 4\n",
    "vm_size = \"STANDARD_D2_V2\"\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    print(\"Found an existing compute target of name: \" + compute_name)\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    # Note: you may want to make sure compute_target is of type AmlCompute        \n",
    "else:\n",
    "    print(\"Creating new compute target...\")\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=vm_size,\n",
    "        min_nodes=compute_min_nodes, \n",
    "        max_nodes=compute_max_nodes)\n",
    "        \n",
    "    # Create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "print(compute_target.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Azure Machine Learning experiment\n",
    "Create an experiment to track the runs in your workspace. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.experiment import Experiment\n",
    "\n",
    "experiment_name = 'CartPole-v0-SC'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Cartpole Agent\n",
    "To facilitate reinforcement learning, Azure Machine Learning Python SDK provides a high level abstraction, the _ReinforcementLearningEstimator_ class, which allows users to easily construct reinforcement learning run configurations for the underlying reinforcement learning framework. Reinforcement Learning in Azure Machine Learning supports the open source [Ray framework](https://ray.io/) and its highly customizable [RLlib](https://ray.readthedocs.io/en/latest/rllib.html#rllib-scalable-reinforcement-learning). In this section we show how to use _ReinforcementLearningEstimator_ and Ray/RLlib framework to train a cartpole playing agent. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create reinforcement learning estimator\n",
    "\n",
    "The code below creates an instance of *ReinforcementLearningEstimator*, `training_estimator`, which then will be used to submit a job to Azure Machine Learning to start the Ray experiment run.\n",
    "\n",
    "Note that this example is purposely simplified to the minimum. Here is a short description of the parameters we are passing into the constructor:\n",
    "\n",
    "- `source_directory`, local directory containing your training script(s) and helper modules,\n",
    "- `entry_script`, path to your entry script relative to the source directory,\n",
    "- `script_params`, constant parameters to be passed to each run of training script,\n",
    "- `compute_target`, reference to the compute target in which the trainer and worker(s) jobs will be executed,\n",
    "- `rl_framework`, the reinforcement learning framework to be used (currently must be Ray).\n",
    "\n",
    "We use the `script_params` parameter to pass in general and algorithm-specific parameters to the training script.\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.train.rl import ReinforcementLearningEstimator, Ray\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "training_algorithm = \"PPO\"\n",
    "rl_environment = \"CartPole-v0\"\n",
    "video_capture = True\n",
    "\n",
    "if video_capture:\n",
    "    algorithm_config = '\\'{\"num_gpus\": 0, \"num_workers\": 1, \"monitor\": true}\\''\n",
    "else:\n",
    "    algorithm_config = '\\'{\"num_gpus\": 0, \"num_workers\": 1, \"monitor\": false}\\''\n",
    "\n",
    "script_params = {\n",
    "\n",
    "    # Training algorithm\n",
    "    \"--run\": training_algorithm,\n",
    "    \n",
    "    # Training environment\n",
    "    \"--env\": rl_environment,\n",
    "    \n",
    "    # Algorithm-specific parameters\n",
    "    \"--config\": algorithm_config,\n",
    "    \n",
    "    # Stop conditions\n",
    "    \"--stop\": '\\'{\"episode_reward_mean\": 200, \"time_total_s\": 300}\\'',\n",
    "    \n",
    "    # Frequency of taking checkpoints\n",
    "    \"--checkpoint-freq\": 2,\n",
    "    \n",
    "    # If a checkpoint should be taken at the end - optional argument with no value\n",
    "    \"--checkpoint-at-end\": \"\",\n",
    "    \n",
    "    # Log directory\n",
    "    \"--local-dir\": './logs'\n",
    "}\n",
    "\n",
    "xvfb_env = None\n",
    "if video_capture:\n",
    "    # Ray's video capture support requires to run everything under a headless display driver called (xvfb).\n",
    "    # There are two parts to this:\n",
    "    # 1. Use a custom docker file with proper instructions to install xvfb, ffmpeg, python-opengl\n",
    "    # and other dependencies.\n",
    "   \n",
    "    with open(\"files/docker/Dockerfile\", \"r\") as f:\n",
    "        dockerfile=f.read()\n",
    "\n",
    "    xvfb_env = Environment(name='xvfb-vdisplay')\n",
    "    xvfb_env.docker.enabled = True\n",
    "    xvfb_env.docker.base_image = None\n",
    "    xvfb_env.docker.base_dockerfile = dockerfile\n",
    "    \n",
    "    # 2.  Execute the Python process via the xvfb-run command to set up the headless display driver.\n",
    "    xvfb_env.python.user_managed_dependencies = True\n",
    "    xvfb_env.python.interpreter_path = \"xvfb-run -s '-screen 0 640x480x16 -ac +extension GLX +render' python\"\n",
    "\n",
    "\n",
    "training_estimator = ReinforcementLearningEstimator(\n",
    "\n",
    "    # Location of source files\n",
    "    source_directory='files',\n",
    "    \n",
    "    # Python script file\n",
    "    entry_script='cartpole_training.py',\n",
    "    \n",
    "    # A dictionary of arguments to pass to the training script specified in ``entry_script``\n",
    "    script_params=script_params,\n",
    "    \n",
    "    # The Azure Machine Learning compute target set up for Ray head nodes\n",
    "    compute_target=compute_target,\n",
    "    \n",
    "    # Reinforcement learning framework. Currently must be Ray.\n",
    "    rl_framework=Ray(),\n",
    "    \n",
    "    # Custom environmnet for Xvfb\n",
    "    environment=xvfb_env\n",
    ")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training script\n",
    "\n",
    "As recommended in RLlib documentations, we use Ray Tune API to run the training algorithm. All the RLlib built-in trainers are compatible with the Tune API. Here we use `tune.run()` to execute a built-in training algorithm. For convenience, down below you can see part of the entry script where we make this call.\n",
    "\n",
    "This is the list of parameters we are passing into `tune.run()` via the `script_params` parameter:\n",
    "\n",
    "- `run_or_experiment`: name of the [built-in algorithm](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#rllib-algorithms), 'PPO' in our example,\n",
    "- `config`: Algorithm-specific configuration. This includes specifying the environment, `env`, which in our example is the gym **[CartPole-v0](https://gym.openai.com/envs/CartPole-v0/)** environment,\n",
    "- `stop`: stopping conditions, which could be any of the metrics returned by the trainer. Here we use \"mean of episode reward\", and \"total training time in seconds\" as stop conditions, and\n",
    "- `checkpoint_freq` and `checkpoint_at_end`: Frequency of taking checkpoints (number of training iterations between checkpoints), and if a checkpoint should be taken at the end.\n",
    "\n",
    "We also specify the `local_dir`, the directory in which the training logs, checkpoints and other training artificats will be recorded. \n",
    "\n",
    "See [RLlib Training APIs](https://ray.readthedocs.io/en/latest/rllib-training.html#rllib-training-apis) for more details, and also [Training (tune.run, tune.Experiment)](https://ray.readthedocs.io/en/latest/tune/api_docs/execution.html#training-tune-run-tune-experiment) for the complete list of parameters.\n",
    "\n",
    "```python\n",
    "import ray\n",
    "import ray.tune as tune\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # parse arguments ...\n",
    "    \n",
    "    # Intitialize ray\n",
    "    ray.init(address=args.ray_address)\n",
    "\n",
    "    # Run training task using tune.run\n",
    "    tune.run(\n",
    "        run_or_experiment=args.run,\n",
    "        config=dict(args.config, env=args.env),\n",
    "        stop=args.stop,\n",
    "        checkpoint_freq=args.checkpoint_freq,\n",
    "        checkpoint_at_end=args.checkpoint_at_end,\n",
    "        local_dir=args.local_dir\n",
    "    )\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the estimator to start experiment\n",
    "Now we use the *training_estimator* to submit a run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_run = exp.submit(training_estimator)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor experiment\n",
    "\n",
    "Azure Machine Learning provides a Jupyter widget to show the status of an experiment run. You could use this widget to monitor the status of the runs.\n",
    "\n",
    "Note that _ReinforcementLearningEstimator_ creates at least two runs: (a) A parent run, i.e. the run returned above, and (b) a collection of child runs. The number of the child runs depends on the configuration of the reinforcement learning estimator. In our simple scenario, configured above, only one child run will be created.\n",
    "\n",
    "The widget will show a list of the child runs as well. You can click on the link under **Status** to see the details of a child run. It will also show the metrics being logged."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(training_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop the run\n",
    "To stop the run, call `training_run.cancel()`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment line below to cancel the run\n",
    "#training_run.cancel()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for completion\n",
    "Wait for the run to complete before proceeding.\n",
    "\n",
    "**Note: The length of the run depends on the provisioning time of the compute target and it may take several minutes to complete.**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_run.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a handle to the child run\n",
    "You can obtain a handle to the child run as follows. In our scenario, there is only one child run, we have it called `child_run_0`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "child_run_0 = None\n",
    "timeout = 30\n",
    "while timeout > 0 and not child_run_0:\n",
    "    child_runs = list(training_run.get_children())\n",
    "    print('Number of child runs:', len(child_runs))\n",
    "    if len(child_runs) > 0:\n",
    "        child_run_0 = child_runs[0]\n",
    "        break\n",
    "    time.sleep(2) # Wait for 2 seconds\n",
    "    timeout -= 2\n",
    "\n",
    "print('Child run info:')\n",
    "print(child_run_0)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get access to training artifacts\n",
    "We can simply use run id to get a handle to an in-progress or a previously concluded run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Run\n",
    "\n",
    "run_id = child_run_0.id # Or set to run id of a completed run (e.g. 'rl-cartpole-v0_1587572312_06e04ace_head')\n",
    "child_run_0 = Run(exp, run_id=run_id)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the Run API to download policy training artifacts (saved model and checkpoints) to local compute."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from distutils import dir_util\n",
    "\n",
    "path_prefix = path.join(\"logs\", training_algorithm)\n",
    "print(\"Path prefix:\", path_prefix)\n",
    "\n",
    "if path.exists(path_prefix):\n",
    "    dir_util.remove_tree(path_prefix)\n",
    "\n",
    "# Uncomment line below to download run artifacts to local compute\n",
    "#child_run_0.download_files(path_prefix)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset of training artifacts\n",
    "To evaluate a trained policy (a checkpoint) we need to make the checkpoint accessible to the rollout script. All the training artifacts are stored in workspace default datastore under **azureml/&lt;run_id&gt;** directory.\n",
    "\n",
    "Here we create a file dataset from the stored artifacts, and then use this dataset to feed these data to rollout estimator."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "run_id = child_run_0.id # Or set to run id of a completed run (e.g. 'rl-cartpole-v0_1587572312_06e04ace_head')\n",
    "run_artifacts_path = os.path.join('azureml', run_id)\n",
    "print(\"Run artifacts path:\", run_artifacts_path)\n",
    "\n",
    "# Create a file dataset object from the files stored on default datastore\n",
    "datastore = ws.get_default_datastore()\n",
    "training_artifacts_ds = Dataset.File.from_files(datastore.path(os.path.join(run_artifacts_path, '**')))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify, we can print out the number (and paths) of all the files in the dataset, as follows."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts_paths = training_artifacts_ds.to_path()\n",
    "print(\"Number of files in dataset:\", len(artifacts_paths))\n",
    "\n",
    "# Uncomment line below to print all file paths\n",
    "#print(\"Artifacts dataset file paths: \", artifacts_paths)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display movies of selected training episodes\n",
    "\n",
    "Ray creates video output of selected training episodes in mp4 format.  Here we will display two of these, i.e. the first and the last recorded videos, so you could see the improvement of the agent after training.\n",
    "\n",
    "First we introduce a few helper functions: a function to download the movies from our dataset, another one to find mp4 movies in a local directory, and one more to display a downloaded movie."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# A helper function to download movies from a dataset to local directory\n",
    "def download_movies(artifacts_ds, movies, destination):\n",
    "    # Create the local destination directory    \n",
    "    if path.exists(destination):\n",
    "        dir_util.remove_tree(destination)\n",
    "    dir_util.mkpath(destination)\n",
    "\n",
    "    for i, artifact in enumerate(artifacts_ds.to_path()):\n",
    "        if artifact in movies:\n",
    "            print('Downloading {} ...'.format(artifact))\n",
    "            artifacts_ds.skip(i).take(1).download(target_path=destination, overwrite=True)\n",
    "\n",
    "    print('Downloading movies completed!')\n",
    "\n",
    "\n",
    "# A helper function to find movies in a directory\n",
    "def find_movies(movie_path):\n",
    "    print(\"Looking in path:\", movie_path)\n",
    "    mp4_movies = []\n",
    "    for root, _, files in os.walk(movie_path):\n",
    "        for name in files:\n",
    "            if name.endswith('.mp4'):\n",
    "                mp4_movies.append(path.join(root, name))\n",
    "    print('Found {} movies'.format(len(mp4_movies)))\n",
    "\n",
    "    return mp4_movies\n",
    "\n",
    "\n",
    "# A helper function to display a movie\n",
    "from IPython.core.display import display, HTML\n",
    "def display_movie(movie_file):\n",
    "    display(\n",
    "        HTML('\\\n",
    "            <video alt=\"cannot display video\" autoplay loop> \\\n",
    "                <source src=\"{}\" type=\"video/mp4\"> \\\n",
    "            </video>'.format(movie_file)\n",
    "        )\n",
    "    )"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find the first and the last recorded videos in training artifacts dataset and download them to a local directory."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find first and last movie\n",
    "mp4_files = [file for file in training_artifacts_ds.to_path() if file.endswith('.mp4')]\n",
    "mp4_files.sort()\n",
    "\n",
    "first_movie = mp4_files[0] if len(mp4_files) > 0 else None\n",
    "last_movie = mp4_files[-1] if len(mp4_files) > 1 else None\n",
    "\n",
    "print(\"First movie:\", first_movie)\n",
    "print(\"Last movie:\", last_movie)\n",
    "\n",
    "# Download movies\n",
    "training_movies_path = path.join(\"training\", \"videos\")\n",
    "download_movies(training_artifacts_ds, [first_movie, last_movie], training_movies_path)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for the downloaded movies in the local directory and sort them."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp4_files = find_movies(training_movies_path)\n",
    "mp4_files.sort()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a movie of the first training episode.  This is how the agent performs with no training."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_movie = mp4_files[0] if len(mp4_files) > 0 else None\n",
    "print(\"First movie:\", first_movie)\n",
    "\n",
    "display_movie(first_movie)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a movie of the last training episode.  This is how a fully-trained agent performs."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_movie = mp4_files[-1] if len(mp4_files) > 0 else None\n",
    "print(\"Last movie:\", last_movie)\n",
    "\n",
    "display_movie(last_movie)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Trained Agent and See Results\n",
    "\n",
    "We can evaluate a previously trained policy using the `rollout.py` helper script provided by RLlib (see [Evaluating Trained Policies](https://ray.readthedocs.io/en/latest/rllib-training.html#evaluating-trained-policies) for more details). Here we use an adaptation of this script to reconstruct a policy from a checkpoint taken and saved during training. We took these checkpoints by setting `checkpoint-freq` and `checkpoint-at-end` parameters above.\n",
    "In this section we show how to use these checkpoints to evaluate the trained policy."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate a trained policy\n",
    "We need to configure another reinforcement learning estimator, `rollout_estimator`, and then use it to submit another run. Note that the entry script for this estimator now points to `cartpole-rollout.py` script.\n",
    "Also note how we pass the checkpoints dataset to this script using `inputs` parameter of the _ReinforcementLearningEstimator_.\n",
    "\n",
    "We are using script parameters to pass in the same algorithm and the same environment used during training. We also specify the checkpoint number of the checkpoint we wish to evaluate, `checkpoint-number`, and number of the steps we shall run the rollout, `steps`.\n",
    "\n",
    "The training artifacts dataset will be accessible to the rollout script as a mounted folder. The mounted folder and the checkpoint number, passed in via `checkpoint-number`, will be used to create a path to the checkpoint we are going to evaluate. The created checkpoint path then will be passed into RLlib rollout script for evaluation.\n",
    "\n",
    "Let's find the checkpoints and the last checkpoint number first."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find checkpoints and last checkpoint number\n",
    "checkpoint_files = [\n",
    "    os.path.basename(file) for file in training_artifacts_ds.to_path() \\\n",
    "        if os.path.basename(file).startswith('checkpoint-') and \\\n",
    "            not os.path.basename(file).endswith('tune_metadata')\n",
    "]\n",
    "\n",
    "checkpoint_numbers = []\n",
    "for file in checkpoint_files:\n",
    "    checkpoint_numbers.append(int(file.split('-')[1]))\n",
    "\n",
    "print(\"Checkpoints:\", checkpoint_numbers)\n",
    "\n",
    "last_checkpoint_number = max(checkpoint_numbers)\n",
    "print(\"Last checkpoint number:\", last_checkpoint_number)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's configure rollout estimator. Note that we use the last checkpoint for evaluation. The assumption is that the last checkpoint points to our best trained agent. You may change this to any of the checkpoint numbers printed above and observe the effect."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {    \n",
    "    # Checkpoint number of the checkpoint from which to roll out\n",
    "    \"--checkpoint-number\": last_checkpoint_number,\n",
    "\n",
    "    # Training algorithm\n",
    "    \"--run\": training_algorithm,\n",
    "    \n",
    "    # Training environment\n",
    "    \"--env\": rl_environment,\n",
    "    \n",
    "    # Algorithm-specific parameters\n",
    "    \"--config\": '{}',\n",
    "    \n",
    "    # Number of rollout steps \n",
    "    \"--steps\": 2000,\n",
    "    \n",
    "    # If should repress rendering of the environment\n",
    "    \"--no-render\": \"\",\n",
    "    \n",
    "    # The place where recorded videos will be stored\n",
    "    \"--video-dir\": \"./logs/video\"\n",
    "}\n",
    "\n",
    "if video_capture:\n",
    "    script_params.pop(\"--no-render\")\n",
    "else:\n",
    "    script_params.pop(\"--video-dir\")\n",
    "\n",
    "\n",
    "# Ray's video capture support requires to run everything under a headless display driver called (xvfb).\n",
    "# There are two parts to this:\n",
    "\n",
    "# 1. Use a custom docker file with proper instructions to install xvfb, ffmpeg, python-opengl\n",
    "# and other dependencies.\n",
    "# Note: Even when the rendering is off pyhton-opengl is needed.\n",
    "\n",
    "with open(\"files/docker/Dockerfile\", \"r\") as f:\n",
    "    dockerfile=f.read()\n",
    "\n",
    "xvfb_env = Environment(name='xvfb-vdisplay')\n",
    "xvfb_env.docker.enabled = True\n",
    "xvfb_env.docker.base_image = None\n",
    "xvfb_env.docker.base_dockerfile = dockerfile\n",
    "    \n",
    "# 2.  Execute the Python process via the xvfb-run command to set up the headless display driver.\n",
    "xvfb_env.python.user_managed_dependencies = True\n",
    "if video_capture:\n",
    "    xvfb_env.python.interpreter_path = \"xvfb-run -s '-screen 0 640x480x16 -ac +extension GLX +render' python\"\n",
    "\n",
    "\n",
    "rollout_estimator = ReinforcementLearningEstimator(\n",
    "    # Location of source files\n",
    "    source_directory='files',\n",
    "    \n",
    "    # Python script file\n",
    "    entry_script='cartpole_rollout.py',\n",
    "    \n",
    "    # A dictionary of arguments to pass to the rollout script specified in ``entry_script``\n",
    "    script_params = script_params,\n",
    "    \n",
    "    # Data inputs\n",
    "    inputs=[\n",
    "        training_artifacts_ds.as_named_input('artifacts_dataset'),\n",
    "        training_artifacts_ds.as_named_input('artifacts_path').as_mount()],\n",
    "    \n",
    "    # The Azure Machine Learning compute target set up for Ray head nodes\n",
    "    compute_target=compute_target,\n",
    "    \n",
    "    # Reinforcement learning framework. Currently must be Ray.\n",
    "    rl_framework=Ray(),\n",
    "    \n",
    "    # Custom environmnet for Xvfb\n",
    "    environment=xvfb_env)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as before, we use the *rollout_estimator* to submit a run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_run = exp.submit(rollout_estimator)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, similar to the training section, we can monitor the real-time progress of the rollout run and its chid as follows. If you browse logs of the child run you can see the evaluation results recorded in driver_log.txt file. Note that you may need to wait several minutes before these results become available."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(rollout_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for completion of the rollout run before moving to the next section, or you may cancel the run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment line below to cancel the run\n",
    "#rollout_run.cancel()\n",
    "rollout_run.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display movies of selected rollout episodes\n",
    "\n",
    "To display recorded movies first we download recorded videos to local machine. Here again we create a dataset of rollout artifacts and use the helper functions introduced above to download and displays rollout videos."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a handle to child run\n",
    "child_runs = list(rollout_run.get_children())\n",
    "print('Number of child runs:', len(child_runs))\n",
    "child_run_0 = child_runs[0]\n",
    "\n",
    "run_id = child_run_0.id # Or set to run id of a completed run (e.g. 'rl-cartpole-v0_1587572312_06e04ace_head')\n",
    "run_artifacts_path = os.path.join('azureml', run_id)\n",
    "print(\"Run artifacts path:\", run_artifacts_path)\n",
    "\n",
    "# Create a file dataset object from the files stored on default datastore\n",
    "datastore = ws.get_default_datastore()\n",
    "rollout_artifacts_ds = Dataset.File.from_files(datastore.path(os.path.join(run_artifacts_path, '**')))\n",
    "\n",
    "artifacts_paths = rollout_artifacts_ds.to_path()\n",
    "print(\"Number of files in dataset:\", len(artifacts_paths))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, similar to the training section, we look for the last video."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find last movie\n",
    "mp4_files = [file for file in rollout_artifacts_ds.to_path() if file.endswith('.mp4')]\n",
    "mp4_files.sort()\n",
    "\n",
    "last_movie = mp4_files[-1] if len(mp4_files) > 1 else None\n",
    "print(\"Last movie:\", last_movie)\n",
    "\n",
    "# Download last movie\n",
    "rollout_movies_path = path.join(\"rollout\", \"videos\")\n",
    "download_movies(rollout_artifacts_ds, [last_movie], rollout_movies_path)\n",
    "\n",
    "# Look for the downloaded movie in local directory\n",
    "mp4_files = find_movies(rollout_movies_path)\n",
    "mp4_files.sort()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display last video recorded during the rollout."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_movie = mp4_files[-1] if len(mp4_files) > 0 else None\n",
    "print(\"Last movie:\", last_movie)\n",
    "\n",
    "display_movie(last_movie)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "For your convenience, below you can find code snippets to clean up any resources created as part of this tutorial that you don't wish to retain."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To archive the created experiment:\n",
    "#exp.archive()\n",
    "\n",
    "# To delete the compute target:\n",
    "#compute_target.delete()\n",
    "\n",
    "# To delete downloaded training artifacts\n",
    "#if os.path.exists(path_prefix):\n",
    "#    dir_util.remove_tree(path_prefix)\n",
    "\n",
    "# To delete downloaded training videos\n",
    "#if path.exists(training_movies_path):\n",
    "#    dir_util.remove_tree(training_movies_path)\n",
    "\n",
    "# To delete downloaded rollout videos\n",
    "#if path.exists(rollout_movies_path):\n",
    "#    dir_util.remove_tree(rollout_movies_path)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "This example was about running Reinforcement Learning in Azure Machine Learning (Ray/RLlib Framework) on a single compute. Please see [Pong Problem](../atari-on-distributed-compute/pong_rllib.ipynb)\n",
    "example which uses Ray RLlib to train a Pong playing agent on a multi-node cluster."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/multiagent-particle-envs/particle.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/reinforcement-learning/multiagent-particle-envs/particle.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/multiagent-particle-envs/particle.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning in Azure Machine Learning - Training multiple agents on collaborative ParticleEnv tasks\n",
    "\n",
    "This tutorial will show you how to train policies in a multi-agent scenario.\n",
    "We use OpenAI Gym's [Particle environments](https://github.com/openai/multiagent-particle-envs),\n",
    "which model agents and landmarks in a two-dimensional world. Particle comes with\n",
    "several predefined scenarios, both competitive and collaborative, and with or without communication.\n",
    "\n",
    "For this tutorial, we pick a cooperative navigation scenario where N agents are in a world with N\n",
    "landmarks.  The agents' goal is to cover all the landmarks without collisions,\n",
    "so agents must learn to avoid each other (social distancing!).  The video below shows training\n",
    "results for N=3 agents/landmarks:\n",
    "\n",
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "      <th style=\"text-align: center;\">\n",
    "          <img src=\"./images/particle_simple_spread.gif\" alt=\"Particle video\" align=\"middle\" margin-left=\"auto\" margin-right=\"auto\"/>\n",
    "      </th>\n",
    "  </tr>\n",
    "  <tr style=\"text-align: center;\">\n",
    "      <th>Fig 1. Video of 3 agents covering 3 landmarks in a multiagent Particle scenario.</th>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "The tutorial will cover the following steps:\n",
    "- Initializing Azure Machine Learning resources for training\n",
    "- Training policies in a multi-agent environment with Azure Machine Learning service\n",
    "- Monitoring training progress\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "The user should have completed the Azure Machine Learning introductory tutorial. You will need to make sure that you have a valid subscription id, a resource group and a workspace. For detailed instructions see [Tutorial: Get started creating your first ML experiment](https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-1st-experiment-sdk-setup).\n",
    "\n",
    "Please ensure that you have a current version of IPython (>= 7.15) installed.\n",
    "\n",
    "While this is a standalone notebook, we highly recommend going over the introductory notebooks for RL first.\n",
    "- Getting started:\n",
    "  - [RL using a compute instance with Azure Machine Learning](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb)\n",
    "  - [RL using Azure Machine Learning compute](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb)\n",
    "- [Scaling RL training runs with Azure Machine Learning](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb)\n",
    "\n",
    "Advanced users might also be interested in [this notebook](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb) demonstrating how to train a Minecraft RL agent in Azure Machine Learning.\n",
    "\n",
    "## Initialize resources\n",
    "\n",
    "All required Azure Machine Learning service resources for this tutorial can be set up from Jupyter. This includes:\n",
    "\n",
    "- Connecting to your existing Azure Machine Learning workspace.\n",
    "- Creating an experiment to track runs.\n",
    "- Creating remote compute targets for [Ray](https://docs.ray.io/en/latest/index.html).\n",
    "\n",
    "\n",
    "### Azure Machine Learning SDK\n",
    "\n",
    "Display the Azure Machine Learning SDK version."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/multiagent-particle-envs/particle.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "print('Azure Machine Learning SDK Version: ', azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/multiagent-particle-envs/particle.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to workspace\n",
    "\n",
    "Get a reference to an existing Azure Machine Learning workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/multiagent-particle-envs/particle.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, sep=' | ')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/multiagent-particle-envs/particle.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiment\n",
    "\n",
    "Create an experiment to track the runs in your workspace. A\n",
    "workspace can have multiple experiments and each experiment\n",
    "can be used to track multiple runs (see [documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment.experiment?view=azure-ml-py)\n",
    "for details)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/multiagent-particle-envs/particle.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "exp = Experiment(workspace=ws, name='particle-multiagent')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/multiagent-particle-envs/particle.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or attach an existing compute resource\n",
    "\n",
    "A compute target is a designated compute resource where you run your training script. For more information, see [What are compute targets in Azure Machine Learning service?](https://docs.microsoft.com/en-us/azure/machine-learning/concept-compute-target).\n",
    "\n",
    "#### CPU target for Ray head\n",
    "\n",
    "In the experiment setup for this tutorial, the Ray head node will\n",
    "run on a CPU node (D3 type). A maximum cluster size of 1 node is\n",
    "therefore sufficient. If you wish to run multiple experiments in\n",
    "parallel using the same CPU cluster, you may elect to increase this\n",
    "number. The cluster will automatically scale down to 0 nodes when\n",
    "no training jobs are scheduled (see min_nodes).\n",
    "\n",
    "The code below creates a compute cluster of D3 type nodes.\n",
    "If the cluster with the specified name is already in your workspace\n",
    "the code will skip the creation process.\n",
    "\n",
    "**Note: Creation of a compute resource can take several minutes**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/multiagent-particle-envs/particle.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "cpu_cluster_name = 'cpu-cl-d3'\n",
    "\n",
    "if cpu_cluster_name in ws.compute_targets:\n",
    "    cpu_cluster = ws.compute_targets[cpu_cluster_name]\n",
    "    if cpu_cluster and type(cpu_cluster) is AmlCompute:\n",
    "        if cpu_cluster.provisioning_state == 'Succeeded':\n",
    "            print('Found existing compute target for {}. Using it.'.format(cpu_cluster_name))\n",
    "        else: \n",
    "            raise Exception('Found existing compute target for {} '.format(cpu_cluster_name)\n",
    "                            + 'but it is in state {}'.format(cpu_cluster.provisioning_state))\n",
    "else:\n",
    "    print('Creating a new compute target for {}...'.format(cpu_cluster_name))\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size='STANDARD_D3',\n",
    "        min_nodes=0, \n",
    "        max_nodes=1)\n",
    "\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, provisioning_config)\n",
    "    cpu_cluster.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "    print('Cluster created.')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/multiagent-particle-envs/particle.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the policies\n",
    "\n",
    "### Training environment\n",
    "\n",
    "This tutorial uses a custom docker image\n",
    "with the necessary software installed. The [Environment](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-environments)\n",
    "class stores the configuration for the training environment. The\n",
    "docker image is set via `env.docker.base_image`.\n",
    "`user_managed_dependencies` is set so that\n",
    "the preinstalled Python packages in the image are preserved.\n",
    "\n",
    "Note that since we want to capture videos of the training runs requiring a display, we set the interpreter_path such that the Python process is started via **xvfb-run**."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/multiagent-particle-envs/particle.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core import Environment\n",
    "    \n",
    "cpu_particle_env = Environment(name='particle-cpu')\n",
    "\n",
    "cpu_particle_env.docker.enabled = True\n",
    "cpu_particle_env.docker.base_image = 'akdmsft/particle-cpu'\n",
    "cpu_particle_env.python.interpreter_path = 'xvfb-run -s \"-screen 0 640x480x16 -ac +extension GLX +render\" python'\n",
    "\n",
    "max_train_time = os.environ.get('AML_MAX_TRAIN_TIME_SECONDS', 2 * 60 * 60)\n",
    "cpu_particle_env.environment_variables['AML_MAX_TRAIN_TIME_SECONDS'] = str(max_train_time)\n",
    "cpu_particle_env.python.user_managed_dependencies = True"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/multiagent-particle-envs/particle.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training script\n",
    "\n",
    "This tutorial uses the multiagent algorithm [Multi-Agent Deep Deterministic Policy Gradient (MADDPG)](https://docs.ray.io/en/latest/rllib-algorithms.html?highlight=maddpg#multi-agent-deep-deterministic-policy-gradient-contrib-maddpg).\n",
    "For training policies in a multiagent scenario, Ray's RLlib also\n",
    "requires the `multiagent` configuration section to be specified. You\n",
    "can find more information in the [common parameters](https://docs.ray.io/en/latest/rllib-training.html?highlight=multiagent#common-parameters)\n",
    "documentation.\n",
    "\n",
    "For monitoring and understanding the training progress, one\n",
    "of the training environments is wrapped in a [Gym monitor](https://github.com/openai/gym/blob/master/gym/wrappers/monitor.py)\n",
    "which periodically captures videos - by default every 200 training\n",
    "iterations.\n",
    "\n",
    "The stopping criteria are set such that the training run is\n",
    "terminated after either a mean reward of -400 is observed, or\n",
    "training has run for over 2 hours.\n",
    "\n",
    "### Submitting a training run\n",
    "\n",
    "Below, you create the training run using a `ReinforcementLearningEstimator`\n",
    "object, which contains all the configuration parameters for this experiment:\n",
    "\n",
    "- `source_directory`: Contains the training script and helper files to be\n",
    "  copied onto the node.\n",
    "- `entry_script`: The training script, described in more detail above.\n",
    "- `script_params`: The command line arguments to pass to the entry script.\n",
    "- `compute_target`: The compute target for training script execution.\n",
    "- `environment`: The Azure Machine Learning environment definition for the node running the training.\n",
    "- `max_run_duration_seconds`: The time after which to abort the run if it is still running.\n",
    "\n",
    "For more details, please take a look at the [online documentation](https://docs.microsoft.com/en-us/python/api/azureml-contrib-reinforcementlearning/?view=azure-ml-py)\n",
    "for Azure Machine Learning service's reinforcement learning offering.\n",
    "\n",
    "Note that you can use the same notebook and scripts to experiment with\n",
    "different Particle environments.  You can find a list of supported\n",
    "environments [here](https://github.com/openai/multiagent-particle-envs/tree/master#list-of-environments).\n",
    "Simply change the `--scenario` parameter to a supported scenario.\n",
    "\n",
    "In order to get the best training results, you can also adjust the\n",
    "`--final-reward` parameter to determine when to stop training. A greater\n",
    "reward means longer running time, but improved results. By default,\n",
    "the final reward will be -400, which should show good progress after\n",
    "about one hour of run time.\n",
    "\n",
    "For this notebook, we use a single D3 nodes, giving us a total of 4 CPUs and\n",
    "0 GPUs. One CPU is used by the MADDPG trainer, and an additional CPU is\n",
    "consumed by the RLlib rollout worker. The other 2 CPUs are not used, though\n",
    "smaller node types will run out of memory for this task.\n",
    "\n",
    "Lastly, the RunDetails widget displays information about the submitted RL\n",
    "experiment, including a link to the Azure portal with more details."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/multiagent-particle-envs/particle.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.train.rl import ReinforcementLearningEstimator\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "estimator = ReinforcementLearningEstimator(\n",
    "    source_directory='files',\n",
    "    entry_script='particle_train.py',\n",
    "    script_params={\n",
    "        '--scenario': 'simple_spread',\n",
    "        '--final-reward': -400\n",
    "    },\n",
    "    compute_target=cpu_cluster,\n",
    "    environment=cpu_particle_env,\n",
    "    max_run_duration_seconds=3 * 60 * 60\n",
    ")\n",
    "\n",
    "train_run = exp.submit(config=estimator)\n",
    "\n",
    "RunDetails(train_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/multiagent-particle-envs/particle.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you wish to cancel the run before it completes, uncomment and execute:\n",
    "#train_run.cancel()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/multiagent-particle-envs/particle.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring training progress\n",
    "\n",
    "### View the Tensorboard\n",
    "\n",
    "The Tensorboard can be displayed via the Azure Machine Learning\n",
    "service's [Tensorboard API](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-monitor-tensorboard).\n",
    "When running locally, please make sure to follow the instructions\n",
    "in the link and install required packages. Running this cell will output a URL for the Tensorboard.\n",
    "\n",
    "Note that the training script sets the log directory when\n",
    "starting RLlib via the local_dir parameter. ./logs will automatically\n",
    "appear in the downloadable files for a run. Since this script is\n",
    "executed on the Ray head node run, we need to get a reference to it\n",
    "as shown below.\n",
    "\n",
    "The Tensorboard API will continuously stream logs from the run.\n",
    "\n",
    "**Note: It may take a couple of minutes after the run is in \"Running\"\n",
    "state before Tensorboard files are available and the board will refresh automatically**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/multiagent-particle-envs/particle.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from azureml.tensorboard import Tensorboard\n",
    "\n",
    "head_run = None\n",
    "\n",
    "timeout = 60\n",
    "while timeout > 0 and head_run is None:\n",
    "    timeout -= 1\n",
    "    \n",
    "    try:\n",
    "        head_run = next(r for r in train_run.get_children() if r.id.endswith('head'))\n",
    "    except StopIteration:\n",
    "        time.sleep(1)\n",
    "\n",
    "tb = Tensorboard([head_run])\n",
    "tb.start()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/multiagent-particle-envs/particle.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View training videos\n",
    "\n",
    "As mentioned above, we record videos of the agents interacting with the\n",
    "Particle world. These videos are often a crucial indicator for training\n",
    "success. The code below downloads the latest video as it becomes available\n",
    "and displays it in-line.\n",
    "\n",
    "Over time, the agents learn to cooperate and avoid collisions while\n",
    "traveling to all landmarks.\n",
    "\n",
    "**Note: It can take several minutes for a video to appear after the run\n",
    "was started.**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/multiagent-particle-envs/particle.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from azureml.core import Dataset\n",
    "from azureml.data.dataset_error_handling import DatasetValidationError\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.core.display import display, Video\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "path_prefix = './tmp_videos'\n",
    "\n",
    "def download_latest_training_video(run, video_checkpoint_counter):\n",
    "    run_artifacts_path = os.path.join('azureml', run.id)\n",
    "    \n",
    "    try:\n",
    "        run_artifacts_ds = Dataset.File.from_files(datastore.path(os.path.join(run_artifacts_path, '**')))\n",
    "    except DatasetValidationError as e:\n",
    "        # This happens at the start of the run when there is no data available\n",
    "        # in the run's artifacts\n",
    "        return None, video_checkpoint_counter\n",
    "    \n",
    "    video_files = [file for file in run_artifacts_ds.to_path() if file.endswith('.mp4')]\n",
    "    if len(video_files) == video_checkpoint_counter:\n",
    "        return None, video_checkpoint_counter\n",
    "    \n",
    "    iteration_numbers = [int(vf[vf.rindex('video') + len('video') : vf.index('.mp4')]) for vf in video_files]\n",
    "    latest_video = next(vf for vf in video_files if vf.endswith('{num}.mp4'.format(num=max(iteration_numbers))))\n",
    "    latest_video = os.path.join(run_artifacts_path, os.path.normpath(latest_video[1:]))\n",
    "    \n",
    "    datastore.download(\n",
    "        target_path=path_prefix,\n",
    "        prefix=latest_video.replace('\\\\', '/'),\n",
    "        show_progress=False)\n",
    "    \n",
    "    return os.path.join(path_prefix, latest_video), len(video_files)\n",
    "\n",
    "\n",
    "def render_video(vf):\n",
    "    clear_output(wait=True)\n",
    "    display(Video(data=vf, embed=True, html_attributes='loop autoplay width=50%'))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/multiagent-particle-envs/particle.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "terminal_statuses = ['Canceled', 'Completed', 'Failed']\n",
    "video_checkpoint_counter = 0\n",
    "\n",
    "while head_run.get_status() not in terminal_statuses:\n",
    "    video_file, video_checkpoint_counter = download_latest_training_video(head_run, video_checkpoint_counter)\n",
    "    if video_file is not None:\n",
    "        render_video(video_file)\n",
    "        \n",
    "        print('Displaying video number {}'.format(video_checkpoint_counter))\n",
    "        shutil.rmtree(path_prefix)\n",
    "    \n",
    "    # Interrupting the kernel can take up to 15 seconds\n",
    "    # depending on when time.sleep started\n",
    "    time.sleep(15)\n",
    "    \n",
    "train_run.wait_for_completion()\n",
    "print('The training run has reached a terminal status.')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/multiagent-particle-envs/particle.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "Below, you can find code snippets for your convenience to clean up any resources created as part of this tutorial you don't wish to retain."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/multiagent-particle-envs/particle.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to stop the Tensorboard, uncomment and run\n",
    "#tb.stop()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/multiagent-particle-envs/particle.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete the cpu compute target, uncomment and run\n",
    "#cpu_cluster.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/multiagent-particle-envs/particle.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "We would love to hear your feedback! Please let us know what you think of Reinforcement Learning in Azure Machine Learning and what features you are looking forward to."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/multiagent-particle-envs/particle.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning in Azure Machine Learning - Training a Minecraft agent using custom environments\n",
    "\n",
    "This tutorial will show how to set up a more complex reinforcement\n",
    "learning (RL) training scenario.  It demonstrates how to train an agent to\n",
    "navigate through a lava maze in the Minecraft game using Azure Machine\n",
    "Learning.\n",
    "\n",
    "**Please note:** This notebook trains an agent on a randomly generated\n",
    "Minecraft level.  As a result, on rare occasions, a training run may fail\n",
    "to produce a model that can solve the maze.  If this happens, you can\n",
    "re-run the training step as indicated below.\n",
    "\n",
    "**Please note:** This notebook uses 1 NC6 type node and 8 D2 type nodes\n",
    "for up to 5 hours of training, which corresponds to approximately $9.06 (USD)\n",
    "as of May 2020.\n",
    "\n",
    "Minecraft is currently one of the most popular video\n",
    "games and as such has been a study object for RL.  [Project \n",
    "Malmo](https://www.microsoft.com/en-us/research/project/project-malmo/) is\n",
    "a platform for artificial intelligence experimentation and research built on\n",
    "top of Minecraft.  We will use Minecraft [gym](https://gym.openai.com) environments from Project\n",
    "Malmo's 2019 MineRL competition, which are part of the \n",
    "[MineRL](http://minerl.io/docs/index.html) Python package.\n",
    "\n",
    "Minecraft environments require a display to run, so we will demonstrate\n",
    "how to set up a virtual display within the docker container used for training.\n",
    "Learning will be based on the agent's visual observations.  To\n",
    "generate the necessary amount of sample data, we will run several\n",
    "instances of the Minecraft game in parallel.  Below, you can see a video of\n",
    "a trained agent navigating a lava maze. Starting from the green position,\n",
    "it moves to the blue position by moving forward, turning left or turning right:\n",
    "\n",
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "      <th style=\"text-align: center;\">\n",
    "          <img src=\"./images/lava_maze_minecraft.gif\" alt=\"Minecraft lava maze\" align=\"middle\" margin-left=\"auto\" margin-right=\"auto\"/>\n",
    "      </th>\n",
    "  </tr>\n",
    "  <tr style=\"text-align: center;\">\n",
    "      <th>Fig 1. Video of a trained Minecraft agent navigating a lava maze.</th>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "The tutorial will cover the following steps:\n",
    "- Initializing Azure Machine Learning resources for training\n",
    "- Training the RL agent with Azure Machine Learning service\n",
    "- Monitoring training progress\n",
    "- Reviewing training results\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "The user should have completed the Azure Machine Learning introductory tutorial.\n",
    "You will need to make sure that you have a valid subscription id, a resource group and a\n",
    "workspace.  For detailed instructions see [Tutorial: Get started creating\n",
    "your first ML experiment.](https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-1st-experiment-sdk-setup)\n",
    "\n",
    "While this is a standalone notebook, we highly recommend going over the\n",
    "introductory notebooks for RL first.\n",
    "- Getting started:\n",
    "  - [RL using a compute instance with Azure Machine Learning service](../cartpole-on-compute-instance/cartpole_ci.ipynb)\n",
    "  - [Using Azure Machine Learning compute](../cartpole-on-single-compute/cartpole_sc.ipynb)\n",
    "- [Scaling RL training runs with Azure Machine Learning service](../atari-on-distributed-compute/pong_rllib.ipynb)\n",
    "\n",
    "\n",
    "## Initialize resources\n",
    "\n",
    "All required Azure Machine Learning service resources for this tutorial can be set up from Jupyter.\n",
    "This includes:\n",
    "- Connecting to your existing Azure Machine Learning workspace.\n",
    "- Creating an experiment to track runs.\n",
    "- Setting up a virtual network\n",
    "- Creating remote compute targets for [Ray](https://docs.ray.io/en/latest/index.html).\n",
    "\n",
    "### Azure Machine Learning SDK\n",
    "\n",
    "Display the Azure Machine Learning SDK version."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "print(\"Azure Machine Learning SDK Version: \", azureml.core.VERSION) "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to workspace\n",
    "\n",
    "Get a reference to an existing Azure Machine Learning workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, sep=' | ')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiment\n",
    "\n",
    "Create an experiment to track the runs in your workspace.  A\n",
    "workspace can have multiple experiments and each experiment\n",
    "can be used to track multiple runs (see [documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment.experiment?view=azure-ml-py)\n",
    "for details)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "bc70f780-c240-4779-96f3-bc5ef9a37d59"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "exp = Experiment(workspace=ws, name='minecraft-maze')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Virtual Network\n",
    "\n",
    "If you are using separate compute targets for the Ray head and worker, a virtual network must be created in the resource group.  If you have alraeady created a virtual network in the resource group, you can skip this step.\n",
    "\n",
    "To do this, you first must install the Azure Networking API.\n",
    "\n",
    "`pip install --upgrade azure-mgmt-network`"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to install the Azure Networking SDK, uncomment the following line.\n",
    "#!pip install --upgrade azure-mgmt-network"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.mgmt.network import NetworkManagementClient\n",
    "\n",
    "# Virtual network name\n",
    "vnet_name =\"rl_minecraft_vnet\"\n",
    "\n",
    "# Default subnet\n",
    "subnet_name =\"default\"\n",
    "\n",
    "# The Azure subscription you are using\n",
    "subscription_id=ws.subscription_id\n",
    "\n",
    "# The resource group for the reinforcement learning cluster\n",
    "resource_group=ws.resource_group\n",
    "\n",
    "# Azure region of the resource group\n",
    "location=ws.location\n",
    "\n",
    "network_client = NetworkManagementClient(ws._auth_object, subscription_id)\n",
    "\n",
    "async_vnet_creation = network_client.virtual_networks.create_or_update(\n",
    "    resource_group,\n",
    "    vnet_name,\n",
    "    {\n",
    "        'location': location,\n",
    "        'address_space': {\n",
    "            'address_prefixes': ['10.0.0.0/16']\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "async_vnet_creation.wait()\n",
    "print(\"Virtual network created successfully: \", async_vnet_creation.result())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Network Security Group on Virtual Network\n",
    "\n",
    "Depending on your Azure setup, you may need to open certain ports to make it possible for Azure to manage the compute targets that you create.  The ports that need to be opened are described [here](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-enable-virtual-network).\n",
    "\n",
    "A common situation is that ports `29876-29877` are closed.  The following code will add a security rule to open these ports.    Or you can do this manually in the [Azure portal](https://portal.azure.com).\n",
    "\n",
    "You may need to modify the code below to match your scenario."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.mgmt.network.models\n",
    "\n",
    "security_group_name = vnet_name + '-' + \"nsg\"\n",
    "security_rule_name = \"AllowAML\"\n",
    "\n",
    "# Create a network security group\n",
    "nsg_params = azure.mgmt.network.models.NetworkSecurityGroup(\n",
    "    location=location,\n",
    "    security_rules=[\n",
    "        azure.mgmt.network.models.SecurityRule(\n",
    "            name=security_rule_name,\n",
    "            access=azure.mgmt.network.models.SecurityRuleAccess.allow,\n",
    "            description='Reinforcement Learning in Azure Machine Learning rule',\n",
    "            destination_address_prefix='*',\n",
    "            destination_port_range='29876-29877',\n",
    "            direction=azure.mgmt.network.models.SecurityRuleDirection.inbound,\n",
    "            priority=400,\n",
    "            protocol=azure.mgmt.network.models.SecurityRuleProtocol.tcp,\n",
    "            source_address_prefix='BatchNodeManagement',\n",
    "            source_port_range='*'\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "async_nsg_creation = network_client.network_security_groups.create_or_update(\n",
    "    resource_group,\n",
    "    security_group_name,\n",
    "    nsg_params,\n",
    ")\n",
    "\n",
    "async_nsg_creation.wait() \n",
    "print(\"Network security group created successfully:\", async_nsg_creation.result())\n",
    "\n",
    "network_security_group = network_client.network_security_groups.get(\n",
    "    resource_group,\n",
    "    security_group_name,\n",
    ")\n",
    "\n",
    "# Define a subnet to be created with network security group\n",
    "subnet = azure.mgmt.network.models.Subnet(\n",
    "            id='default',\n",
    "            address_prefix='10.0.0.0/24',\n",
    "            network_security_group=network_security_group\n",
    "            )\n",
    "    \n",
    "# Create subnet on virtual network\n",
    "async_subnet_creation = network_client.subnets.create_or_update(\n",
    "    resource_group_name=resource_group,\n",
    "    virtual_network_name=vnet_name,\n",
    "    subnet_name=subnet_name,\n",
    "    subnet_parameters=subnet\n",
    ")\n",
    "\n",
    "async_subnet_creation.wait()\n",
    "print(\"Subnet created successfully:\", async_subnet_creation.result())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the virtual network security rules\n",
    "Ensure that the virtual network is configured correctly with required ports open. It is possible that you have configured rules with broader range of ports that allows ports 29876-29877 to be opened. Kindly review your network security group rules.  "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from files.networkutils import *\n",
    "\n",
    "check_vnet_security_rules(ws._auth_object, ws.subscription_id, ws.resource_group, vnet_name, True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or attach an existing compute resource\n",
    "\n",
    "A compute target is a designated compute resource where you\n",
    "run your training script.  For more information, see [What\n",
    "are compute targets in Azure Machine Learning service?](https://docs.microsoft.com/en-us/azure/machine-learning/concept-compute-target).\n",
    "\n",
    "#### GPU target for Ray head\n",
    "\n",
    "In the experiment setup for this tutorial, the Ray head node\n",
    "will run on a GPU-enabled node.  A maximum cluster size\n",
    "of 1 node is therefore sufficient.  If you wish to run\n",
    "multiple experiments in parallel using the same GPU\n",
    "cluster, you may elect to increase this number.  The cluster\n",
    "will automatically scale down to 0 nodes when no training jobs\n",
    "are scheduled (see `min_nodes`).\n",
    "\n",
    "The code below creates a compute cluster of GPU-enabled NC6\n",
    "nodes.  If the cluster with the specified name is already in\n",
    "your workspace the code will skip the creation process.\n",
    "\n",
    "Note that we must specify a Virtual Network during compute\n",
    "creation to allow communication between the cluster running\n",
    "the Ray head node and the additional Ray compute nodes.  For\n",
    "details on how to setup the Virtual Network, please follow the\n",
    "instructions in the \"Prerequisites\" section above.\n",
    "\n",
    "**Note: Creation of a compute resource can take several minutes**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "gpu_cluster_name = 'gpu-cl-nc6-vnet'\n",
    "\n",
    "try:\n",
    "    gpu_cluster = ComputeTarget(workspace=ws, name=gpu_cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size='Standard_NC6',\n",
    "        min_nodes=0,\n",
    "        max_nodes=1,\n",
    "        vnet_resourcegroup_name=ws.resource_group,\n",
    "        vnet_name=vnet_name,\n",
    "        subnet_name=subnet_name)\n",
    "\n",
    "    gpu_cluster = ComputeTarget.create(ws, gpu_cluster_name, compute_config)\n",
    "    gpu_cluster.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "    print('Cluster created.')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CPU target for additional Ray nodes\n",
    "\n",
    "The code below creates a compute cluster of D2 nodes. If the cluster with the specified name is already in your workspace the code will skip the creation process.\n",
    "\n",
    "This cluster will be used to start additional Ray nodes\n",
    "increasing the clusters CPU resources.\n",
    "\n",
    "**Note: Creation of a compute resource can take several minutes**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_cluster_name = 'cpu-cl-d2-vnet'\n",
    "\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size='STANDARD_D2',\n",
    "        min_nodes=0,\n",
    "        max_nodes=10,\n",
    "        vnet_resourcegroup_name=ws.resource_group,\n",
    "        vnet_name=vnet_name,\n",
    "        subnet_name=subnet_name)\n",
    "\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "    cpu_cluster.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "    print('Cluster created.')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the agent\n",
    "\n",
    "### Training environments\n",
    "\n",
    "This tutorial uses custom docker images (CPU and GPU respectively)\n",
    "with the necessary software installed.  The\n",
    "[Environment](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-environments)\n",
    "class stores the configuration for the training environment.  The docker\n",
    "image is set via `env.docker.base_image` which can point to any\n",
    "publicly available docker image.  `user_managed_dependencies`\n",
    "is set so that the preinstalled Python packages in the image are preserved.\n",
    "\n",
    "Note that since Minecraft requires a display to start, we set the `interpreter_path`\n",
    "such that the Python process is started via **xvfb-run**."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core import Environment\n",
    "\n",
    "max_train_time = os.environ.get(\"AML_MAX_TRAIN_TIME_SECONDS\", 5 * 60 * 60)\n",
    "\n",
    "def create_env(env_type):\n",
    "    env = Environment(name='minecraft-{env_type}'.format(env_type=env_type))\n",
    "\n",
    "    env.docker.enabled = True\n",
    "    env.docker.base_image = 'akdmsft/minecraft-{env_type}'.format(env_type=env_type)\n",
    "\n",
    "    env.python.interpreter_path = \"xvfb-run -s '-screen 0 640x480x16 -ac +extension GLX +render' python\"\n",
    "    env.environment_variables[\"AML_MAX_TRAIN_TIME_SECONDS\"] = str(max_train_time)\n",
    "    env.python.user_managed_dependencies = True\n",
    "    \n",
    "    return env\n",
    "    \n",
    "cpu_minecraft_env = create_env('cpu')\n",
    "gpu_minecraft_env = create_env('gpu')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training script\n",
    "\n",
    "As described above, we use the MineRL Python package to launch\n",
    "Minecraft game instances.  MineRL provides several OpenAI gym\n",
    "environments for different scenarios, such as chopping wood.\n",
    "Besides predefined environments, MineRL lets its users create\n",
    "custom Minecraft environments through\n",
    "[minerl.env](http://minerl.io/docs/api/env.html).  In the helper\n",
    "file **minecraft_environment.py** provided with this tutorial, we use the\n",
    "latter option to customize a Minecraft level with a lava maze\n",
    "that the agent has to navigate. The agent receives a negative\n",
    "reward of -1 for falling into the lava, a negative reward of\n",
    "-0.02 for sending a command (i.e. navigating through the maze\n",
    "with fewer actions yields a higher total reward) and a positive reward\n",
    "of 1 for reaching the goal.  To encourage the agent to explore\n",
    "the maze, it also receives a positive reward of 0.1 for visiting\n",
    "a tile for the first time.\n",
    "\n",
    "The agent learns purely from visual observations and the image\n",
    "is scaled to an 84x84 format, stacking four frames.  For the\n",
    "purposes of this example, we use a small action space of size\n",
    "three: move forward, turn 90 degrees to the left, and turn 90\n",
    "degrees to the right.\n",
    "\n",
    "The training script itself registers the function to create training\n",
    "environments with the `tune.register_env` function and connects to\n",
    "the Ray cluster Azure Machine Learning service started on the GPU \n",
    "and CPU nodes. Lastly, it starts a RL training run with `tune.run()`.\n",
    "\n",
    "We recommend setting the `local_dir` parameter to `./logs` as this\n",
    "directory will automatically become available as part of the training\n",
    "run's files in the Azure Portal.  The Tensorboard integration\n",
    "(see \"View the Tensorboard\" section below) also depends on the files'\n",
    "availability.  For a list of common parameter options, please refer\n",
    "to the [Ray documentation](https://docs.ray.io/en/latest/rllib-training.html#common-parameters).\n",
    "\n",
    "\n",
    "```python\n",
    "# Taken from minecraft_environment.py and minecraft_train.py\n",
    "\n",
    "# Define a function to create a MineRL environment\n",
    "def create_env(config):\n",
    "    mission = config['mission']\n",
    "    port = 1000 * config.worker_index + config.vector_index\n",
    "    print('*********************************************')\n",
    "    print(f'* Worker {config.worker_index} creating from mission: {mission}, port {port}')\n",
    "    print('*********************************************')\n",
    "\n",
    "    if config.worker_index == 0:\n",
    "        # The first environment is only used for checking the action and observation space.\n",
    "        # By using a dummy environment, there's no need to spin up a Minecraft instance behind it\n",
    "        # saving some CPU resources on the head node.\n",
    "        return DummyEnv()\n",
    "\n",
    "    env = EnvWrapper(mission, port)\n",
    "    env = TrackingEnv(env)\n",
    "    env = FrameStack(env, 2)\n",
    "    \n",
    "    return env\n",
    "\n",
    "\n",
    "def stop(trial_id, result):\n",
    "    return result[\"episode_reward_mean\"] >= 1 \\\n",
    "        or result[\"time_total_s\"] > 5 * 60 * 60\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tune.register_env(\"Minecraft\", create_env)\n",
    "\n",
    "    ray.init(address='auto')\n",
    "\n",
    "    tune.run(\n",
    "        run_or_experiment=\"IMPALA\",\n",
    "        config={\n",
    "            \"env\": \"Minecraft\",\n",
    "            \"env_config\": {\n",
    "                \"mission\": \"minecraft_missions/lava_maze-v0.xml\"\n",
    "            },\n",
    "            \"num_workers\": 10,\n",
    "            \"num_cpus_per_worker\": 2,\n",
    "            \"rollout_fragment_length\": 50,\n",
    "            \"train_batch_size\": 1024,\n",
    "            \"replay_buffer_num_slots\": 4000,\n",
    "            \"replay_proportion\": 10,\n",
    "            \"learner_queue_timeout\": 900,\n",
    "            \"num_sgd_iter\": 2,\n",
    "            \"num_data_loader_buffers\": 2,\n",
    "            \"exploration_config\": {\n",
    "                \"type\": \"EpsilonGreedy\",\n",
    "                \"initial_epsilon\": 1.0,\n",
    "                \"final_epsilon\": 0.02,\n",
    "                \"epsilon_timesteps\": 500000\n",
    "            },\n",
    "            \"callbacks\": {\"on_train_result\": callbacks.on_train_result},\n",
    "        },\n",
    "        stop=stop,\n",
    "        checkpoint_at_end=True,\n",
    "        local_dir='./logs'\n",
    "    )\n",
    "```\n",
    "\n",
    "### Submitting a training run\n",
    "\n",
    "Below, you create the training run using a `ReinforcementLearningEstimator`\n",
    "object, which contains all the configuration parameters for this experiment:\n",
    "- `source_directory`: Contains the training script and helper files to be\n",
    "copied onto the node running the Ray head.\n",
    "- `entry_script`: The training script, described in more detail above..\n",
    "- `compute_target`: The compute target for the Ray head and training\n",
    "script execution.\n",
    "- `environment`: The Azure machine learning environment definition for\n",
    "the node running the Ray head.\n",
    "- `worker_configuration`: The configuration object for the additional\n",
    "Ray nodes to be attached to the Ray cluster:\n",
    "  - `compute_target`: The compute target for the additional Ray nodes.\n",
    "  - `node_count`: The number of nodes to attach to the Ray cluster.\n",
    "  - `environment`: The environment definition for the additional Ray nodes.\n",
    "- `max_run_duration_seconds`: The time after which to abort the run if it\n",
    "is still running.\n",
    "- `shm_size`: The size of docker container's shared memory block. \n",
    "\n",
    "For more details, please take a look at the [online documentation](https://docs.microsoft.com/en-us/python/api/azureml-contrib-reinforcementlearning/?view=azure-ml-py)\n",
    "for Azure Machine Learning service's reinforcement learning offering.\n",
    "\n",
    "We configure 8 extra D2 (worker) nodes for the Ray cluster, giving us a total of\n",
    "22 CPUs and 1 GPU.  The GPU and one CPU are used by the IMPALA learner,\n",
    "and each MineRL environment receives 2 CPUs allowing us to spawn a total\n",
    "of 10 rollout workers (see `num_workers` parameter in the training script).\n",
    "\n",
    "\n",
    "Lastly, the `RunDetails` widget displays information about the submitted\n",
    "RL experiment, including a link to the Azure portal with more details."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.train.rl import ReinforcementLearningEstimator, WorkerConfiguration\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "worker_config = WorkerConfiguration(\n",
    "    compute_target=cpu_cluster, \n",
    "    node_count=8,\n",
    "    environment=cpu_minecraft_env)\n",
    "\n",
    "rl_est = ReinforcementLearningEstimator(\n",
    "    source_directory='files',\n",
    "    entry_script='minecraft_train.py',\n",
    "    compute_target=gpu_cluster,\n",
    "    environment=gpu_minecraft_env,\n",
    "    worker_configuration=worker_config,\n",
    "    max_run_duration_seconds=6 * 60 * 60,\n",
    "    shm_size=1024 * 1024 * 1024 * 30)\n",
    "\n",
    "train_run = exp.submit(rl_est)\n",
    "\n",
    "RunDetails(train_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you wish to cancel the run before it completes, uncomment and execute:\n",
    "#train_run.cancel()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring training progress\n",
    "\n",
    "### View the Tensorboard\n",
    "\n",
    "The Tensorboard can be displayed via the Azure Machine Learning service's\n",
    "[Tensorboard API](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-monitor-tensorboard).\n",
    "When running locally, please make sure to follow the instructions in the\n",
    "link and install required packages.  Running this cell will output a URL\n",
    "for the Tensorboard.\n",
    "\n",
    "Note that the training script sets the log directory when starting RLlib\n",
    "via the `local_dir` parameter.  `./logs` will automatically appear in\n",
    "the downloadable files for a run.  Since this script is executed on the\n",
    "Ray head node run, we need to get a reference to it as shown below.\n",
    "\n",
    "The Tensorboard API will continuously stream logs from the run.\n",
    "\n",
    "**Note: It may take a couple of minutes after the run is in \"Running\" state\n",
    "before Tensorboard files are available and the board will refresh automatically**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from azureml.tensorboard import Tensorboard\n",
    "\n",
    "head_run = None\n",
    "\n",
    "timeout = 60\n",
    "while timeout > 0 and head_run is None:\n",
    "    timeout -= 1\n",
    "    \n",
    "    try:\n",
    "        head_run = next(r for r in train_run.get_children() if r.id.endswith('head'))\n",
    "    except StopIteration:\n",
    "        time.sleep(1)\n",
    "\n",
    "tb = Tensorboard([head_run], port=6007)\n",
    "tb.start()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review results\n",
    "\n",
    "Please ensure that the training run has completed before continuing with this section."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_run.wait_for_completion()\n",
    "\n",
    "print('Training run completed.')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note:** If the final \"episode_reward_mean\" metric from the training run is negative,\n",
    "the produced model does not solve the problem of navigating the maze well.  You can view\n",
    "the metric on the Tensorboard or in \"Metrics\" section of the head run in the Azure Machine Learning\n",
    "portal.  We recommend training a new model by rerunning the notebook starting from \"Submitting a training run\".\n",
    "\n",
    "\n",
    "### Export final model\n",
    "\n",
    "The key result from the training run is the final checkpoint\n",
    "containing the state of the IMPALA trainer (model) upon meeting the\n",
    "stopping criteria specified in `minecraft_train.py`.\n",
    "\n",
    "Azure Machine Learning service offers the [Model.register()](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py)\n",
    "API which allows you to persist the model files from the\n",
    "training run.  We identify the directory containing the\n",
    "final model written during the training run and register\n",
    "it with Azure Machine Learning service.  We use a Dataset\n",
    "object to filter out the correct files."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tempfile\n",
    "\n",
    "from azureml.core import Dataset\n",
    "\n",
    "path_prefix = os.path.join(tempfile.gettempdir(), 'tmp_training_artifacts')\n",
    "\n",
    "run_artifacts_path = os.path.join('azureml', head_run.id)\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "run_artifacts_ds = Dataset.File.from_files(datastore.path(os.path.join(run_artifacts_path, '**')))\n",
    "\n",
    "cp_pattern = re.compile('.*checkpoint-\\\\d+$')\n",
    "\n",
    "checkpoint_files = [file for file in run_artifacts_ds.to_path() if cp_pattern.match(file)]\n",
    "\n",
    "# There should only be one checkpoint with our training settings...\n",
    "final_checkpoint = os.path.dirname(os.path.join(run_artifacts_path, os.path.normpath(checkpoint_files[-1][1:])))\n",
    "datastore.download(target_path=path_prefix, prefix=final_checkpoint.replace('\\\\', '/'), show_progress=True)\n",
    "\n",
    "print('Download complete.')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model_name = 'final_model_minecraft_maze'\n",
    "\n",
    "model = Model.register(\n",
    "    workspace=ws,\n",
    "    model_path=os.path.join(path_prefix, final_checkpoint),\n",
    "    model_name=model_name,\n",
    "    description='Model of an agent trained to navigate a lava maze in Minecraft.')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models can be used through a varity of APIs.  Please see the\n",
    "[documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where)\n",
    "for more details.\n",
    "\n",
    "### Test agent performance in a rollout\n",
    "\n",
    "To observe the trained agent's behavior, it is a common practice to\n",
    "view its behavior in a rollout.  The previous reinforcement learning\n",
    "tutorials explain rollouts in more detail.\n",
    "\n",
    "The provided `minecraft_rollout.py` script loads the final checkpoint\n",
    "of the trained agent from the model registered with Azure Machine Learning\n",
    "service.  It then starts a rollout on 4 different lava maze layouts, that\n",
    "are all larger and thus more difficult than the maze the agent was trained\n",
    "on.  The script further records videos by replaying the agent's decisions\n",
    "in [Malmo](https://github.com/microsoft/malmo).  Malmo supports multiple\n",
    "agents in the same environment, thus allowing us to capture videos that\n",
    "depict the agent from another agent's perspective.  The provided\n",
    "`malmo_video_recorder.py` file and the Malmo Github repository have more\n",
    "details on the video recording setup.\n",
    "\n",
    "You can view the rewards for each rollout episode in the logs for the 'head'\n",
    "run submitted below.  In some episodes, the agent may fail to reach the goal\n",
    "due to the higher level of difficulty - in practice, we could continue\n",
    "training the agent on harder tasks starting with the final checkpoint."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    '--model_name': model_name\n",
    "}\n",
    "\n",
    "rollout_est = ReinforcementLearningEstimator(\n",
    "    source_directory='files',\n",
    "    entry_script='minecraft_rollout.py',\n",
    "    script_params=script_params,\n",
    "    compute_target=gpu_cluster,\n",
    "    environment=gpu_minecraft_env,\n",
    "    shm_size=1024 * 1024 * 1024 * 30)\n",
    "\n",
    "rollout_run = exp.submit(rollout_est)\n",
    "\n",
    "RunDetails(rollout_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View videos captured during rollout\n",
    "\n",
    "To inspect the agent's training progress you can view the videos captured\n",
    "during the rollout episodes.  First, ensure that the training run has\n",
    "completed."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_run.wait_for_completion()\n",
    "\n",
    "head_run_rollout = next(r for r in rollout_run.get_children() if r.id.endswith('head'))\n",
    "\n",
    "print('Rollout completed.')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to download the video files from the training run.  We use a\n",
    "Dataset to filter out the video files which are in tgz archives."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_run_artifacts_path = os.path.join('azureml', head_run_rollout.id)\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "rollout_run_artifacts_ds = Dataset.File.from_files(datastore.path(os.path.join(rollout_run_artifacts_path, '**')))\n",
    "\n",
    "video_archives = [file for file in rollout_run_artifacts_ds.to_path() if file.endswith('.tgz')]\n",
    "video_archives = [os.path.join(rollout_run_artifacts_path, os.path.normpath(file[1:])) for file in video_archives]\n",
    "\n",
    "datastore.download(\n",
    "    target_path=path_prefix,\n",
    "    prefix=os.path.dirname(video_archives[0]).replace('\\\\', '/'),\n",
    "    show_progress=True)\n",
    "\n",
    "print('Download complete.')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, unzip the video files and rename them by the Minecraft mission seed used\n",
    "(see `minecraft_rollout.py` for more details on how the seed is used)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "training_artifacts_dir = './training_artifacts'\n",
    "video_dir = os.path.join(training_artifacts_dir, 'videos')\n",
    "video_files = []\n",
    "\n",
    "for tar_file_path in video_archives:\n",
    "    seed = tar_file_path[tar_file_path.index('rollout_') + len('rollout_'): tar_file_path.index('.tgz')]\n",
    "    \n",
    "    tar = tarfile.open(os.path.join(path_prefix, tar_file_path).replace('\\\\', '/'), 'r')\n",
    "    tar_info = next(t_info for t_info in tar.getmembers() if t_info.name.endswith('mp4'))\n",
    "    tar.extract(tar_info, video_dir)\n",
    "    tar.close()\n",
    "    \n",
    "    unzipped_folder = os.path.join(video_dir, next(f_ for f_ in os.listdir(video_dir) if not f_.endswith('mp4')))    \n",
    "    video_file = os.path.join(unzipped_folder,'video.mp4')\n",
    "    final_video_path = os.path.join(video_dir, '{seed}.mp4'.format(seed=seed))\n",
    "    \n",
    "    shutil.move(video_file, final_video_path)    \n",
    "    video_files.append(final_video_path)\n",
    "    \n",
    "    shutil.rmtree(unzipped_folder)\n",
    "\n",
    "# Clean up any downloaded 'tmp' files\n",
    "shutil.rmtree(path_prefix)\n",
    "\n",
    "print('Local video files:\\n', video_files)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the cell below to display the videos in-line.  In some cases,\n",
    "the agent may struggle to find the goal since the maze size was increased\n",
    "compared to training."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "index = 0\n",
    "while index < len(video_files) - 1:\n",
    "    display(\n",
    "        HTML('\\\n",
    "        <video controls alt=\"cannot display video\" autoplay loop width=49%> \\\n",
    "            <source src=\"{f1}\" type=\"video/mp4\"> \\\n",
    "        </video> \\\n",
    "        <video controls alt=\"cannot display video\" autoplay loop width=49%> \\\n",
    "            <source src=\"{f2}\" type=\"video/mp4\"> \\\n",
    "        </video>'.format(f1=video_files[index], f2=video_files[index + 1]))\n",
    "    )\n",
    "    \n",
    "    index += 2\n",
    "\n",
    "if index < len(video_files):\n",
    "    display(\n",
    "        HTML('\\\n",
    "        <video controls alt=\"cannot display video\" autoplay loop width=49%> \\\n",
    "            <source src=\"{f1}\" type=\"video/mp4\"> \\\n",
    "        </video>'.format(f1=video_files[index]))\n",
    "    )"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "Below, you can find code snippets for your convenience to clean up any resources created as part of this tutorial you don't wish to retain."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to stop the Tensorboard, uncomment and run\n",
    "#tb.stop()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete the gpu compute target, uncomment and run\n",
    "#gpu_cluster.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete the cpu compute target, uncomment and run\n",
    "#cpu_cluster.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete the registered model, uncomment and run\n",
    "#model.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete the local video files, uncomment and run\n",
    "#shutil.rmtree(training_artifacts_dir)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "This is currently the last introductory tutorial for Azure Machine Learning\n",
    "service's Reinforcement\n",
    "Learning offering.  We would love to hear your feedback to build the features\n",
    "you need!\n",
    "\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/minecraft-on-distributed-compute/minecraft.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning in Azure Machine Learning - Pong problem\n",
    "Reinforcement Learning in Azure Machine Learning is a managed service for running distributed reinforcement learning training and simulation using the open source Ray framework.\n",
    "This example uses Ray RLlib to train a Pong playing agent on a multi-node cluster.\n",
    "\n",
    "## Pong problem\n",
    "[Pong](https://en.wikipedia.org/wiki/Pong) is a two-dimensional sports game that simulates table tennis. The player controls an in-game paddle by moving it vertically across the left or right side of the screen. They can compete against another player controlling a second paddle on the opposing side. Players use the paddles to hit a ball back and forth."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "      <th style=\"text-align: center;\"><img src=\"./images/pong.gif\" alt=\"Pong image\" align=\"middle\" margin-left=\"auto\" margin-right=\"auto\"/></th>\n",
    "  </tr>\n",
    "  <tr style=\"text-align: center;\">\n",
    "      <th>Fig 1. Pong game animation (from <a href=\"https://towardsdatascience.com/intro-to-reinforcement-learning-pong-92a94aa0f84d\">towardsdatascience.com</a>).</th>\n",
    "  </tr>\n",
    "</table>"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to train an agent to win an episode of Pong game against opponent with the score of at least 18 points. An episode in Pong runs until one of the players reaches a score of 21. Episodes are a terminology that is used across all the [OpenAI gym](https://gym.openai.com/envs/Pong-v0/) environments that contains a strictly defined task.\n",
    "\n",
    "Training a Pong agent is a compute-intensive task and this example demonstrates the use of Reinforcement Learning in Azure Machine Learning service to train an agent faster in a distributed, parallel environment. You'll learn more about using the head and the worker compute targets to train an agent in this notebook below."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "\n",
    "It is highly recommended that the user should go through the [Reinforcement Learning in Azure Machine Learning - Cartpole Problem on Single Compute](../cartpole-on-single-compute/cartpole_sc.ipynb) to understand the basics of Reinforcement Learning in Azure Machine Learning and Ray RLlib used in this notebook."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Development Environment\n",
    "The following subsections show typical steps to setup your development environment. Setup includes:\n",
    "\n",
    "* Connecting to a workspace to enable communication between your local machine and remote resources\n",
    "* Creating an experiment to track all your runs\n",
    "* Setting up a virtual network\n",
    "* Creating remote head and worker compute target on a virtual network to use for training"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Machine Learning SDK\n",
    "Display the Azure Machine Learning SDK version."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Azure Machine Learning core imports\n",
    "import azureml.core\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"Azure Machine Learning SDK Version: \", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Azure Machine Learning workspace\n",
    "Get a reference to an existing Azure Machine Learning workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, sep = ' | ')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Azure Machine Learning experiment\n",
    "Create an experiment to track the runs in your workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.experiment import Experiment\n",
    "\n",
    "# Experiment name\n",
    "experiment_name = 'rllib-pong-multi-node'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Virtual Network\n",
    "\n",
    "If you are using separate compute targets for the Ray head and worker, a virtual network must be created in the resource group.  If you have alraeady created a virtual network in the resource group, you can skip this step.\n",
    "\n",
    "To do this, you first must install the Azure Networking API.\n",
    "\n",
    "`pip install --upgrade azure-mgmt-network`"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to install the Azure Networking SDK, uncomment the following line.\n",
    "#!pip install --upgrade azure-mgmt-network"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.mgmt.network import NetworkManagementClient\n",
    "\n",
    "# Virtual network name\n",
    "vnet_name =\"rl_pong_vnet\"\n",
    "\n",
    "# Default subnet\n",
    "subnet_name =\"default\"\n",
    "\n",
    "# The Azure subscription you are using\n",
    "subscription_id=ws.subscription_id\n",
    "\n",
    "# The resource group for the reinforcement learning cluster\n",
    "resource_group=ws.resource_group\n",
    "\n",
    "# Azure region of the resource group\n",
    "location=ws.location\n",
    "\n",
    "network_client = NetworkManagementClient(ws._auth_object, subscription_id)\n",
    "\n",
    "async_vnet_creation = network_client.virtual_networks.create_or_update(\n",
    "    resource_group,\n",
    "    vnet_name,\n",
    "    {\n",
    "        'location': location,\n",
    "        'address_space': {\n",
    "            'address_prefixes': ['10.0.0.0/16']\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "async_vnet_creation.wait()\n",
    "print(\"Virtual network created successfully: \", async_vnet_creation.result())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Network Security Group on Virtual Network\n",
    "\n",
    "Depending on your Azure setup, you may need to open certain ports to make it possible for Azure to manage the compute targets that you create.  The ports that need to be opened are described [here](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-enable-virtual-network).\n",
    "\n",
    "A common situation is that ports `29876-29877` are closed.  The following code will add a security rule to open these ports.    Or you can do this manually in the [Azure portal](https://portal.azure.com).\n",
    "\n",
    "You may need to modify the code below to match your scenario."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.mgmt.network.models\n",
    "\n",
    "security_group_name = vnet_name + '-' + \"nsg\"\n",
    "security_rule_name = \"AllowAML\"\n",
    "\n",
    "# Create a network security group\n",
    "nsg_params = azure.mgmt.network.models.NetworkSecurityGroup(\n",
    "    location=location,\n",
    "    security_rules=[\n",
    "        azure.mgmt.network.models.SecurityRule(\n",
    "            name=security_rule_name,\n",
    "            access=azure.mgmt.network.models.SecurityRuleAccess.allow,\n",
    "            description='Reinforcement Learning in Azure Machine Learning rule',\n",
    "            destination_address_prefix='*',\n",
    "            destination_port_range='29876-29877',\n",
    "            direction=azure.mgmt.network.models.SecurityRuleDirection.inbound,\n",
    "            priority=400,\n",
    "            protocol=azure.mgmt.network.models.SecurityRuleProtocol.tcp,\n",
    "            source_address_prefix='BatchNodeManagement',\n",
    "            source_port_range='*'\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "async_nsg_creation = network_client.network_security_groups.create_or_update(\n",
    "    resource_group,\n",
    "    security_group_name,\n",
    "    nsg_params,\n",
    ")\n",
    "\n",
    "async_nsg_creation.wait() \n",
    "print(\"Network security group created successfully:\", async_nsg_creation.result())\n",
    "\n",
    "network_security_group = network_client.network_security_groups.get(\n",
    "    resource_group,\n",
    "    security_group_name,\n",
    ")\n",
    "\n",
    "# Define a subnet to be created with network security group\n",
    "subnet = azure.mgmt.network.models.Subnet(\n",
    "            id='default',\n",
    "            address_prefix='10.0.0.0/24',\n",
    "            network_security_group=network_security_group\n",
    "            )\n",
    "    \n",
    "# Create subnet on virtual network\n",
    "async_subnet_creation = network_client.subnets.create_or_update(\n",
    "    resource_group_name=resource_group,\n",
    "    virtual_network_name=vnet_name,\n",
    "    subnet_name=subnet_name,\n",
    "    subnet_parameters=subnet\n",
    ")\n",
    "\n",
    "async_subnet_creation.wait()\n",
    "print(\"Subnet created successfully:\", async_subnet_creation.result())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the virtual network security rules\n",
    "Ensure that the virtual network is configured correctly with required ports open. It is possible that you have configured rules with broader range of ports that allows ports 29876-29877 to be opened. Kindly review your network security group rules.  "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from files.networkutils import *\n",
    "\n",
    "check_vnet_security_rules(ws._auth_object, ws.subscription_id, ws.resource_group, vnet_name, True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create head compute target\n",
    "\n",
    "In this example, we show how to set up separate compute targets for the Ray head and Ray worker nodes. First we define the head cluster with GPU for the Ray head node. One CPU of the head node will be used for the Ray head process and the rest of the CPUs will be used by the Ray worker processes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "# Choose a name for the Ray head cluster\n",
    "head_compute_name = 'head-gpu'\n",
    "head_compute_min_nodes = 0\n",
    "head_compute_max_nodes = 2\n",
    "\n",
    "# This example uses GPU VM. For using CPU VM, set SKU to STANDARD_D2_V2\n",
    "head_vm_size = 'STANDARD_NC6'\n",
    "\n",
    "if head_compute_name in ws.compute_targets:\n",
    "    head_compute_target = ws.compute_targets[head_compute_name]\n",
    "    if head_compute_target and type(head_compute_target) is AmlCompute:\n",
    "        if head_compute_target.provisioning_state == 'Succeeded':\n",
    "            print('found head compute target. just use it', head_compute_name)\n",
    "        else: \n",
    "            raise Exception(\n",
    "                'found head compute target but it is in state', head_compute_target.provisioning_state)\n",
    "else:\n",
    "    print('creating a new head compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=head_vm_size,\n",
    "        min_nodes=head_compute_min_nodes, \n",
    "        max_nodes=head_compute_max_nodes,\n",
    "        vnet_resourcegroup_name=ws.resource_group,\n",
    "        vnet_name=vnet_name,\n",
    "        subnet_name='default')\n",
    "\n",
    "    # Create the cluster\n",
    "    head_compute_target = ComputeTarget.create(ws, head_compute_name, provisioning_config)\n",
    "    \n",
    "    # Can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # If no min node count is provided it will use the scale settings for the cluster\n",
    "    head_compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "    # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(head_compute_target.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create worker compute target\n",
    "\n",
    "Now we create a compute target with CPUs for the additional Ray worker nodes. CPUs in these worker nodes are used by Ray worker processes. Each Ray worker node, depending on the CPUs on the node, may have multiple Ray worker processes. There can be multiple worker tasks on each worker process (core)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a name for your Ray worker compute target\n",
    "worker_compute_name = 'worker-cpu'\n",
    "worker_compute_min_nodes = 0 \n",
    "worker_compute_max_nodes = 4\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "worker_vm_size = 'STANDARD_D2_V2'\n",
    "\n",
    "# Create the compute target if it hasn't been created already\n",
    "if worker_compute_name in ws.compute_targets:\n",
    "    worker_compute_target = ws.compute_targets[worker_compute_name]\n",
    "    if worker_compute_target and type(worker_compute_target) is AmlCompute:\n",
    "        if worker_compute_target.provisioning_state == 'Succeeded':\n",
    "            print('found worker compute target. just use it', worker_compute_name)\n",
    "        else: \n",
    "            raise Exception(\n",
    "                'found worker compute target but it is in state', head_compute_target.provisioning_state)\n",
    "else:\n",
    "    print('creating a new worker compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=worker_vm_size,\n",
    "        min_nodes=worker_compute_min_nodes,\n",
    "        max_nodes=worker_compute_max_nodes,\n",
    "        vnet_resourcegroup_name=ws.resource_group,\n",
    "        vnet_name=vnet_name,\n",
    "        subnet_name='default')\n",
    "\n",
    "    # Create the compute target\n",
    "    worker_compute_target = ComputeTarget.create(ws, worker_compute_name, provisioning_config)\n",
    "    \n",
    "    # Can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # If no min node count is provided it will use the scale settings for the cluster\n",
    "    worker_compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "    # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(worker_compute_target.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Pong Agent\n",
    "To facilitate reinforcement learning, Azure Machine Learning Python SDK provides a high level abstraction, the _ReinforcementLearningEstimator_ class, which allows users to easily construct  reinforcement learning run configurations for the underlying reinforcement learning framework.  Reinforcement Learning in Azure Machine Learning supports the open source [Ray framework](https://ray.io/) and its highly customizable [RLLib](https://ray.readthedocs.io/en/latest/rllib.html#rllib-scalable-reinforcement-learning). In this section we show how to use _ReinforcementLearningEstimator_ and Ray/RLLib framework to train a Pong playing agent.\n",
    "\n",
    "\n",
    "### Define worker configuration\n",
    "Define a `WorkerConfiguration` using your worker compute target. We specify the number of nodes in the worker compute target to be used for training and additional PIP packages to install on those nodes as a part of setup.\n",
    "In this case, we define the PIP packages as dependencies for both head and worker nodes. With this setup, the game simulations will run directly on the worker compute nodes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.train.rl import WorkerConfiguration\n",
    "\n",
    "# Pip packages we will use for both head and worker\n",
    "pip_packages=[\"ray[rllib]==0.8.3\"] # Latest version of Ray has fixes for isses related to object transfers\n",
    "\n",
    "# Specify the Ray worker configuration\n",
    "worker_conf = WorkerConfiguration(\n",
    "    \n",
    "    # Azure Machine Learning compute target to run Ray workers\n",
    "    compute_target=worker_compute_target, \n",
    "    \n",
    "    # Number of worker nodes\n",
    "    node_count=4,\n",
    "    \n",
    "    # GPU\n",
    "    use_gpu=False, \n",
    "    \n",
    "    # PIP packages to use\n",
    "    pip_packages=pip_packages\n",
    ")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create reinforcement learning estimator\n",
    "\n",
    "The `ReinforcementLearningEstimator` is used to submit a job to Azure Machine Learning to start the Ray experiment run. We define the training script parameters here that will be passed to the estimator. \n",
    "\n",
    "We specify `episode_reward_mean` to 18 as we want to stop the training as soon as the trained agent reaches an average win margin of at least 18 point over opponent over all episodes in the training epoch.\n",
    "Number of Ray worker processes are defined by parameter `num_workers`. We set it to 13 as we have 13 CPUs available in our compute targets. Multiple Ray worker processes parallelizes agent training and helps in achieving our goal faster. \n",
    "\n",
    "```\n",
    "Number of CPUs in head_compute_target = 6 CPUs in 1 node = 6\n",
    "Number of CPUs in worker_compute_target = 2 CPUs in each of 4 nodes = 8\n",
    "Number of CPUs available = (Number of CPUs in head_compute_target) + (Number of CPUs in worker_compute_target) - (1 CPU for head node) = 6 + 8 - 1 = 13\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.train.rl import ReinforcementLearningEstimator, Ray\n",
    "\n",
    "training_algorithm = \"IMPALA\"\n",
    "rl_environment = \"PongNoFrameskip-v4\"\n",
    "\n",
    "# Training script parameters\n",
    "script_params = {\n",
    "    \n",
    "    # Training algorithm, IMPALA in this case\n",
    "    \"--run\": training_algorithm,\n",
    "    \n",
    "    # Environment, Pong in this case\n",
    "    \"--env\": rl_environment,\n",
    "    \n",
    "    # Add additional single quotes at the both ends of string values as we have spaces in the \n",
    "    # string parameters, outermost quotes are not passed to scripts as they are not actually part of string\n",
    "    # Number of GPUs\n",
    "    # Number of ray workers\n",
    "    \"--config\": '\\'{\"num_gpus\": 1, \"num_workers\": 13}\\'',\n",
    "    \n",
    "    # Target episode reward mean to stop the training\n",
    "    # Total training time in seconds\n",
    "    \"--stop\": '\\'{\"episode_reward_mean\": 18, \"time_total_s\": 3600}\\'',\n",
    "}\n",
    "\n",
    "#  Reinforcement learning estimator\n",
    "rl_estimator = ReinforcementLearningEstimator(\n",
    "    \n",
    "    # Location of source files\n",
    "    source_directory='files',\n",
    "    \n",
    "    # Python script file\n",
    "    entry_script=\"pong_rllib.py\",\n",
    "    \n",
    "    # Parameters to pass to the script file\n",
    "    # Defined above.\n",
    "    script_params=script_params,\n",
    "    \n",
    "    # The Azure Machine Learning compute target set up for Ray head nodes\n",
    "    compute_target=head_compute_target,\n",
    "    \n",
    "    # Pip packages\n",
    "    pip_packages=pip_packages,\n",
    "    \n",
    "    # GPU usage\n",
    "    use_gpu=True,\n",
    "    \n",
    "    # Reinforcement learning framework. Currently must be Ray.\n",
    "    rl_framework=Ray(),\n",
    "    \n",
    "    # Ray worker configuration defined above.\n",
    "    worker_configuration=worker_conf,\n",
    "    \n",
    "    # How long to wait for whole cluster to start\n",
    "    cluster_coordination_timeout_seconds=3600,\n",
    "    \n",
    "    # Maximum time for the whole Ray job to run\n",
    "    # This will cut off the run after an hour\n",
    "    max_run_duration_seconds=3600,\n",
    "    \n",
    "    # Allow the docker container Ray runs in to make full use\n",
    "    # of the shared memory available from the host OS.\n",
    "    shm_size=24*1024*1024*1024\n",
    ")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training script\n",
    "As recommended in [RLlib](https://ray.readthedocs.io/en/latest/rllib.html) documentations, we use Ray [Tune](https://ray.readthedocs.io/en/latest/tune.html) API to run the training algorithm. All the RLlib built-in trainers are compatible with the Tune API. Here we use tune.run() to execute a built-in training algorithm. For convenience, down below you can see part of the entry script where we make this call.\n",
    "\n",
    "```python\n",
    "    tune.run(\n",
    "        run_or_experiment=args.run,\n",
    "        config={\n",
    "            \"env\": args.env,\n",
    "            \"num_gpus\": args.config[\"num_gpus\"],\n",
    "            \"num_workers\": args.config[\"num_workers\"],\n",
    "            \"callbacks\": {\"on_train_result\": callbacks.on_train_result},\n",
    "            \"sample_batch_size\": 50,\n",
    "            \"train_batch_size\": 1000,\n",
    "            \"num_sgd_iter\": 2,\n",
    "            \"num_data_loader_buffers\": 2,\n",
    "            \"model\": {\"dim\": 42},\n",
    "        },\n",
    "        stop=args.stop,\n",
    "        local_dir='./logs')\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the estimator to start a run\n",
    "Now we use the rl_estimator configured above to submit a run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(config=rl_estimator)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor the run\n",
    "\n",
    "Azure Machine Learning provides a Jupyter widget to show the status of an experiment run. You could use this widget to monitor the status of the runs. The widget shows the list of two child runs, one for head compute target run and one for worker compute target run. You can click on the link under **Status** to see the details of the child run. It will also show the metrics being logged."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop the run\n",
    "\n",
    "To stop the run, call `run.cancel()`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment line below to cancel the run\n",
    "# run.cancel()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for completion\n",
    "Wait for the run to complete before proceeding. If you want to stop the run, you may skip this and move to next section below. \n",
    "\n",
    "**Note: The run may take anywhere from 30 minutes to 45 minutes to complete.**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of the agent during training\n",
    "\n",
    "Let's get the reward metrics for the training run agent and observe how the agent's rewards improved over the training iterations and how the agent learns to win the Pong game. \n",
    "\n",
    "Collect the episode reward metrics from the worker run's metrics. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all child runs\n",
    "child_runs = list(run.get_children(_rehydrate_runs=False))\n",
    "\n",
    "# Get the reward metrics from worker run\n",
    "if child_runs[0].id.endswith(\"_worker\"):\n",
    "    episode_reward_mean = child_runs[0].get_metrics(name='episode_reward_mean')\n",
    "else:\n",
    "    episode_reward_mean = child_runs[1].get_metrics(name='episode_reward_mean')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the reward metrics. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(episode_reward_mean['episode_reward_mean'])\n",
    "plt.xlabel('training_iteration')\n",
    "plt.ylabel('episode_reward_mean')\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that during the training over multiple episodes, the agent learns to win the Pong game against opponent with our target of 18 points in each episode of 21 points.\n",
    "**Congratulations!! You have trained your Pong agent to win a game.**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "For your convenience, below you can find code snippets to clean up any resources created as part of this tutorial that you don't wish to retain."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To archive the created experiment:\n",
    "#experiment.archive()\n",
    "\n",
    "# To delete the compute targets:\n",
    "#head_compute_target.delete()\n",
    "#worker_compute_target.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "In this example, you learned how to solve distributed reinforcement learning training problems using head and worker compute targets. This was an introductory tutorial on Reinforement Learning in Azure Machine Learning service offering. We would love to hear your feedback to build the features you need!"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/atari-on-distributed-compute/pong_rllib.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning in Azure Machine Learning - Cartpole Problem on Compute Instance\n",
    "\n",
    "Reinforcement Learning in Azure Machine Learning is a managed service for running reinforcement learning training and simulation. With Reinforcement Learning in Azure Machine Learning, data scientists can start developing reinforcement learning systems on one machine, and scale to compute targets with 100s of nodes if needed.\n",
    "\n",
    "This example shows how to use Reinforcement Learning in Azure Machine Learning to train a Cartpole playing agent on a compute instance."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cartpole problem\n",
    "\n",
    "Cartpole, also known as [Inverted Pendulum](https://en.wikipedia.org/wiki/Inverted_pendulum), is a pendulum with a center of mass above its pivot point. This formation is essentially unstable and will easily fall over but can be kept balanced by applying appropriate horizontal forces to the pivot point.\n",
    "\n",
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "    <th>\n",
    "      <img src=\"./images/cartpole.png\" alt=\"Cartpole image\" /> \n",
    "    </th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <th><p>Fig 1. Cartpole problem schematic description (from <a href=\"https://towardsdatascience.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288\">towardsdatascience.com</a>).</p></th>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "The goal here is to train an agent to keep the cartpole balanced by applying appropriate forces to the pivot point.\n",
    "\n",
    "See [this video](https://www.youtube.com/watch?v=XiigTGKZfks) for a real-world demonstration of cartpole problem."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisite\n",
    "The user should have completed the Azure Machine Learning Tutorial: [Get started creating your first ML experiment with the Python SDK](https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-1st-experiment-sdk-setup). You will need to make sure that you have a valid subscription ID, a resource group, and an Azure Machine Learning workspace. All datastores and datasets you use should be associated with your workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Development Environment\n",
    "The following subsections show typical steps to setup your development environment. Setup includes:\n",
    "\n",
    "* Connecting to a workspace to enable communication between your local machine and remote resources\n",
    "* Creating an experiment to track all your runs\n",
    "* Using a Compute Instance as compute target"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Machine Learning SDK \n",
    "Display the Azure Machine Learning SDK version."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "print(\"Azure Machine Learning SDK Version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Azure Machine Learning workspace\n",
    "Get a reference to an existing Azure Machine Learning workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, sep = ' | ')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use  Compute Instance as compute target\n",
    "\n",
    "A compute target is a designated compute resource where you run your training and simulation scripts. This location may be your local machine or a cloud-based compute resource. For more information see [What are compute targets in Azure Machine Learning?](https://docs.microsoft.com/en-us/azure/machine-learning/concept-compute-target)\n",
    "\n",
    "The code below shows how to use current compute instance as a compute target. First some helper functions:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "# Get information about the currently running compute instance (notebook VM), like its name and prefix.\n",
    "def load_nbvm():\n",
    "    if not os.path.isfile(\"/mnt/azmnt/.nbvm\"):\n",
    "        return None\n",
    "    with open(\"/mnt/azmnt/.nbvm\", 'r') as nbvm_file:\n",
    "        return { key:value for (key, value) in [ line.strip().split('=') for line in nbvm_file if '=' in line ] }\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use these helper functions to get a handle to current compute instance."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeInstance\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Load current compute instance info\n",
    "current_compute_instance = load_nbvm()\n",
    "\n",
    "# For this demo, let's use the current compute instance as the compute target, if available\n",
    "if current_compute_instance:\n",
    "    print(\"Current compute instance:\", current_compute_instance)\n",
    "    instance_name = current_compute_instance['instance']\n",
    "else:\n",
    "    # Compute instance name needs to be unique across all existing compute instances within an Azure region\n",
    "    instance_name = \"cartpole-ci-\" + \"\".join(random.choice(string.ascii_lowercase) for _ in range(5))\n",
    "    try:\n",
    "        instance = ComputeInstance(workspace=ws, name=instance_name)\n",
    "        print('Found existing instance, use it.')\n",
    "    except ComputeTargetException:\n",
    "        print(\"Creating new compute instance...\")\n",
    "        compute_config = ComputeInstance.provisioning_configuration(\n",
    "            vm_size='STANDARD_D2_V2'\n",
    "        )\n",
    "        instance = ComputeInstance.create(ws, instance_name, compute_config)\n",
    "        instance.wait_for_completion(show_output=True)\n",
    "    print(\"Instance name:\", instance_name)\n",
    "\n",
    "compute_target = ws.compute_targets[instance_name]\n",
    "\n",
    "print(\"Compute target status:\")\n",
    "print(compute_target.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Azure Machine Learning experiment\n",
    "Create an experiment to track the runs in your workspace. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.experiment import Experiment\n",
    "\n",
    "experiment_name = 'CartPole-v0-CI'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Cartpole Agent\n",
    "To facilitate reinforcement learning, Azure Machine Learning Python SDK provides a high level abstraction, the _ReinforcementLearningEstimator_ class, which allows users to easily construct reinforcement learning run configurations for the underlying reinforcement learning framework. Reinforcement Learning in Azure Machine Learning supports the open source [Ray framework](https://ray.io/) and its highly customizable [RLlib](https://ray.readthedocs.io/en/latest/rllib.html#rllib-scalable-reinforcement-learning). In this section we show how to use _ReinforcementLearningEstimator_ and Ray/RLlib framework to train a cartpole playing agent. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create reinforcement learning estimator\n",
    "\n",
    "The code below creates an instance of *ReinforcementLearningEstimator*, `training_estimator`, which then will be used to submit a job to Azure Machine Learning to start the Ray experiment run.\n",
    "\n",
    "Note that this example is purposely simplified to the minimum. Here is a short description of the parameters we are passing into the constructor:\n",
    "\n",
    "- `source_directory`, local directory containing your training script(s) and helper modules,\n",
    "- `entry_script`, path to your entry script relative to the source directory,\n",
    "- `script_params`, constant parameters to be passed to each run of training script,\n",
    "- `compute_target`, reference to the compute target in which the trainer and worker(s) jobs will be executed,\n",
    "- `rl_framework`, the reinforcement learning framework to be used (currently must be Ray).\n",
    "\n",
    "We use the `script_params` parameter to pass in general and algorithm-specific parameters to the training script.\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.train.rl import ReinforcementLearningEstimator, Ray\n",
    "\n",
    "training_algorithm = \"PPO\"\n",
    "rl_environment = \"CartPole-v0\"\n",
    "\n",
    "script_params = {\n",
    "\n",
    "    # Training algorithm\n",
    "    \"--run\": training_algorithm,\n",
    "    \n",
    "    # Training environment\n",
    "    \"--env\": rl_environment,\n",
    "    \n",
    "    # Algorithm-specific parameters\n",
    "    \"--config\": '\\'{\"num_gpus\": 0, \"num_workers\": 1}\\'',\n",
    "    \n",
    "    # Stop conditions\n",
    "    \"--stop\": '\\'{\"episode_reward_mean\": 200, \"time_total_s\": 300}\\'',\n",
    "    \n",
    "    # Frequency of taking checkpoints\n",
    "    \"--checkpoint-freq\": 2,\n",
    "    \n",
    "    # If a checkpoint should be taken at the end - optional argument with no value\n",
    "    \"--checkpoint-at-end\": \"\",\n",
    "    \n",
    "    # Log directory\n",
    "    \"--local-dir\": './logs'\n",
    "}\n",
    "\n",
    "training_estimator = ReinforcementLearningEstimator(\n",
    "\n",
    "    # Location of source files\n",
    "    source_directory='files',\n",
    "    \n",
    "    # Python script file\n",
    "    entry_script='cartpole_training.py',\n",
    "    \n",
    "    # A dictionary of arguments to pass to the training script specified in ``entry_script``\n",
    "    script_params=script_params,\n",
    "    \n",
    "    # The Azure Machine Learning compute target set up for Ray head nodes\n",
    "    compute_target=compute_target,\n",
    "    \n",
    "    # Reinforcement learning framework. Currently must be Ray.\n",
    "    rl_framework=Ray()\n",
    ")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training script\n",
    "\n",
    "As recommended in RLlib documentations, we use Ray Tune API to run the training algorithm. All the RLlib built-in trainers are compatible with the Tune API. Here we use `tune.run()` to execute a built-in training algorithm. For convenience, down below you can see part of the entry script where we make this call.\n",
    "\n",
    "This is the list of parameters we are passing into `tune.run()` via the `script_params` parameter:\n",
    "\n",
    "- `run_or_experiment`: name of the [built-in algorithm](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#rllib-algorithms), 'PPO' in our example,\n",
    "- `config`: Algorithm-specific configuration. This includes specifying the environment, `env`, which in our example is the gym **[CartPole-v0](https://gym.openai.com/envs/CartPole-v0/)** environment,\n",
    "- `stop`: stopping conditions, which could be any of the metrics returned by the trainer. Here we use \"mean of episode reward\", and \"total training time in seconds\" as stop conditions, and\n",
    "- `checkpoint_freq` and `checkpoint_at_end`: Frequency of taking checkpoints (number of training iterations between checkpoints), and if a checkpoint should be taken at the end.\n",
    "\n",
    "We also specify the `local_dir`, the directory in which the training logs, checkpoints and other training artificats will be recorded. \n",
    "\n",
    "See [RLlib Training APIs](https://ray.readthedocs.io/en/latest/rllib-training.html#rllib-training-apis) for more details, and also [Training (tune.run, tune.Experiment)](https://ray.readthedocs.io/en/latest/tune/api_docs/execution.html#training-tune-run-tune-experiment) for the complete list of parameters.\n",
    "\n",
    "```python\n",
    "import ray\n",
    "import ray.tune as tune\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # parse arguments ...\n",
    "    \n",
    "    # Intitialize ray\n",
    "    ay.init(address=args.ray_address)\n",
    "\n",
    "    # Run training task using tune.run\n",
    "    tune.run(\n",
    "        run_or_experiment=args.run,\n",
    "        config=dict(args.config, env=args.env),\n",
    "        stop=args.stop,\n",
    "        checkpoint_freq=args.checkpoint_freq,\n",
    "        checkpoint_at_end=args.checkpoint_at_end,\n",
    "        local_dir=args.local_dir\n",
    "    )\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the estimator to start experiment\n",
    "Now we use the *training_estimator* to submit a run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_run = exp.submit(training_estimator)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor experiment\n",
    "Azure Machine Learning provides a Jupyter widget to show the status of an experiment run. You could use this widget to monitor the status of the runs.\n",
    "\n",
    "Note that _ReinforcementLearningEstimator_ creates at least two runs: (a) A parent run, i.e. the run returned above, and (b) a collection of child runs. The number of the child runs depends on the configuration of the reinforcement learning estimator. In our simple scenario, configured above, only one child run will be created.\n",
    "\n",
    "The widget will show a list of the child runs as well. You can click on the link under **Status** to see the details of a child run. It will also show the metrics being logged."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(training_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop the run\n",
    "\n",
    "To stop the run, call `training_run.cancel()`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment line below to cancel the run\n",
    "# training_run.cancel()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for completion\n",
    "Wait for the run to complete before proceeding.\n",
    "\n",
    "**Note: The run may take a few minutes to complete.**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_run.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a handle to the child run\n",
    "You can obtain a handle to the child run as follows. In our scenario, there is only one child run, we have it called `child_run_0`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "child_run_0 = None\n",
    "timeout = 30\n",
    "while timeout > 0 and not child_run_0:\n",
    "    child_runs = list(training_run.get_children())\n",
    "    print('Number of child runs:', len(child_runs))\n",
    "    if len(child_runs) > 0:\n",
    "        child_run_0 = child_runs[0]\n",
    "        break\n",
    "    time.sleep(2) # Wait for 2 seconds\n",
    "    timeout -= 2\n",
    "\n",
    "print('Child run info:')\n",
    "print(child_run_0)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Trained Agent and See Results\n",
    "\n",
    "We can evaluate a previously trained policy using the `rollout.py` helper script provided by RLlib (see [Evaluating Trained Policies](https://ray.readthedocs.io/en/latest/rllib-training.html#evaluating-trained-policies) for more details). Here we use an adaptation of this script to reconstruct a policy from a checkpoint taken and saved during training. We took these checkpoints by setting `checkpoint-freq` and `checkpoint-at-end` parameters above.\n",
    "\n",
    "In this section we show how to get access to these checkpoints data, and then how to use them to evaluate the trained policy."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset of training artifacts\n",
    "To evaluate a trained policy (a checkpoint) we need to make the checkpoint accessible to the rollout script. All the training artifacts are stored in workspace default datastore under **azureml/&lt;run_id&gt;** directory.\n",
    "\n",
    "Here we create a file dataset from the stored artifacts, and then use this dataset to feed these data to rollout estimator."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "run_id = child_run_0.id # Or set to run id of a completed run (e.g. 'rl-cartpole-v0_1587572312_06e04ace_head')\n",
    "run_artifacts_path = os.path.join('azureml', run_id)\n",
    "print(\"Run artifacts path:\", run_artifacts_path)\n",
    "\n",
    "# Create a file dataset object from the files stored on default datastore\n",
    "datastore = ws.get_default_datastore()\n",
    "training_artifacts_ds = Dataset.File.from_files(datastore.path(os.path.join(run_artifacts_path, '**')))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify, we can print out the number (and paths) of all the files in the dataset, as follows."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts_paths = training_artifacts_ds.to_path()\n",
    "print(\"Number of files in dataset:\", len(artifacts_paths))\n",
    "\n",
    "# Uncomment line below to print all file paths\n",
    "#print(\"Artifacts dataset file paths: \", artifacts_paths)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate a trained policy\n",
    "We need to configure another reinforcement learning estimator, `rollout_estimator`, and then use it to submit another run. Note that the entry script for this estimator now points to `cartpole-rollout.py` script.\n",
    "Also note how we pass the checkpoints dataset to this script using `inputs` parameter of the _ReinforcementLearningEstimator_.\n",
    "\n",
    "We are using script parameters to pass in the same algorithm and the same environment used during training. We also specify the checkpoint number of the checkpoint we wish to evaluate, `checkpoint-number`, and number of the steps we shall run the rollout, `steps`.\n",
    "\n",
    "The checkpoints dataset will be accessible to the rollout script as a mounted folder. The mounted folder and the checkpoint number, passed in via `checkpoint-number`, will be used to create a path to the checkpoint we are going to evaluate. The created checkpoint path then will be passed into RLlib rollout script for evaluation.\n",
    "\n",
    "Let's find the checkpoints and the last checkpoint number first."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find checkpoints and last checkpoint number\n",
    "checkpoint_files = [\n",
    "    os.path.basename(file) for file in training_artifacts_ds.to_path() \\\n",
    "        if os.path.basename(file).startswith('checkpoint-') and \\\n",
    "            not os.path.basename(file).endswith('tune_metadata')\n",
    "]\n",
    "\n",
    "checkpoint_numbers = []\n",
    "for file in checkpoint_files:\n",
    "    checkpoint_numbers.append(int(file.split('-')[1]))\n",
    "\n",
    "print(\"Checkpoints:\", checkpoint_numbers)\n",
    "\n",
    "last_checkpoint_number = max(checkpoint_numbers)\n",
    "print(\"Last checkpoint number:\", last_checkpoint_number)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's configure rollout estimator. Note that we use the last checkpoint for evaluation. The assumption is that the last checkpoint points to our best trained agent. You may change this to any of the checkpoint numbers printed above and observe the effect."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {    \n",
    "    # Checkpoint number of the checkpoint from which to roll out\n",
    "    \"--checkpoint-number\": last_checkpoint_number,\n",
    "\n",
    "    # Training algorithm\n",
    "    \"--run\": training_algorithm,\n",
    "    \n",
    "    # Training environment\n",
    "    \"--env\": rl_environment,\n",
    "    \n",
    "    # Algorithm-specific parameters\n",
    "    \"--config\": '{}',\n",
    "    \n",
    "    # Number of rollout steps \n",
    "    \"--steps\": 2000,\n",
    "    \n",
    "    # If should repress rendering of the environment\n",
    "    \"--no-render\": \"\"\n",
    "}\n",
    "\n",
    "rollout_estimator = ReinforcementLearningEstimator(\n",
    "    # Location of source files\n",
    "    source_directory='files',\n",
    "    \n",
    "    # Python script file\n",
    "    entry_script='cartpole_rollout.py',\n",
    "    \n",
    "    # A dictionary of arguments to pass to the rollout script specified in ``entry_script``\n",
    "    script_params = script_params,\n",
    "    \n",
    "    # Data inputs\n",
    "    inputs=[\n",
    "        training_artifacts_ds.as_named_input('artifacts_dataset'),\n",
    "        training_artifacts_ds.as_named_input('artifacts_path').as_mount()],\n",
    "    \n",
    "    # The Azure Machine Learning compute target\n",
    "    compute_target=compute_target,\n",
    "    \n",
    "    # Reinforcement learning framework. Currently must be Ray.\n",
    "    rl_framework=Ray(),\n",
    "    \n",
    "    # Additional pip packages to install\n",
    "    pip_packages = ['azureml-dataset-runtime[fuse,pandas]'])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as before, we use the *rollout_estimator* to submit a run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_run = exp.submit(rollout_estimator)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, similar to the training section, we can monitor the real-time progress of the rollout run and its chid as follows. If you browse logs of the child run you can see the evaluation results recorded in driver_log.txt file. Note that you may need to wait several minutes before these results become available."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(rollout_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for completion of the rollout run, or you may cancel the run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment line below to cancel the run\n",
    "#rollout_run.cancel()\n",
    "rollout_run.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "For your convenience, below you can find code snippets to clean up any resources created as part of this tutorial that you don't wish to retain."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To archive the created experiment:\n",
    "#exp.archive()\n",
    "\n",
    "# To delete created compute instance\n",
    "if not current_compute_instance:\n",
    "    compute_target.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "This example was about running Reinforcement Learning in Azure Machine Learning (Ray/RLlib Framework) on a compute instance. Please see [Cartpole Problem on Single Compute](../cartpole-on-single-compute/cartpole_sc.ipynb)\n",
    "example which uses Ray RLlib to train a Cartpole playing agent on a single node remote compute.\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/reinforcement-learning/cartpole-on-compute-instance/cartpole_ci.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-quickdemo.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze data drift in Azure Machine Learning datasets \n",
    "\n",
    "In this tutorial, you will setup a data drift monitor on a weather dataset to:\n",
    "\n",
    "&#x2611; Analyze historical data for drift\n",
    "\n",
    "&#x2611; Setup a monitor to recieve email alerts if data drift is detected going forward\n",
    "\n",
    "If your workspace is Enterprise level, view and exlpore the results in the Azure Machine Learning studio. The video below shows the results from this tutorial. \n",
    "\n",
    "![gif](media/video.gif)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "If you are using an Azure Machine Learning Compute instance, you are all set. Otherwise, go through the [configuration notebook](../../../configuration.ipynb) if you haven't already established your connection to the AzureML Workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print('SDK version:', azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "ws"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup target and baseline datasets\n",
    "\n",
    "Setup the baseline and target datasets. The baseline will be used to compare each time slice of the target dataset, which is sampled by a given frequency. For further details, see [our documentation](http://aka.ms/datadrift). \n",
    "\n",
    "The next few cells will:\n",
    "  * get the default datastore\n",
    "  * upload the `weather-data` to the datastore\n",
    "  * create the Tabular dataset from the data\n",
    "  * add the timeseries trait by specifying the timestamp column `datetime`\n",
    "  * register the dataset\n",
    "  * create the baseline as a time slice of the target dataset\n",
    "  * optionally, register the baseline dataset\n",
    "  \n",
    "The folder `weather-data` contains weather data from the [NOAA Integrated Surface Data](https://azure.microsoft.com/services/open-datasets/catalog/noaa-integrated-surface-data/) filtered down to to station names containing the string 'FLORIDA' to reduce the size of data. See `get_data.py` to see how this data is curated and modify as desired. This script may take a long time to run, hence the data is provided in the `weather-data` folder for this demo."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use default datastore\n",
    "dstore = ws.get_default_datastore()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload weather data\n",
    "dstore.upload('weather-data', 'datadrift-data', overwrite=True, show_progress=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Dataset class\n",
    "from azureml.core import Dataset\n",
    "\n",
    "# create target dataset \n",
    "target = Dataset.Tabular.from_parquet_files(dstore.path('datadrift-data/**/data.parquet'))\n",
    "# set the timestamp column\n",
    "target = target.with_timestamp_columns('datetime')\n",
    "# register the target dataset\n",
    "target = target.register(ws, 'target')\n",
    "# retrieve the dataset from the workspace by name\n",
    "target = Dataset.get_by_name(ws, 'target')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime \n",
    "from datetime import datetime\n",
    "\n",
    "# set baseline dataset as January 2019 weather data\n",
    "baseline = Dataset.Tabular.from_parquet_files(dstore.path('datadrift-data/2019/01/data.parquet'))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally, register the baseline dataset. if skipped, an unregistered dataset will be used\n",
    "#baseline = baseline.register(ws, 'baseline')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create compute target\n",
    "\n",
    "Create an Azure Machine Learning compute cluster to run the data drift monitor and associated runs. The below cell will create a compute cluster named `'cpu-cluster'`. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "compute_name = 'cpu-cluster'\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3_V2', min_nodes=0, max_nodes=2)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "    # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data drift monitor\n",
    "\n",
    "See [our documentation](http://aka.ms/datadrift) for a complete description for all of the parameters. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "datadrift-remarks-sample"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.datadrift import DataDriftDetector, AlertConfiguration\n",
    "\n",
    "alert_config = AlertConfiguration(['user@contoso.com']) # replace with your email to recieve alerts from the scheduled pipeline after enabling\n",
    "\n",
    "monitor = DataDriftDetector.create_from_datasets(ws, 'weather-monitor', baseline, target, \n",
    "                                                      compute_target='cpu-cluster',         # compute target for scheduled pipeline and backfills \n",
    "                                                      frequency='Week',                     # how often to analyze target data\n",
    "                                                      feature_list=None,                    # list of features to detect drift on\n",
    "                                                      drift_threshold=None,                 # threshold from 0 to 1 for email alerting\n",
    "                                                      latency=0,                            # SLA in hours for target data to arrive in the dataset\n",
    "                                                      alert_config=alert_config)            # email addresses to send alert"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update data drift monitor\n",
    "\n",
    "Many settings of the data drift monitor can be updated after creation. In this demo, we will update the `drift_threshold` and `feature_list`. See [our documentation](http://aka.ms/datadrift) for details on which settings can be changed."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get monitor by name\n",
    "monitor = DataDriftDetector.get_by_name(ws, 'weather-monitor')\n",
    "\n",
    "# create feature list - need to exclude columns that naturally drift or increment over time, such as year, day, index\n",
    "columns  = list(baseline.take(1).to_pandas_dataframe())\n",
    "exclude  = ['year', 'day', 'version', '__index_level_0__']\n",
    "features = [col for col in columns if col not in exclude]\n",
    "\n",
    "# update the feature list\n",
    "monitor  = monitor.update(feature_list=features)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze historical data and backfill\n",
    "\n",
    "You can use the `backfill` method to:\n",
    "  * analyze historical data\n",
    "  * backfill metrics after updating the settings (mainly the feature list)\n",
    "  * backfill metrics for failed runs\n",
    "  \n",
    "The below cells will run two backfills that will produce data drift results for 2019 weather data, with January used as the baseline in the monitor. The output can be seen from the `show` method after the runs have completed, or viewed from the Azure Machine Learning studio for Enterprise workspaces.\n",
    "\n",
    "![Drift results](media/drift-results.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    ">**Tip!** When starting with the data drift capability, start by backfilling on a small section of data to get initial results. Update the feature list as needed by removing columns that are causing drift, but can be ignored, and backfill this section of data until satisfied with the results. Then, backfill on a larger slice of data and/or set the alert configuration, threshold, and enable the schedule to recieve alerts to drift on your dataset. All of this can be done through the UI (Enterprise) or Python SDK."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it depends on many factors, the below backfill should typically take less than 20 minutes to run. Results will show as soon as they become available, not when the backfill is completed, so you may begin to see some metrics in a few minutes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backfill for one month\n",
    "backfill_start_date = datetime(2019, 9, 1)\n",
    "backfill_end_date = datetime(2019, 10, 1)\n",
    "backfill = monitor.backfill(backfill_start_date, backfill_end_date)\n",
    "backfill"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query metrics and show results in Python\n",
    "\n",
    "The below cell will plot some key data drift metrics, and can be used to query the results. Run `help(monitor.get_output)` for specifics on the object returned."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the backfill has completed\n",
    "backfill.wait_for_completion(wait_post_processing=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get results from Python SDK (wait for backfills or monitor runs to finish)\n",
    "results, metrics = monitor.get_output(start_time=datetime(year=2019, month=9, day=1))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results from Python SDK \n",
    "monitor.show(backfill_start_date, backfill_end_date)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable the monitor's pipeline schedule\n",
    "\n",
    "Turn on a scheduled pipeline which will anlayze the target dataset for drift every `frequency`. Use the latency parameter to adjust the start time of the pipeline. For instance, if it takes 24 hours for my data processing pipelines for data to arrive in the target dataset, set latency to 24. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable the pipeline schedule and recieve email alerts\n",
    "monitor.enable_schedule()\n",
    "\n",
    "# disable the pipeline schedule \n",
    "#monitor.disable_schedule()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete compute target\n",
    "\n",
    "Do not delete the compute target if you intend to keep using it for the data drift monitor scheduled runs or otherwise. If the minimum nodes are set to 0, it will scale down soon after jobs are completed, and scale up the next time the cluster is needed."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally delete the compute target\n",
    "#compute_target.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the DataDriftDetector\n",
    "\n",
    "Invoking the `delete()` method on the object deletes the the drift monitor permanently and cannot be undone. You will no longer be able to find it in the UI and the `list()` or `get()` methods. The object on which delete() was called will have its state set to deleted and name suffixed with deleted. The baseline and target datasets and model data that was collected, if any, are not deleted. The compute is not deleted. The DataDrift schedule pipeline is disabled and archived."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "  * See [our documentation](https://aka.ms/datadrift) or [Python SDK reference](https://docs.microsoft.com/python/api/overview/azure/ml/intro)\n",
    "  * [Send requests or feedback](mailto:driftfeedback@microsoft.com) on data drift directly to the team\n",
    "  * Please open issues with data drift here on GitHub or on StackOverflow if others are likely to run into the same issue"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/scriptrun-with-data-input-output/how-to-use-scriptrun.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/work-with-data/datasets-tutorial/scriptun-with-data-input-output.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/scriptrun-with-data-input-output/how-to-use-scriptrun.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use configure a training run with data input and output\n",
    "\n",
    "This notebook shows how to use [ScriptRunConfig](https://docs.microsoft.com/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py) with input and output. A run submitted with ScriptRunConfig represents a single trial in an experiment. Submitting the run returns a ScriptRun object, which can be used to monitor the asynchronous execution of the run, log metrics and store output of the run, and analyze results and access artifacts generated by the run.\n",
    "\n",
    "\n",
    "## Prerequisite:\n",
    "* Understand the [architecture and terms](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning\n",
    "* If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration notebook](https://aka.ms/pl-config) to:\n",
    "    * install the AML SDK\n",
    "    * create a workspace and its configuration file (`config.json`)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/scriptrun-with-data-input-output/how-to-use-scriptrun.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/scriptrun-with-data-input-output/how-to-use-scriptrun.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/scriptrun-with-data-input-output/how-to-use-scriptrun.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Attach existing AmlCompute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training your model. In this tutorial, you create `AmlCompute` as your training compute resource."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/scriptrun-with-data-input-output/how-to-use-scriptrun.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we could not find the cluster with the given name, then we will create a new cluster here. We will create an `AmlCompute` cluster of `STANDARD_D2_V2` GPU VMs. This process is broken down into 3 steps:\n",
    "1. create the configuration (this step is local and only takes a second)\n",
    "2. create the cluster (this step will take about **20 seconds**)\n",
    "3. provision the VMs to bring the cluster to the initial size (of 1 in this case). This step will take about **3-5 minutes** and is providing only sparse output in the process. Please make sure to wait until the call returns before moving to the next cell"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/scriptrun-with-data-input-output/how-to-use-scriptrun.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"cpu-cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2', max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it uses the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/scriptrun-with-data-input-output/how-to-use-scriptrun.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have created the compute target, let's see what the workspace's `compute_targets` property returns. You should now see one entry named 'cpu-cluster' of type `AmlCompute`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/scriptrun-with-data-input-output/how-to-use-scriptrun.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a simple script\n",
    "We have already created a simple \"hello world\" script. This is the script that we will submit through the [ScriptRunConfig](https://docs.microsoft.com/python/api/azureml-core/azureml.core.script_run_config.scriptrunconfig?view=azure-ml-py). It reads iris dataset as input, and write it out to `outputdataset` folder in default blob datastore. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/scriptrun-with-data-input-output/how-to-use-scriptrun.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory = 'script_run'"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/scriptrun-with-data-input-output/how-to-use-scriptrun.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $source_directory/dummy_train.py\n",
    "\n",
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"*********************************************************\")\n",
    "print(\"Hello Azure ML!\")\n",
    "\n",
    "mounted_input_path = sys.argv[1]\n",
    "mounted_output_path = sys.argv[2]\n",
    "\n",
    "print(\"Argument 1: %s\" % mounted_input_path)\n",
    "print(\"Argument 2: %s\" % mounted_output_path)\n",
    "    \n",
    "with open(mounted_input_path, 'r') as f:\n",
    "    content = f.read()\n",
    "    with open(os.path.join(mounted_output_path, 'output.csv'), 'w') as fw:\n",
    "        fw.write(content)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/scriptrun-with-data-input-output/how-to-use-scriptrun.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every workspace comes with a default datastore (and you can register more) which is backed by the Azure blob storage account associated with the workspace. We can use it to transfer data from local to the cloud, and create dataset from it. We will now upload the Iris data to the default datastore (blob) within your workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/scriptrun-with-data-input-output/how-to-use-scriptrun.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_blob_store = ws.get_default_datastore()\n",
    "def_blob_store.upload_files(files = ['iris.csv'],\n",
    "                       target_path = 'script-run/',\n",
    "                       overwrite = True,\n",
    "                       show_progress = True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/scriptrun-with-data-input-output/how-to-use-scriptrun.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to define the input and output of your script. They can be passed in via `arguments`, which is a list of command-line arguments to pass to the training script specified in `script`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/scriptrun-with-data-input-output/how-to-use-scriptrun.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "\n",
    "input_data = Dataset.File.from_files(def_blob_store.path('script-run/iris.csv')).as_named_input('input').as_mount()\n",
    "\n",
    "# output is configured to write the result back to def_blob_store, under \"sample/outputdataset\" folder\n",
    "# learn more about options to configure the output, run 'help(OutputFileDatasetConfig)'\n",
    "output = OutputFileDatasetConfig(destination=(def_blob_store, 'sample/outputdataset'))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/scriptrun-with-data-input-output/how-to-use-scriptrun.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "myenv = Environment(\"myenv\")\n",
    "\n",
    "myenv.docker.enabled = True\n",
    "myenv.python.conda_dependencies = CondaDependencies.create(pip_packages=['azureml-sdk>=1.12.0'])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/scriptrun-with-data-input-output/how-to-use-scriptrun.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "src = ScriptRunConfig(source_directory=source_directory, \n",
    "                      script='dummy_train.py', \n",
    "                      # to mount the dataset on the remote compute and pass the mounted path as an argument to the training script\n",
    "                      arguments =[input_data, output],\n",
    "                      compute_target=compute_target,\n",
    "                      environment=myenv)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/scriptrun-with-data-input-output/how-to-use-scriptrun.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Submit the Experiment"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/scriptrun-with-data-input-output/how-to-use-scriptrun.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "exp = Experiment(ws, 'ScriptRun_sample')\n",
    "run = exp.submit(config=src)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/scriptrun-with-data-input-output/how-to-use-scriptrun.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Run Details"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/scriptrun-with-data-input-output/how-to-use-scriptrun.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/scriptrun-with-data-input-output/how-to-use-scriptrun.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with Azure Machine Learning datasets\n",
    "Datasets are categorized into TabularDataset and FileDataset based on how users consume them in training. \n",
    "* A TabularDataset represents data in a tabular format by parsing the provided file or list of files. TabularDataset can be created from csv, tsv, parquet files, SQL query results etc. For the complete list, please visit our [documentation](https://aka.ms/tabulardataset-api-reference). It provides you with the ability to materialize the data into a pandas DataFrame.\n",
    "* A FileDataset references single or multiple files in your datastores or public urls. This provides you with the ability to download or mount the files to your compute. The files can be of any format, which enables a wider range of machine learning scenarios including deep learning.\n",
    "\n",
    "In this tutorial, you will learn how to train with Azure Machine Learning datasets:\n",
    "\n",
    "&#x2611; Use datasets directly in your training script\n",
    "\n",
    "&#x2611; Use datasets to mount files to a remote compute"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration notebook](../../../configuration.ipynb) first if you haven't already established your connection to the AzureML Workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print('SDK version:', azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experiment\n",
    "\n",
    "**Experiment** is a logical container in an Azure ML Workspace. It hosts run records which can include run metrics and output artifacts from your experiments."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'train-with-datasets'\n",
    "\n",
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Attach existing compute resource\n",
    "By using Azure Machine Learning Compute, a managed service, data scientists can train machine learning models on clusters of Azure virtual machines. Examples include VMs with GPU support. In this tutorial, you create Azure Machine Learning Compute as your training environment. The code below creates the compute clusters for you if they don't already exist in your workspace.\n",
    "\n",
    "**Creation of compute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace the code will skip the creation process."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get('AML_COMPUTE_CLUSTER_NAME', 'cpu-cluster')\n",
    "compute_min_nodes = os.environ.get('AML_COMPUTE_CLUSTER_MIN_NODES', 0)\n",
    "compute_max_nodes = os.environ.get('AML_COMPUTE_CLUSTER_MAX_NODES', 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get('AML_COMPUTE_CLUSTER_SKU', 'STANDARD_D2_V2')\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n",
    "                                                                min_nodes=compute_min_nodes, \n",
    "                                                                max_nodes=compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have the necessary packages and compute resources to train a model in the cloud.\n",
    "## Use datasets directly in training\n",
    "\n",
    "### Create a TabularDataset\n",
    "By creating a dataset, you create a reference to the data source location. If you applied any subsetting transformations to the dataset, they will be stored in the dataset as well. The data remains in its existing location, so no extra storage cost is incurred. \n",
    "\n",
    "Every workspace comes with a default [datastore](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-access-data) (and you can register more) which is backed by the Azure blob storage account associated with the workspace. We can use it to transfer data from local to the cloud, and create dataset from it. We will now upload the [Iris data](./train-dataset/Iris.csv) to the default datastore (blob) within your workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = ws.get_default_datastore()\n",
    "datastore.upload_files(files = ['./train-dataset/iris.csv'],\n",
    "                       target_path = 'train-dataset/tabular/',\n",
    "                       overwrite = True,\n",
    "                       show_progress = True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will create an unregistered TabularDataset pointing to the path in the datastore. You can also create a dataset from multiple paths. [learn more](https://aka.ms/azureml/howto/createdatasets) \n",
    "\n",
    "[TabularDataset](https://docs.microsoft.com/python/api/azureml-core/azureml.data.tabulardataset?view=azure-ml-py) represents data in a tabular format by parsing the provided file or list of files. This provides you with the ability to materialize the data into a Pandas or Spark DataFrame. You can create a TabularDataset object from .csv, .tsv, and parquet files, and from SQL query results. For a complete list, see [TabularDatasetFactory](https://docs.microsoft.com/python/api/azureml-core/azureml.data.dataset_factory.tabulardatasetfactory?view=azure-ml-py) class."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "dataset-remarks-tabular-sample"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "dataset = Dataset.Tabular.from_delimited_files(path = [(datastore, 'train-dataset/tabular/iris.csv')])\n",
    "\n",
    "# preview the first 3 rows of the dataset\n",
    "dataset.take(3).to_pandas_dataframe()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training script\n",
    "\n",
    "To submit the job to the cluster, first create a training script. Run the following code to create the training script called `train_titanic.py` in the script_folder. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_folder = os.path.join(os.getcwd(), 'train-dataset')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/train_iris.py\n",
    "\n",
    "import os\n",
    "\n",
    "from azureml.core import Dataset, Run\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# sklearn.externals.joblib is removed in 0.23\n",
    "from sklearn import __version__ as sklearnver\n",
    "from packaging.version import Version\n",
    "if Version(sklearnver) < Version(\"0.23.0\"):\n",
    "    from sklearn.externals import joblib\n",
    "else:\n",
    "    import joblib\n",
    "\n",
    "run = Run.get_context()\n",
    "# get input dataset by name\n",
    "dataset = run.input_datasets['iris']\n",
    "\n",
    "df = dataset.to_pandas_dataframe()\n",
    "\n",
    "x_col = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "y_col = ['species']\n",
    "x_df = df.loc[:, x_col]\n",
    "y_df = df.loc[:, y_col]\n",
    "\n",
    "#dividing X,y into train and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.2, random_state=223)\n",
    "\n",
    "data = {'train': {'X': x_train, 'y': y_train},\n",
    "\n",
    "        'test': {'X': x_test, 'y': y_test}}\n",
    "\n",
    "clf = DecisionTreeClassifier().fit(data['train']['X'], data['train']['y'])\n",
    "model_file_name = 'decision_tree.pkl'\n",
    "\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'.format(clf.score(x_train, y_train)))\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.2f}'.format(clf.score(x_test, y_test)))\n",
    "\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "with open(model_file_name, 'wb') as file:\n",
    "    joblib.dump(value=clf, filename='outputs/' + model_file_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an environment\n",
    "\n",
    "Define a conda environment YAML file with your training script dependencies and create an Azure ML environment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile conda_dependencies.yml\n",
    "\n",
    "dependencies:\n",
    "- python=3.6.2\n",
    "- scikit-learn\n",
    "- pip:\n",
    "  - azureml-defaults\n",
    "  - packaging"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "sklearn_env = Environment.from_conda_specification(name = 'sklearn-env', file_path = './conda_dependencies.yml')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure training run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ScriptRunConfig object specifies the configuration details of your training job, including your training script, environment to use, and the compute target to run on. Specify the following in your script run configuration:\n",
    "* The directory that contains your scripts. All the files in this directory are uploaded into the cluster nodes for execution. \n",
    "* The training script name, train_iris.py\n",
    "* The input dataset for training, passed as an argument to your training script. `as_named_input()` is required so that the input dataset can be referenced by the assigned name in your training script. \n",
    "* The compute target. In this case you will use the AmlCompute you created\n",
    "* The environment definition for the experiment"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "src = ScriptRunConfig(source_directory=script_folder,\n",
    "                      script='train_iris.py',\n",
    "                      arguments=[dataset.as_named_input('iris')],\n",
    "                      compute_target=compute_target,\n",
    "                      environment=sklearn_env)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job to run\n",
    "Submit the ScriptRunConfig to the Azure ML experiment to kick off the execution."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(src)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# monitor the run\n",
    "RunDetails(run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use datasets to mount files to a remote compute\n",
    "\n",
    "You can use the `Dataset` object to mount or download files referred by it. When you mount a file system, you attach that file system to a directory (mount point) and make it available to the system. Because mounting load files at the time of processing, it is usually faster than download.<br> \n",
    "Note: mounting is only available for Linux-based compute (DSVM/VM, AMLCompute, HDInsights)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data files into datastore\n",
    "We will first load diabetes data from `scikit-learn` to the train-dataset folder."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "import numpy as np\n",
    "\n",
    "training_data = load_diabetes()\n",
    "np.save(file='train-dataset/features.npy', arr=training_data['data'])\n",
    "np.save(file='train-dataset/labels.npy', arr=training_data['target'])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's upload the 2 files into the default datastore under a path named `diabetes`:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore.upload_files(['train-dataset/features.npy', 'train-dataset/labels.npy'], target_path='diabetes', overwrite=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a FileDataset\n",
    "\n",
    "[FileDataset](https://docs.microsoft.com/python/api/azureml-core/azureml.data.file_dataset.filedataset?view=azure-ml-py) references single or multiple files in your datastores or public URLs. Using this method, you can download or mount the files to your compute as a FileDataset object. The files can be in any format, which enables a wider range of machine learning scenarios, including deep learning."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "dataset = Dataset.File.from_files(path = [(datastore, 'diabetes/')])\n",
    "\n",
    "# see a list of files referenced by dataset\n",
    "dataset.to_path()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training script\n",
    "\n",
    "To submit the job to the cluster, first create a training script. Run the following code to create the training script called `train_diabetes.py` in the script_folder. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/train_diabetes.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "\n",
    "from azureml.core.run import Run\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "# sklearn.externals.joblib is removed in 0.23\n",
    "from sklearn import __version__ as sklearnver\n",
    "from packaging.version import Version\n",
    "if Version(sklearnver) < Version(\"0.23.0\"):\n",
    "    from sklearn.externals import joblib\n",
    "else:\n",
    "    import joblib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, help='training dataset')\n",
    "args = parser.parse_args()\n",
    "\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "\n",
    "base_path = args.data_folder\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "X = np.load(glob.glob(os.path.join(base_path, '**/features.npy'), recursive=True)[0])\n",
    "y = np.load(glob.glob(os.path.join(base_path, '**/labels.npy'), recursive=True)[0])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)\n",
    "data = {'train': {'X': X_train, 'y': y_train},\n",
    "        'test': {'X': X_test, 'y': y_test}}\n",
    "\n",
    "# list of numbers from 0.0 to 1.0 with a 0.05 interval\n",
    "alphas = np.arange(0.0, 1.0, 0.05)\n",
    "\n",
    "for alpha in alphas:\n",
    "    # use Ridge algorithm to create a regression model\n",
    "    reg = Ridge(alpha=alpha)\n",
    "    reg.fit(data['train']['X'], data['train']['y'])\n",
    "\n",
    "    preds = reg.predict(data['test']['X'])\n",
    "    mse = mean_squared_error(preds, data['test']['y'])\n",
    "    run.log('alpha', alpha)\n",
    "    run.log('mse', mse)\n",
    "\n",
    "    model_file_name = 'ridge_{0:.2f}.pkl'.format(alpha)\n",
    "    with open(model_file_name, 'wb') as file:\n",
    "        joblib.dump(value=reg, filename='outputs/' + model_file_name)\n",
    "\n",
    "    print('alpha is {0:.2f}, and mse is {1:0.2f}'.format(alpha, mse))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure & Run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now configure your run. We will reuse the same `sklearn_env` environment from the previous run. Once the environment is built, and if you don't change your dependencies, it will be reused in subsequent runs. \n",
    "\n",
    "We will pass in the DatasetConsumptionConfig of our FileDataset to the `'--data-folder'` argument of the script. Azure ML will resolve this to mount point of the data on the compute target, which we parse in the training script."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "src = ScriptRunConfig(source_directory=script_folder, \n",
    "                      script='train_diabetes.py', \n",
    "                      # to mount the dataset on the remote compute and pass the mounted path as an argument to the training script\n",
    "                      arguments =['--data-folder', dataset.as_mount()],\n",
    "                      compute_target=compute_target,\n",
    "                      environment=sklearn_env)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(config=src)\n",
    "\n",
    "# monitor the run\n",
    "RunDetails(run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display run results\n",
    "You now have a model trained on a remote cluster. Retrieve all the metrics logged during the run, including the accuracy of the model:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion()\n",
    "metrics = run.get_metrics()\n",
    "print(metrics)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register datasets\n",
    "Use the register() method to register datasets to your workspace so they can be shared with others, reused across various experiments, and referred to by name in your training script."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.register(workspace = ws,\n",
    "                           name = 'diabetes dataset',\n",
    "                           description='training dataset',\n",
    "                           create_new_version=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register models with datasets\n",
    "The last step in the training script wrote the model files in a directory named `outputs` in the VM of the cluster where the job is executed. `outputs` is a special directory in that all content in this directory is automatically uploaded to your workspace. This content appears in the run record in the experiment under your workspace. Hence, the model file is now also available in your workspace.\n",
    "\n",
    "You can register models with datasets for reproducibility and auditing purpose."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the index where MSE is the smallest\n",
    "indices = list(range(0, len(metrics['mse'])))\n",
    "min_mse_index = min(indices, key=lambda x: metrics['mse'][x])\n",
    "\n",
    "print('When alpha is {1:0.2f}, we have min MSE {0:0.2f}.'.format(\n",
    "    metrics['mse'][min_mse_index], \n",
    "    metrics['alpha'][min_mse_index]\n",
    "))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best model\n",
    "best_alpha = metrics['alpha'][min_mse_index]\n",
    "model_file_name = 'ridge_{0:.2f}.pkl'.format(best_alpha)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the best model with the input dataset\n",
    "model = run.register_model(model_name='sklearn_diabetes', model_path=os.path.join('outputs', model_file_name),\n",
    "                           datasets =[('training data',dataset)])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to labeled datasets\n",
    "\n",
    "Labeled datasets are output from Azure Machine Learning [labeling projects](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-create-labeling-projects). It captures the reference to the data (e.g. image files) and its labels. \n",
    "\n",
    "This tutorial introduces the capabilities of labeled datasets and how to use it in training.\n",
    "\n",
    "Learn how-to:\n",
    "\n",
    "> * Set up your development environment\n",
    "> * Explore labeled datasets\n",
    "> * Train a simple deep learning neural network on a remote cluster\n",
    "\n",
    "## Prerequisite:\n",
    "* Understand the [architecture and terms](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning\n",
    "* Go through Azure Machine Learning [labeling projects](https://docs.microsoft.com/azure/machine-learning/service/how-to-create-labeling-projects) and export the labels as an Azure Machine Learning dataset\n",
    "* Go through the [configuration notebook](../../../configuration.ipynb) to:\n",
    "    * install the latest version of azureml-sdk\n",
    "    * install the latest version of azureml-contrib-dataset\n",
    "    * install [PyTorch](https://pytorch.org/)\n",
    "    * create a workspace and its configuration file (`config.json`)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your development environment\n",
    "\n",
    "All the setup for your development work can be accomplished in a Python notebook.  Setup includes:\n",
    "\n",
    "* Importing Python packages\n",
    "* Connecting to a workspace to enable communication between your local computer and remote resources\n",
    "* Creating an experiment to track all your runs\n",
    "* Creating a remote compute target to use for training\n",
    "\n",
    "### Import packages\n",
    "\n",
    "Import Python packages you need in this session. Also display the Azure Machine Learning SDK version."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "import azureml.contrib.dataset\n",
    "from azureml.core import Dataset, Workspace, Experiment\n",
    "from azureml.contrib.dataset import FileHandlingOption\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)\n",
    "print(\"Azure ML Contrib Version\", azureml.contrib.dataset.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to workspace\n",
    "\n",
    "Create a workspace object from the existing workspace. `Workspace.from_config()` reads the file **config.json** and loads the details into an object named `workspace`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load workspace\n",
    "workspace = Workspace.from_config()\n",
    "print('Workspace name: ' + workspace.name, \n",
    "      'Azure region: ' + workspace.location, \n",
    "      'Subscription id: ' + workspace.subscription_id, \n",
    "      'Resource group: ' + workspace.resource_group, sep='\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create experiment and a directory\n",
    "\n",
    "Create an experiment to track the runs in your workspace and a directory to deliver the necessary code from your computer to the remote resource."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an ML experiment\n",
    "exp = Experiment(workspace=workspace, name='labeled-datasets')\n",
    "\n",
    "# create a directory\n",
    "script_folder = './labeled-datasets'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Attach existing compute resource\n",
    "By using Azure Machine Learning Compute, a managed service, data scientists can train machine learning models on clusters of Azure virtual machines. Examples include VMs with GPU support. In this tutorial, you will create Azure Machine Learning Compute as your training environment. The code below creates the compute clusters for you if they don't already exist in your workspace.\n",
    "\n",
    "**Creation of compute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace the code will skip the creation process."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"openhack\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=workspace, name=cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', \n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(workspace, cluster_name, compute_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it uses the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore labeled datasets\n",
    "\n",
    "**Note**: How to create labeled datasets is not covered in this tutorial. To create labeled datasets, you can go through [labeling projects](https://docs.microsoft.com/azure/machine-learning/service/how-to-create-labeling-projects) and export the output labels as Azure Machine Lerning datasets. \n",
    "\n",
    "`animal_labels` used in this tutorial section is the output from a labeling project, with the task type of \"Object Identification\"."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get animal_labels dataset from the workspace\n",
    "animal_labels = Dataset.get_by_name(workspace, 'animal_labels')\n",
    "animal_labels"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load labeled datasets into pandas DataFrame. There are 3 file handling option that you can choose to load the data files referenced by the labeled datasets:\n",
    "* Streaming: The default option to load data files.\n",
    "* Download: Download your data files to a local path.\n",
    "* Mount: Mount your data files to a mount point. Mount only works for Linux-based compute, including Azure Machine Learning notebook VM and Azure Machine Learning Compute."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_pd = animal_labels.to_pandas_dataframe(file_handling_option=FileHandlingOption.DOWNLOAD, target_path='./download/', overwrite_download=True)\n",
    "animal_pd"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# read images from downloaded path\n",
    "img = mpimg.imread(animal_pd.loc[0,'image_url'])\n",
    "imgplot = plt.imshow(img)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also load labeled datasets into [torchvision datasets](https://pytorch.org/docs/stable/torchvision/datasets.html), so that you can leverage on the open source libraries provided by PyTorch for image transformation and training."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import functional as F\n",
    "\n",
    "# load animal_labels dataset into torchvision dataset\n",
    "pytorch_dataset = animal_labels.to_torchvision()\n",
    "img = pytorch_dataset[0][0]\n",
    "print(type(img))\n",
    "\n",
    "# use methods from torchvision to transform the img into grayscale\n",
    "pil_image = F.to_pil_image(img)\n",
    "gray_image = F.to_grayscale(pil_image, num_output_channels=3)\n",
    "\n",
    "imgplot = plt.imshow(gray_image)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an image classification model\n",
    "\n",
    " `crack_labels` dataset used in this tutorial section is the output from a labeling project, with the task type of \"Image Classification Multi-class\". We will use this dataset to train an image classification model that classify whether an image has cracks or not."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get crack_labels dataset from the workspace\n",
    "crack_labels = Dataset.get_by_name(workspace, 'crack_labels')\n",
    "crack_labels"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure training job\n",
    "\n",
    "You can ask the system to build a conda environment based on your dependency specification. Once the environment is built, and if you don't change your dependencies, it will be reused in subsequent runs."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "conda_env = Environment('conda-env')\n",
    "conda_env.python.conda_dependencies = CondaDependencies.create(pip_packages=['azureml-sdk',\n",
    "                                                                             'azureml-contrib-dataset',\n",
    "                                                                             'torch','torchvision',\n",
    "                                                                             'azureml-dataset-runtime[pandas]'])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ScriptRunConfig object is used to submit the run. Create a ScriptRunConfig by specifying\n",
    "\n",
    "* The directory that contains your scripts. All the files in this directory are uploaded into the cluster nodes for execution. \n",
    "* The training script name, train.py\n",
    "* The input dataset for training\n",
    "* The compute target. In this case you will use the AmlCompute you created\n",
    "* The environment for the experiment"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "src = ScriptRunConfig(source_directory=script_folder,\n",
    "                      script='train.py',\n",
    "                      arguments=[crack_labels.as_named_input('crack_labels')],\n",
    "                      compute_target=compute_target,\n",
    "                      enviroment=conda_env)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job to run\n",
    "\n",
    "Submit the ScriptRunConfig to the Azure ML experiment to kick off the execution."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(src)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Time Series Related API Demo with NOAA Weather Data"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved. <br>\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will learn how to use the Tabular Time Series related API to filter the data by time windows for sample data uploaded to Azure blob storage. \n",
    "\n",
    "The detailed APIs to be demoed in this script are:\n",
    "- Create Tabular Dataset instance\n",
    "- Assign timestamp column and partition timestamp column for Tabular Dataset to activate Time Series related APIs\n",
    "- Clear timestamp column and partition timestamp column\n",
    "- Filter in data before a specific time\n",
    "- Filter in data after a specific time\n",
    "- Filter in data in a specific time range\n",
    "- Filter in data for recent time range\n",
    "\n",
    "Besides above APIs, you'll also see:\n",
    "- Create and load a Workspace\n",
    "- Load weather data into Azure blob storage\n",
    "- Create and register weather data as a Tabular dataset\n",
    "- Re-load Tabular Dataset from your Workspace"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies\n",
    "\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, run the cells below to install the Azure Machine Learning Python SDK and create an Azure ML Workspace that's required for this demo."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Environment"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out your version of the Azure ML Python SDK. Version 1.0.60 or above is required for TabularDataset with timeseries attribute. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "azureml.data.__version__"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from azureml.core import Dataset, Workspace"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Configuraton and Create Azure ML Workspace\n",
    "\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration notebook](https://github.com/Azure/MachineLearningNotebooks/blob/master/configuration.ipynb) first if you haven't already to establish your connection to the Azure ML Workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "dstore = ws.get_default_datastore()\n",
    "\n",
    "dset_name = 'weather-data-florida'\n",
    "\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, dstore.name, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data to Blob Storage\n",
    "\n",
    "This demo uses 2019 weather data under within weather-data folder. You can replace this data with your own."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload data to blob storage so it can be used as a Dataset."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstore.upload('weather-data', dset_name, overwrite=True, show_progress=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create & Register Tabular Dataset with time-series trait from Blob\n",
    "\n",
    "The API on Tabular datasets with time-series trait is specially designed to handle Tabular time-series data and time related operations more efficiently. By registering your time-series dataset, you are publishing your dataset to your workspace so that it is accessible to anyone with the same subscription id. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Tabular Dataset instance from blob storage datapath.\n",
    "\n",
    "**TIP:** you can set virtual columns in the partition_format. I.e. if you partition the weather data by state and city, the path can be '/{STATE}/{CITY}/{partition_time:yyy/MM}/data.parquet'. STATE and CITY would then appear as virtual columns in the dataset, allowing for efficient filtering by these timestamps. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_path = [(dstore, dset_name + '/*/*/data.parquet')]\n",
    "dataset        = Dataset.Tabular.from_parquet_files(path=datastore_path, partition_format = dset_name + '/{partition_time:yyyy/MM}/data.parquet')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign \"datetime\" column as timestamp and \"partition_time\" from folder path as partition_timestamp for Tabular Dataset to activate Time Series related APIs. The column to be assigned should be a Date type, otherwise the assigning will fail."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsd = dataset.with_timestamp_columns(timestamp='datetime', partition_timestamp='partition_time')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the dataset for easy access from anywhere in Azure ML and to keep track of versions, lineage. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register dataset to Workspace\n",
    "registered_ds = tsd.register(ws, dset_name, create_new_version=True, description='Data for Tabular Dataset with time-series trait demo.', tags={ 'type': 'TabularDataset' })"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload the Dataset from Workspace"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset by dataset name\n",
    "tsd = Dataset.get_by_name(ws, name=dset_name)\n",
    "tsd.to_pandas_dataframe().head(5)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Data by Time Windows\n",
    "\n",
    "Once your data has been loaded into the notebook, you can query by time using the time_before(), time_after(), time_between(), and time_recent() functions.The filter is optimized to only load those data files within the partition_timestamp range when partition_timestamp is specified.\n",
    "\n",
    "include_boundary is default to be true for all the time series related filters, please pass include_boundary=False to exclude boundary."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before Time Input"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select data that occurs before a specified date\n",
    "tsd2 = tsd.time_before(datetime(2019, 6, 12))\n",
    "tsd2.to_pandas_dataframe().tail(5)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Time Input"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select data that occurs after a specified date\n",
    "tsd2 = tsd.time_after(datetime(2019, 5, 30))\n",
    "tsd2.to_pandas_dataframe().head(5)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before & After Time Inputs\n",
    "\n",
    "You can chain time functions together."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select data that occurs within a given time range\n",
    "tsd2 = tsd.time_after(datetime(2019, 1, 1)).time_before(datetime(2019, 1, 10))\n",
    "tsd2.to_pandas_dataframe().head(5)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Range Input"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to select data that occurs within a given time range\n",
    "tsd2 = tsd.time_between(start_time=datetime(2019, 1, 31, 23, 59, 59), end_time=datetime(2019, 2, 7))\n",
    "tsd2.to_pandas_dataframe().head(5)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Recent Input"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in a datetime.timedelta and returns a dataset containing the data from datetime.now()-timedelta() to datetime.now()."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** This will return an empty dataframe there is no data within the last 2 days."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsd2 = tsd.time_recent(timedelta(days=2))\n",
    "tsd2.to_pandas_dataframe().tail(5)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop and keep columns\n",
    "\n",
    "You can also choose to drop or keep certain columns."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Columns"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>If a timeseries column is dropped, the corresponding capabilities will be dropped for the returned dataset.</font><br>"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsd2 = tsd.drop_columns(columns=['snowDepth', 'version', 'datetime'])\n",
    "tsd2.take(5).to_pandas_dataframe()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exception is expected because dataset loses timeseries capabilities to do time travel."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.exceptions import DatasetTimestampMissingError\n",
    "\n",
    "try:\n",
    "    tsd2.time_before(datetime(2019, 6, 12)).to_pandas_dataframe().tail(5)\n",
    "except DatasetTimestampMissingError as e:\n",
    "    print('Expected exception : {}'.format(str(e)))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop will return dataset with timeseries capabilities if modify column list to exclude timestamp columns."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsd2 = tsd.drop_columns(columns=['snowDepth', 'version', 'upload_date'])\n",
    "tsd2.take(5).to_pandas_dataframe()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsd2.time_before(datetime(2019, 6, 12)).to_pandas_dataframe().tail(5)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep Columns"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>If a timeseries column is not included, the timeseries capabilities will be dropped for the returned dataset.</font><br>"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsd2 = tsd.keep_columns(columns=['snowDepth'], validate=False)\n",
    "tsd2.to_pandas_dataframe().tail()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exception is expected because dataset loses timeseries capabilities to do time travel."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tsd2.time_before(datetime(2019, 6, 12)).to_pandas_dataframe().tail(5)\n",
    "except DatasetTimestampMissingError as e:\n",
    "    print('Expected exception : {}'.format(str(e)))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep will return dataset with timeseries capabilities if modify column list to include timestamp columns."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsd2 = tsd.keep_columns(columns=['snowDepth', 'datetime', 'partition_time'], validate=False)\n",
    "tsd2.to_pandas_dataframe().tail()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsd2.time_before(datetime(2019, 6, 12)).to_pandas_dataframe().tail(5)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resetting Timestamp Columns"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rules for reseting are:\n",
    "- You cannot assign 'None' to timestamp while assign a valid column name to partition_timestamp because partition_timestamp is optional while timestamp is mandatory for Tabular time series data.\n",
    "- If you assign 'None' to timestamp, then both timestamp and partition_timestamp will all be cleared.\n",
    "- If you assign only 'None' to partition_timestamp, then only partition_timestamp will be cleared."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.exceptions import UserErrorException\n",
    "# Illegal clearing, exception is expected.\n",
    "try:\n",
    "    tsd2 = tsd.with_timestamp_columns(timestamp=None, partition_timestamp='partition_time')\n",
    "except UserErrorException as e:\n",
    "    print('Cleaning not allowed because {}'.format(str(e)))\n",
    "\n",
    "# clear both\n",
    "tsd2 = tsd.with_timestamp_columns(timestamp=None, partition_timestamp=None)\n",
    "print('after clean both with None/None, timestamp columns are: {}'.format(tsd2.timestamp_columns))\n",
    "\n",
    "# clear partition_timestamp only and assign 'datetime' as timestamp column\n",
    "tsd2 = tsd2.with_timestamp_columns(timestamp='datetime', partition_timestamp=None)\n",
    "print('after clean partition timestamp column, timestamp columns are: {}'.format(tsd2.timestamp_columns))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/work-with-data/datasets-tutorial/datasets-tutorial.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License [2017] Zalando SE, https://tech.zalando.com"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a simple ML pipeline for image classification\n",
    "\n",
    "## Introduction\n",
    "This tutorial shows how to train a simple deep neural network using the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset and Keras on Azure Machine Learning. Fashion-MNIST is a dataset of Zalando's article images\u00e2\u20ac\u201dconsisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.\n",
    "\n",
    "Learn how to:\n",
    "\n",
    "> * Set up your development environment\n",
    "> * Create the Fashion MNIST dataset\n",
    "> * Create a machine learning pipeline to train a simple deep learning neural network on a remote cluster\n",
    "> * Retrieve input datasets from the experiment and register the output model with datasets\n",
    "\n",
    "## Prerequisite:\n",
    "* Understand the [architecture and terms](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning\n",
    "* If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration notebook](../../../configuration.ipynb) to:\n",
    "    * install the latest version of AzureML SDK\n",
    "    * create a workspace and its configuration file (`config.json`)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your development environment\n",
    "\n",
    "All the setup for your development work can be accomplished in a Python notebook.  Setup includes:\n",
    "\n",
    "* Importing Python packages\n",
    "* Connecting to a workspace to enable communication between your local computer and remote resources\n",
    "* Creating an experiment to track all your runs\n",
    "* Creating a remote compute target to use for training\n",
    "\n",
    "### Import packages\n",
    "\n",
    "Import Python packages you need in this session. Also display the Azure Machine Learning SDK version."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Dataset, Datastore, ComputeTarget, Experiment, ScriptRunConfig\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import Pipeline\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to workspace\n",
    "\n",
    "Create a workspace object from the existing workspace. `Workspace.from_config()` reads the file **config.json** and loads the details into an object named `workspace`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load workspace\n",
    "workspace = Workspace.from_config()\n",
    "print('Workspace name: ' + workspace.name, \n",
    "      'Azure region: ' + workspace.location, \n",
    "      'Subscription id: ' + workspace.subscription_id, \n",
    "      'Resource group: ' + workspace.resource_group, sep='\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create experiment and a directory\n",
    "\n",
    "Create an experiment to track the runs in your workspace and a directory to deliver the necessary code from your computer to the remote resource."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an ML experiment\n",
    "exp = Experiment(workspace=workspace, name='keras-mnist-fashion')\n",
    "\n",
    "# create a directory\n",
    "script_folder = './keras-mnist-fashion'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Attach existing compute resource\n",
    "By using Azure Machine Learning Compute, a managed service, data scientists can train machine learning models on clusters of Azure virtual machines. Examples include VMs with GPU support. In this tutorial, you create Azure Machine Learning Compute as your training environment. The code below creates the compute clusters for you if they don't already exist in your workspace.\n",
    "\n",
    "**Creation of compute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace the code will skip the creation process."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"gpu-cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=workspace, name=cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', \n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(workspace, cluster_name, compute_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it uses the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Fashion MNIST dataset\n",
    "\n",
    "By creating a dataset, you create a reference to the data source location. If you applied any subsetting transformations to the dataset, they will be stored in the dataset as well. The data remains in its existing location, so no extra storage cost is incurred."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_urls = ['https://data4mldemo6150520719.blob.core.windows.net/demo/mnist-fashion']\n",
    "fashion_ds = Dataset.File.from_files(data_urls)\n",
    "\n",
    "# list the files referenced by fashion_ds\n",
    "fashion_ds.to_path()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build 2-step ML pipeline\n",
    "\n",
    "The [Azure Machine Learning Pipeline](https://docs.microsoft.com/azure/machine-learning/service/concept-ml-pipelines) enables data scientists to create and manage multiple simple and complex workflows concurrently. A typical pipeline would have multiple tasks to prepare data, train, deploy and evaluate models. Individual steps in the pipeline can make use of diverse compute options (for example: CPU for data preparation and GPU for training) and languages. [Learn More](https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/machine-learning-pipelines)\n",
    "\n",
    "\n",
    "### Step 1: data preparation\n",
    "\n",
    "In step one, we will load the image and labels from Fashion MNIST dataset into mnist_train.csv and mnist_test.csv\n",
    "\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. Both mnist_train.csv and mnist_test.csv contain 785 columns. The first column consists of the class labels, which represent the article of clothing. The rest of the columns contain the pixel-values of the associated image."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intermediate data (or output of a step) is represented by a `OutputFileDatasetConfig` object. preprared_fashion_ds is produced as the output of step 1, and used as the input of step 2. `OutputFileDatasetConfig` introduces a data dependency between steps, and creates an implicit execution order in the pipeline. You can register a `OutputFileDatasetConfig` as a dataset and version the output data automatically."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data import OutputFileDatasetConfig\n",
    "\n",
    "# learn more about the output config\n",
    "help(OutputFileDatasetConfig)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write output to datastore under folder `outputdataset` and register it as a dataset after the experiment completes\n",
    "# make sure the service principal in your datastore has blob data contributor role in order to write data back\n",
    "datastore=workspace.get_default_datastore()\n",
    "prepared_fashion_ds = OutputFileDatasetConfig(destination=(datastore, 'outputdataset/{run-id}')).register_on_complete(name='prepared_fashion_ds')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **PythonScriptStep** is a basic, built-in step to run a Python Script on a compute target. It takes a script name and optionally other parameters like arguments for the script, compute target, inputs and outputs. If no compute target is specified, default compute target for the workspace is used. You can also use a [**RunConfiguration**](https://docs.microsoft.com/python/api/azureml-core/azureml.core.runconfiguration?view=azure-ml-py) to specify requirements for the PythonScriptStep, such as conda dependencies and docker image."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_step = PythonScriptStep(name='prepare step',\n",
    "                             script_name=\"prepare.py\",\n",
    "                             # mount fashion_ds dataset to the compute_target\n",
    "                             arguments=[fashion_ds.as_named_input('fashion_ds').as_mount(), prepared_fashion_ds],\n",
    "                             source_directory=script_folder,\n",
    "                             compute_target=compute_target,\n",
    "                             allow_reuse=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: train CNN with Keras\n",
    "\n",
    "Next, construct a ScriptRunConfig to configure the training run that trains a CNN model using Keras. It takes a dataset as the input."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile conda_dependencies.yml\n",
    "\n",
    "dependencies:\n",
    "- python=3.6.2\n",
    "- pip:\n",
    "  - azureml-defaults\n",
    "  - keras\n",
    "  - tensorflow\n",
    "  - numpy\n",
    "  - scikit-learn\n",
    "  - pandas\n",
    "  - matplotlib"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "keras_env = Environment.from_conda_specification(name = 'keras-env', file_path = './conda_dependencies.yml')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src = ScriptRunConfig(source_directory=script_folder,\n",
    "                            script='train.py',\n",
    "                            compute_target=compute_target,\n",
    "                            environment=keras_env)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the run configuration details into the PythonScriptStep."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = PythonScriptStep(name='train step',\n",
    "                              arguments=[prepared_fashion_ds.read_delimited_files().as_input(name='prepared_fashion_ds')],\n",
    "                              source_directory=train_src.source_directory,\n",
    "                              script_name=train_src.script,\n",
    "                              runconfig=train_src.run_config)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the pipeline\n",
    "Once we have the steps (or steps collection), we can build the [pipeline](https://docs.microsoft.com/python/api/azureml-pipeline-core/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py).\n",
    "\n",
    "A pipeline is created with a list of steps and a workspace. Submit a pipeline using `submit`. When submit is called, a [PipelineRun](https://docs.microsoft.com/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinerun?view=azure-ml-py) is created which in turn creates [StepRun](https://docs.microsoft.com/python/api/azureml-pipeline-core/azureml.pipeline.core.steprun?view=azure-ml-py) objects for each step in the workflow."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline & run experiment\n",
    "pipeline = Pipeline(workspace, steps=[prep_step, train_step])\n",
    "run = exp.submit(pipeline)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor the PipelineRun"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.find_step_run('train step')[0].get_metrics()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the input dataset and the output model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure Machine Learning dataset makes it easy to trace how your data is used in ML. [Learn More](https://docs.microsoft.com/azure/machine-learning/service/how-to-version-track-datasets#track-datasets-in-experiments)<br>\n",
    "For each Machine Learning experiment, you can easily trace the datasets used as the input through `Run` object."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get input datasets\n",
    "prep_step = run.find_step_run('prepare step')[0]\n",
    "inputs = prep_step.get_details()['inputDatasets']\n",
    "input_dataset = inputs[0]['dataset']\n",
    "\n",
    "# list the files referenced by input_dataset\n",
    "input_dataset.to_path()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the input Fashion MNIST dataset with the workspace so that you can reuse it in other experiments or share it with your colleagues who have access to your workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_ds = input_dataset.register(workspace = workspace,\n",
    "                                    name = 'fashion_ds',\n",
    "                                    description = 'image and label files from fashion mnist',\n",
    "                                    create_new_version = True)\n",
    "fashion_ds"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the output model with dataset"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.find_step_run('train step')[0].register_model(model_name = 'keras-model', model_path = 'outputs/model/', \n",
    "                                                  datasets =[('train test data',fashion_ds)])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-in-spark/train-in-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/training/train-in-spark/train-in-spark.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-in-spark/train-in-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Train in Spark\n",
    "* Create Workspace\n",
    "* Create Experiment\n",
    "* Copy relevant files to the script folder\n",
    "* Configure and Run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-in-spark/train-in-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration](../../../configuration.ipynb) Notebook first if you haven't already to establish your connection to the AzureML Workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-in-spark/train-in-spark.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-in-spark/train-in-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-in-spark/train-in-spark.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-in-spark/train-in-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experiment\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-in-spark/train-in-spark.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'train-on-spark'\n",
    "\n",
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-in-spark/train-in-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View `train-spark.py`\n",
    "\n",
    "For convenience, we created a training script for you. It is printed below as a text, but you can also run `%pfile ./train-spark.py` in a cell to show the file."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-in-spark/train-in-spark.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train-spark.py', 'r') as training_script:\n",
    "    print(training_script.read())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-in-spark/train-in-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure & Run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-in-spark/train-in-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** You can use Docker-based execution to run the Spark job in local computer or a remote VM. Please see the `train-in-remote-vm` notebook for example on how to configure and run in Docker mode in a VM. Make sure you choose a Docker image that has Spark installed, such as `microsoft/mmlspark:0.12`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-in-spark/train-in-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach an HDI cluster\n",
    "Here we will use a actual Spark cluster, HDInsight for Spark, to run this job. To use HDI commpute target:\n",
    " 1. Create a Spark for HDI cluster in Azure. Here are some [quick instructions](https://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-jupyter-spark-sql). Make sure you use the Ubuntu flavor, NOT CentOS.\n",
    " 2. Enter the IP address, username and password below"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-in-spark/train-in-spark.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "sample-hdinsightcompute-attach"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, HDInsightCompute\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "import os\n",
    "\n",
    "try:\n",
    "# If you want to connect using SSH key instead of username/password you can provide parameters private_key_file and private_key_passphrase\n",
    "\n",
    "# Attaching a HDInsight cluster using the public address of the HDInsight cluster is no longer supported.\n",
    "# Instead, use resourceId of the HDInsight cluster.\n",
    "# The resourceId of the HDInsight Cluster can be constructed using the following string format:\n",
    "# /subscriptions/<subscription_id>/resourceGroups/<resource_group>/providers/Microsoft.HDInsight/clusters/<cluster_name>.\n",
    "# You can also use subscription_id, resource_group and cluster_name without constructing resourceId.\n",
    "    attach_config = HDInsightCompute.attach_configuration(resource_id='<resource_id>',\n",
    "                                                          ssh_port=22,\n",
    "                                                          username=os.environ.get('hdiusername', '<ssh_username>'),\n",
    "                                                          password=os.environ.get('hdipassword', '<my_password>'))\n",
    "\n",
    "    hdi_compute = ComputeTarget.attach(workspace=ws, \n",
    "                                       name='myhdi', \n",
    "                                       attach_configuration=attach_config)\n",
    "\n",
    "except ComputeTargetException as e:\n",
    "    print(\"Caught = {}\".format(e.message))\n",
    "    \n",
    "        \n",
    "hdi_compute.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-in-spark/train-in-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure HDI run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-in-spark/train-in-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure an execution using the HDInsight cluster with a conda environment that has `numpy`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-in-spark/train-in-spark.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# use pyspark framework\n",
    "hdi_run_config = RunConfiguration(framework=\"pyspark\")\n",
    "\n",
    "# Set compute target to the HDI cluster\n",
    "hdi_run_config.target = hdi_compute.name\n",
    "\n",
    "# specify CondaDependencies object to ask system installing numpy\n",
    "cd = CondaDependencies()\n",
    "cd.add_conda_package('numpy')\n",
    "hdi_run_config.environment.python.conda_dependencies = cd"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-in-spark/train-in-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the script to HDI"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-in-spark/train-in-spark.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "script_run_config = ScriptRunConfig(source_directory = '.',\n",
    "                                    script= 'train-spark.py',\n",
    "                                    run_config = hdi_run_config)\n",
    "run = exp.submit(config=script_run_config)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-in-spark/train-in-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitor the run using a Juypter widget"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-in-spark/train-in-spark.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-in-spark/train-in-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you need to cancel a run, you can follow [these instructions](https://aka.ms/aml-docs-cancel-run)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-in-spark/train-in-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the run is succesfully finished, you can check the metrics logged."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-in-spark/train-in-spark.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all metris logged in the run\n",
    "metrics = run.get_metrics()\n",
    "print(metrics)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-in-spark/train-in-spark.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the generated model\n",
    "model = run.register_model(model_name='iris.model', model_path='outputs/iris.model')\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-in-spark/train-in-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Train using Azure Machine Learning Compute\n",
    "\n",
    "* Initialize a Workspace\n",
    "* Create an Experiment\n",
    "* Introduction to AmlCompute\n",
    "* Submit an AmlCompute run in a few different ways\n",
    "    - Provision as a persistent compute target (Basic)\n",
    "    - Provision as a persistent compute target (Advanced)\n",
    "* Additional operations to perform on AmlCompute\n",
    "* Find the best model in the run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set.  Otherwise, go through the [configuration](../../../configuration.ipynb) Notebook first if you haven't already to establish your connection to the AzureML Workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create workspace"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create An Experiment\n",
    "\n",
    "**Experiment** is a logical container in an Azure ML Workspace. It hosts run records which can include run metrics and output artifacts from your experiments."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "experiment_name = 'train-on-amlcompute'\n",
    "experiment = Experiment(workspace = ws, name = experiment_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to AmlCompute\n",
    "\n",
    "Azure Machine Learning Compute is managed compute infrastructure that allows the user to easily create single to multi-node compute of the appropriate VM Family. It is created **within your workspace region** and is a resource that can be used by other users in your workspace. It autoscales by default to the max_nodes, when a job is submitted, and executes in a containerized environment packaging the dependencies as specified by the user. \n",
    "\n",
    "Since it is managed compute, job scheduling and cluster management are handled internally by Azure Machine Learning service. \n",
    "\n",
    "For more information on Azure Machine Learning Compute, please read [this article](https://docs.microsoft.com/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute)\n",
    "\n",
    "If you are an existing BatchAI customer who is migrating to Azure Machine Learning, please read [this article](https://aka.ms/batchai-retirement)\n",
    "\n",
    "**Note**: As with other Azure services, there are limits on certain resources (for eg. AmlCompute quota) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota.\n",
    "\n",
    "\n",
    "The training script `train.py` is already created for you. Let's have a look."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit an AmlCompute run in a few different ways\n",
    "\n",
    "First lets check which VM families are available in your region. Azure is a regional service and some specialized SKUs (especially GPUs) are only available in certain regions. Since AmlCompute is created in the region of your workspace, we will use the supported_vms () function to see if the VM family we want to use ('STANDARD_D2_V2') is supported.\n",
    "\n",
    "You can also pass a different region to check availability and then re-create your workspace in that region through the [configuration notebook](../../../configuration.ipynb)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "\n",
    "AmlCompute.supported_vmsizes(workspace = ws)\n",
    "#AmlCompute.supported_vmsizes(workspace = ws, location='southcentralus')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create project directory\n",
    "\n",
    "Create a directory that will contain all the necessary code from your local machine that you will need access to on the remote resource. This includes the training script, and any additional files your training script depends on"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "project_folder = './train-on-amlcompute'\n",
    "os.makedirs(project_folder, exist_ok=True)\n",
    "shutil.copy('train.py', project_folder)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create environment\n",
    "\n",
    "Create Docker based environment with scikit-learn installed."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "myenv = Environment(\"myenv\")\n",
    "\n",
    "myenv.docker.enabled = True\n",
    "myenv.python.conda_dependencies = CondaDependencies.create(conda_packages=['scikit-learn', 'packaging'])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provision as a persistent compute target (Basic)\n",
    "\n",
    "You can provision a persistent AmlCompute resource by simply defining two parameters thanks to smart defaults. By default it autoscales from 0 nodes and provisions dedicated VMs to run your job in a container. This is useful when you want to continously re-use the same target, debug it between jobs or simply share the resource with other users of your workspace.\n",
    "\n",
    "* `vm_size`: VM family of the nodes provisioned by AmlCompute. Simply choose from the supported_vmsizes() above\n",
    "* `max_nodes`: Maximum nodes to autoscale to while running a job on AmlCompute"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "sample-amlcompute-provision"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"cpu-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=4)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "cpu_cluster.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure & Run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "src = ScriptRunConfig(source_directory=project_folder, \n",
    "                      script='train.py', \n",
    "                      compute_target=cpu_cluster, \n",
    "                      environment=myenv)\n",
    " \n",
    "run = experiment.submit(config=src)\n",
    "run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you need to cancel a run, you can follow [these instructions](https://aka.ms/aml-docs-cancel-run)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Shows output of the run on stdout.\n",
    "run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_metrics()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provision as a persistent compute target (Advanced)\n",
    "\n",
    "You can also specify additional properties or change defaults while provisioning AmlCompute using a more advanced configuration. This is useful when you want a dedicated cluster of 4 nodes (for example you can set the min_nodes and max_nodes to 4), or want the compute to be within an existing VNet in your subscription.\n",
    "\n",
    "In addition to `vm_size` and `max_nodes`, you can specify:\n",
    "* `min_nodes`: Minimum nodes (default 0 nodes) to downscale to while running a job on AmlCompute\n",
    "* `vm_priority`: Choose between 'dedicated' (default) and 'lowpriority' VMs when provisioning AmlCompute. Low Priority VMs use Azure's excess capacity and are thus cheaper but risk your run being pre-empted\n",
    "* `idle_seconds_before_scaledown`: Idle time (default 120 seconds) to wait after run completion before auto-scaling to min_nodes\n",
    "* `vnet_resourcegroup_name`: Resource group of the **existing** VNet within which AmlCompute should be provisioned\n",
    "* `vnet_name`: Name of VNet\n",
    "* `subnet_name`: Name of SubNet within the VNet\n",
    "* `admin_username`: Name of Admin user account which will be created on all the nodes of the cluster\n",
    "* `admin_user_password`: Password that you want to set for the user account above\n",
    "* `admin_user_ssh_key`: SSH Key for the user account above. You can specify either a password or an SSH key or both\n",
    "* `remote_login_port_public_access`: Flag to enable or disable the public SSH port. If you dont specify, AmlCompute will smartly close the port when deploying inside a VNet\n",
    "* `identity_type`: Compute Identity type that you want to set on the cluster, which can either be SystemAssigned or UserAssigned\n",
    "* `identity_id`: Resource ID of identity in case it is a UserAssigned identity, optional otherwise\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"cpu-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           vm_priority='lowpriority',\n",
    "                                                           min_nodes=2,\n",
    "                                                           max_nodes=4,\n",
    "                                                           idle_seconds_before_scaledown='300',\n",
    "                                                           vnet_resourcegroup_name='<my-resource-group>',\n",
    "                                                           vnet_name='<my-vnet-name>',\n",
    "                                                           subnet_name='<my-subnet-name>',\n",
    "                                                           admin_username='<my-username>',\n",
    "                                                           admin_user_password='<my-password>',\n",
    "                                                           admin_user_ssh_key='<my-sshkey>',\n",
    "                                                           remote_login_port_public_access='enabled',\n",
    "                                                           identity_type='UserAssigned',\n",
    "                                                           identity_id=['<resource-id1>'])\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "cpu_cluster.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure & Run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set compute target to the one created in previous step\n",
    "src.run_config.target = cpu_cluster\n",
    " \n",
    "run = experiment.submit(config=src)\n",
    "run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Shows output of the run on stdout.\n",
    "run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_metrics()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional operations to perform on AmlCompute\n",
    "\n",
    "You can perform more operations on AmlCompute such as updating the node counts or deleting the compute. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_status () gets the latest status of the AmlCompute target\n",
    "cpu_cluster.get_status().serialize()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_nodes () gets the list of nodes on the cluster with status, IP and associated run\n",
    "cpu_cluster.list_nodes()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update () takes in the min_nodes, max_nodes and idle_seconds_before_scaledown and updates the AmlCompute target\n",
    "#cpu_cluster.update(min_nodes=1)\n",
    "#cpu_cluster.update(max_nodes=10)\n",
    "cpu_cluster.update(idle_seconds_before_scaledown=300)\n",
    "#cpu_cluster.update(min_nodes=2, max_nodes=4, idle_seconds_before_scaledown=600)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete () is used to deprovision and delete the AmlCompute target. Useful if you want to re-use the compute name \n",
    "#'cpu-cluster' in this case but use a different VM family for instance.\n",
    "\n",
    "#cpu_cluster.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success!\n",
    "Great, you are ready to move on to the remaining notebooks."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/training/using-environments/using-environments.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using environments\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Use curated environment](#Use-curated-environment)\n",
    "1. [Create environment](#Create-environment)\n",
    "    1. Add Python packages\n",
    "    1. Specify environment variables\n",
    "1. [Submit run using environment](#Submit-run-using-environment)\n",
    "1. [Register environment](#Register-environment)\n",
    "1. [List and get existing environments](#List-and-get-existing-environments)\n",
    "1. [Other ways to create environments](#Other-ways-to-create-environments)\n",
    "    1. From existing Conda environment\n",
    "    1. From Conda or pip files\n",
    "1. [Using environments for inferencing](#Using-environments-for-inferencing)\n",
    "1. [Docker settings](#Docker-settings)\n",
    "1. [Spark and Azure Databricks settings](#Spark-and-Azure-Databricks-settings)\n",
    "1. [Next steps](#Next-steps)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Azure ML environments are an encapsulation of the environment where your machine learning training happens. They define Python packages, environment variables, Docker settings and other attributes in declarative fashion. Environments are versioned: you can update them and retrieve old versions to revisit and review your work.\n",
    "\n",
    "Environments allow you to:\n",
    "* Encapsulate dependencies of your training process, such as Python packages and their versions.\n",
    "* Reproduce the Python environment on your local computer in a remote run on VM or ML Compute cluster\n",
    "* Reproduce your experimentation environment in production setting.\n",
    "* Revisit and audit the environment in which an existing model was trained.\n",
    "\n",
    "Environment, compute target and training script together form run configuration: the full specification of training run.\n",
    "\n",
    "## Setup\n",
    "\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the [configuration notebook](../../../configuration.ipynb) first if you haven't.\n",
    "\n",
    "First, let's validate Azure ML SDK version and connect to workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "print(azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "ws = Workspace.from_config()\n",
    "ws.get_details()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use curated environments\n",
    "\n",
    "Curated environments are provided by Azure Machine Learning and are available in your workspace by default. They contain collections of Python packages and settings to help you get started different machine learning frameworks. \n",
    "\n",
    "  * The __AzureML-Minimal__ environment contains a minimal set of packages to enable run tracking and asset uploading. You can use it as a starting point for your own environment.\n",
    "  * The __AzureML-Tutorial__ environment contains common data science packages, such as Scikit-Learn, Pandas and Matplotlib, and larger set of azureml-sdk packages.\n",
    " \n",
    "Curated environments are backed by cached Docker images, reducing the run preparation cost.\n",
    " \n",
    "You can get a curated environment using"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "curated_env = Environment.get(workspace=ws, name=\"AzureML-Minimal\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To list curated environments, use following code.\n",
    "\n",
    "**Note**: The name prefixes _AzureML_ and _Microsoft_ are reserved for curated environments. Do not use them for your own environments"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = Environment.list(workspace=ws)\n",
    "\n",
    "for env in envs:\n",
    "    if env.startswith(\"AzureML\"):\n",
    "        print(\"Name\",env)\n",
    "        print(\"packages\", envs[env].python.conda_dependencies.serialize_to_string())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your own environment\n",
    "\n",
    "You can create an environment by instantiating ```Environment``` object and then setting its attributes: set of Python packages, environment variables and others.\n",
    "\n",
    "### Add Python packages\n",
    "\n",
    "The recommended way is to specify Conda packages, as they typically come with complete set of pre-built binaries."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "condadependencies-remarks-sample"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.environment import CondaDependencies\n",
    "\n",
    "myenv = Environment(name=\"myenv\")\n",
    "conda_dep = CondaDependencies()\n",
    "conda_dep.add_conda_package(\"scikit-learn\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add pip packages, and specify the version of package"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "condadependencies-remarks-sample2"
    ]
   },
   "outputs": [],
   "source": [
    "conda_dep.add_pip_package(\"pillow==5.4.1\")\n",
    "myenv.python.conda_dependencies=conda_dep"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify environment variables\n",
    "\n",
    "You can add environment variables to your environment. These then become available using ```os.environ.get``` in your training script."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myenv.environment_variables = {\"MESSAGE\":\"Hello from Azure Machine Learning\"}"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit run using environment\n",
    "\n",
    "When you submit a run, you can specify which environment to use. \n",
    "\n",
    "On the first run in given environment, Azure ML spends some time building the environment. On the subsequent runs, Azure ML keeps track of changes and uses the existing environment, resulting in faster run completion."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig, Experiment\n",
    "\n",
    "myexp = Experiment(workspace=ws, name = \"environment-example\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit a run, create a run configuration that combines the script file and environment, and pass it to ```Experiment.submit```. In this example, the script is submitted to local computer, but you can specify other compute targets such as remote clusters as well."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = ScriptRunConfig(source_directory=\".\",\n",
    "                      script=\"example.py\",\n",
    "                      compute_target=\"local\",\n",
    "                      environment=myenv)\n",
    "\n",
    "run = myexp.submit(config=src)\n",
    "\n",
    "run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To audit the environment used by for a run, you can use ```get_environment```."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_environment()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register environment\n",
    "\n",
    "You can manage environments by registering them. This allows you to track their versions, and reuse them in future runs. For example, once you've constructed an environment that meets your requirements, you can register it and use it in other experiments so as to standardize your workflow.\n",
    "\n",
    "If you register the environment with same name, the version number is increased by one. Note that Azure ML keeps track of differences between the version, so if you re-register an identical version, the version number is not increased."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myenv.register(workspace=ws)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List and get existing environments\n",
    "\n",
    "Your workspace contains a dictionary of registered environments. You can then use ```Environment.get``` to retrieve a specific environment with specific version."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,env in ws.environments.items():\n",
    "    print(\"Name {} \\t version {}\".format(name,env.version))\n",
    "\n",
    "restored_environment = Environment.get(workspace=ws,name=\"myenv\",version=\"1\")\n",
    "\n",
    "print(\"Attributes of restored environment\")\n",
    "restored_environment"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other ways to create environments\n",
    "\n",
    "### From existing Conda environment\n",
    "\n",
    "You can create an environment from existing conda environment. This make it easy to reuse your local interactive environment in Azure ML remote runs. For example, if you've created conda environment using\n",
    "```\n",
    "conda create -n mycondaenv\n",
    "```\n",
    "you can create Azure ML environment out of that conda environment using\n",
    "```\n",
    "myenv = Environment.from_existing_conda_environment(name=\"myenv\",conda_environment_name=\"mycondaenv\")\n",
    "```\n",
    "\n",
    "### From conda or pip files\n",
    "\n",
    "You can create environments from conda specification or pip requirements files using\n",
    "```\n",
    "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"path-to-conda-specification-file\")\n",
    "\n",
    "myenv = Environment.from_pip_requirements(name=\"myenv\", file_path=\"path-to-pip-requirements-file\")\n",
    "```\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using environments for inferencing\n",
    "\n",
    "You can re-use the training environment when you deploy your model as a web service, by specifying inferencing stack version, and adding then environment to ```InferenceConfig```.\n",
    "\n",
    "```\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "myenv.inferencing_stack_version = \"latest\"\n",
    "\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\n",
    "```\n",
    "\n",
    "See [Register Model and deploy as Webservice Notebook](../../deployment/deploy-to-cloud/model-register-and-deploy.ipynb) for an end-to-end example of web service deployment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker settings\n",
    "\n",
    "Docker container provides an efficient way to encapsulate the dependencies. When you enable Docker, Azure ML builds a Docker image and creates a Python environment within that container, given your specifications. The Docker images are reused: the first run in a new environment typically takes longer as the image is build.\n",
    "\n",
    "**Note:** For runs on local computer or attached virtual machine, that computer must have Docker installed and enabled. Machine Learning Compute has Docker pre-installed.\n",
    "\n",
    "Attribute ```docker.enabled``` controls whether to use Docker container or host OS for execution. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myenv.docker.enabled = True"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can specify custom Docker base image and registry. This allows you to customize and control in detail the guest OS in which your training run executes. whether to use GPU, whether to use shared volumes, and shm size."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myenv.docker.base_image\n",
    "myenv.docker.base_image_registry"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify shared volumes, and shm size."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myenv.docker.shared_volumes\n",
    "myenv.docker.shm_size"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark and Azure Databricks settings\n",
    "\n",
    "In addition to Python and Docker settings, Environment also contains attributes for Spark and Azure Databricks runs. These attributes become relevant when you submit runs on those compute targets."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Train with ML frameworks on Azure ML:\n",
    "\n",
    "* [Train with ML frameworks](../../ml-frameworks)\n",
    "\n",
    "Learn more about registering and deploying a model:\n",
    "\n",
    "* [Register Model and deploy as Webservice](../../deployment/deploy-to-cloud/model-register-and-deploy.ipynb)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/using-environments/using-environments.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Train in a remote Linux VM\n",
    "* Create Workspace\n",
    "* Create `train.py` file\n",
    "* Create and Attach a Remote VM (eg. DSVM) as compute resource\n",
    "* Upload data files into default datastore\n",
    "* Configure & execute a run in a few different ways\n",
    "    - Use system-built conda\n",
    "    - Use existing Python environment\n",
    "    - Use Docker \n",
    "* Find the best model in the run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration notebook](../../../configuration.ipynb) first if you haven't already to establish your connection to the AzureML Workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experiment\n",
    "\n",
    "**Experiment** is a logical container in an Azure ML Workspace. It hosts run records which can include run metrics and output artifacts from your experiments."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'train-on-remote-vm'\n",
    "\n",
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create a local folder to hold the training script."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = './vm-run'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data files into datastore\n",
    "Every workspace comes with a default [datastore](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-access-data) (and you can register more) which is backed by the Azure blob storage account associated with the workspace. We can use it to transfer data from local to the cloud, and access it from the compute target."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default datastore\n",
    "ds = ws.get_default_datastore()\n",
    "print(ds.name, ds.datastore_type, ds.account_name, ds.container_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load diabetes data from `scikit-learn` and save it as 2 local files."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "import numpy as np\n",
    "\n",
    "training_data = load_diabetes()\n",
    "np.save(file='./features.npy', arr=training_data['data'])\n",
    "np.save(file='./labels.npy', arr=training_data['target'])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's upload the 2 files into the default datastore under a path named `diabetes`:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.upload_files(['./features.npy', './labels.npy'], target_path='diabetes', overwrite=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dataset for Files\n",
    "A Dataset can reference single or multiple files in your datastores or public urls. The files can be of any format. Dataset provides you with the ability to download or mount the files to your compute. By creating a dataset, you create a reference to the data source location. The data remains in its existing location, so no extra storage cost is incurred. [Learn More](https://aka.ms/azureml/howto/createdatasets)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize file dataset \n",
    "from azureml.core import Dataset\n",
    "ds_paths = [(ds, 'diabetes/')]\n",
    "dataset = Dataset.File.from_files(path = ds_paths)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the files referenced by the dataset\n",
    "dataset.to_path()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View `train.py`\n",
    "\n",
    "For convenience, we created a training script for you. It is printed below as a text, but you can also run `%pfile ./train.py` in a cell to show the file. Please pay special attention on how we are loading the features and labels from files in the `data_folder` path, which is passed in as an argument of the training script (shown later)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy train.py into the script folder\n",
    "import shutil\n",
    "shutil.copy('./train.py', os.path.join(script_folder, 'train.py'))\n",
    "\n",
    "with open(os.path.join(script_folder, './train.py'), 'r') as training_script:\n",
    "    print(training_script.read())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Attach a DSVM as a compute target\n",
    "\n",
    "The DSVM can be created using the below single line command and then attached(like any VM) using the sample code below. Also note, that we only support Linux VMs for remote execution from AML and the commands below will spin a Linux VM only.\n",
    "\n",
    "```shell\n",
    "# create a DSVM in your resource group\n",
    "# note you need to be at least a contributor to the resource group in order to execute this command successfully\n",
    "(myenv) $ az vm create --resource-group <resource_group_name> --name <some_vm_name> --image microsoft-dsvm:linux-data-science-vm-ubuntu:linuxdsvmubuntu:latest --admin-username <username> --admin-password <password> --generate-ssh-keys --authentication-type password\n",
    "```\n",
    "\n",
    "**Note**: You can also use [this url](https://portal.azure.com/#create/microsoft-dsvm.linux-data-science-vm-ubuntulinuxdsvmubuntu) to create the VM using the Azure Portal\n",
    "\n",
    "**Note**: By default SSH runs on port 22 and you don't need to specify it. But if for security reasons you switch to a different port (such as 5022), you can specify the port number in the provisioning configuration object."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "sample-remotecompute-attach"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, RemoteCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "username = os.getenv('AZUREML_DSVM_USERNAME', default='<my_username>')\n",
    "address = os.getenv('AZUREML_DSVM_ADDRESS', default='<ip_address_or_fqdn>')\n",
    "\n",
    "compute_target_name = 'cpudsvm'\n",
    "# if you want to connect using SSH key instead of username/password you can provide parameters private_key_file and private_key_passphrase \n",
    "try:\n",
    "    attached_dsvm_compute = RemoteCompute(workspace=ws, name=compute_target_name)\n",
    "    print('found existing:', attached_dsvm_compute.name)\n",
    "except ComputeTargetException:\n",
    "    attach_config = RemoteCompute.attach_configuration(address=address,\n",
    "                                                       ssh_port=22,\n",
    "                                                       username=username,\n",
    "                                                       private_key_file='./.ssh/id_rsa')\n",
    "\n",
    "\n",
    "# Attaching a virtual machine using the public IP address of the VM is no longer supported.\n",
    "# Instead, use resourceId of the VM.\n",
    "# The resourceId of the VM can be constructed using the following string format:\n",
    "# /subscriptions/<subscription_id>/resourceGroups/<resource_group>/providers/Microsoft.Compute/virtualMachines/<vm_name>.\n",
    "# You can also use subscription_id, resource_group and vm_name without constructing resourceId.\n",
    "    attach_config = RemoteCompute.attach_configuration(resource_id='<resource_id>',\n",
    "                                                       ssh_port=22,\n",
    "                                                       username='username',\n",
    "                                                       private_key_file='./.ssh/id_rsa')\n",
    "    attached_dsvm_compute.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure & Run\n",
    "\n",
    "Now we can try a few different ways to run the training script in the VM."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conda run\n",
    "You can ask the system to build a conda environment based on your dependency specification, and submit your script to run there. Once the environment is built, and if you don't change your dependencies, it will be reused in subsequent runs."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "conda_env = Environment(\"conda-env\")\n",
    "conda_env.python.conda_dependencies = CondaDependencies.create(pip_packages=['scikit-learn',\n",
    "                                                                             'azureml-sdk',\n",
    "                                                                             'azureml-dataset-runtime[pandas,fuse]'])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "from uuid import uuid4\n",
    "\n",
    "script_arguments = ['--data-folder', dataset.as_named_input('diabetes').as_mount('/tmp/{}'.format(uuid4()))]\n",
    "src = ScriptRunConfig(source_directory=script_folder, \n",
    "                      script='train.py', \n",
    "                      # pass the dataset as a parameter to the training script\n",
    "                      arguments=script_arguments,\n",
    "                      compute_target = attached_dsvm_compute,\n",
    "                      environment = conda_env) "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(config=src)\n",
    "\n",
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()\n",
    "\n",
    "run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you need to cancel a run, you can follow [these instructions](https://aka.ms/aml-docs-cancel-run)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the run object. You can navigate to the Azure portal to see detailed information about the run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native VM run\n",
    "You can also configure to use an exiting Python environment in the VM to execute the script without asking the system to create a conda environment for you."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda_env.python.user_managed_dependencies = True"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below run will likely fail because `train.py` needs dependency `azureml`, `scikit-learn` and others, which are not found in that Python environment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " run = exp.submit(config=src)\n",
    "\n",
    " from azureml.exceptions import ActivityFailedException\n",
    "\n",
    " try:\n",
    "     run.wait_for_completion(show_output=True)\n",
    " except ActivityFailedException as ex:\n",
    "     print(ex)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can choose to SSH into the VM and install Azure ML SDK, and any other missing dependencies, in that Python environment. For demonstration purposes, we simply are going to use another script `train2.py` that doesn't have azureml or data store dependencies, and submit it instead."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy train2.py into the script folder\n",
    "shutil.copy('./train2.py', os.path.join(script_folder, 'train2.py'))\n",
    "\n",
    "with open(os.path.join(script_folder, './train2.py'), 'r') as training_script:\n",
    "    print(training_script.read())\n",
    "\n",
    "src = ScriptRunConfig(source_directory=script_folder, \n",
    "                      script='train2.py', \n",
    "                      compute_target = attached_dsvm_compute,\n",
    "                      environment = conda_env) "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try again. And this time it should work fine."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(config=src)\n",
    "\n",
    "run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note even in this case you get a run record with some basic statistics."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure a Docker run with new conda environment on the VM\n",
    "You can execute in a Docker container in the VM. If you choose this option, the system will pull down a base Docker image, build a new conda environment in it if you ask for (you can also skip this if you are using a customer Docker image when a preconfigured Python environment), start a container, and run your script in there. This image is also uploaded into your ACR (Azure Container Registry) assoicated with your workspace, an reused if your dependencies don't change in the subsequent runs."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda_env.docker.enabled = True\n",
    "conda_env.python.user_managed_dependencies = False\n",
    "\n",
    "print('Base Docker image is:', conda_env.docker.base_image)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the Experiment\n",
    "Submit script to run in the Docker image in the remote VM. If you run this for the first time, the system will download the base image, layer in packages specified in the `conda_dependencies.yml` file on top of the base image, create a container and then execute the script in the container."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = ScriptRunConfig(source_directory=script_folder, \n",
    "                      script='train.py', \n",
    "                      arguments=script_arguments,\n",
    "                      compute_target = attached_dsvm_compute,\n",
    "                      environment = conda_env) \n",
    "\n",
    "run = exp.submit(config=src)\n",
    "\n",
    "run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a custom Docker image instead\n",
    "\n",
    "You can also specify a custom Docker image if you don't want to use the default image provided by Azure ML.\n",
    "\n",
    "```python\n",
    "# use an image available in Docker Hub without authentication\n",
    "conda_env.docker.base_image = \"continuumio/miniconda3\"\n",
    "\n",
    "# or, use an image available in a private Azure Container Registry\n",
    "conda_env.docker.base_image = \"mycustomimage:1.0\"\n",
    "conda_env.docker.base_image_registry.address = \"myregistry.azurecr.io\"\n",
    "conda_env.docker.base_image_registry.username = \"username\"\n",
    "conda_env.docker.base_image_registry.password = \"password\"\n",
    "```\n",
    "\n",
    "When you are using a custom Docker image, you might already have your environment setup properly in a Python environment in the Docker image. In that case, you can skip specifying conda dependencies, and just use `user_managed_dependencies` option instead:\n",
    "```python\n",
    "conda_env.python.user_managed_dependencies = True\n",
    "# path to the Python environment in the custom Docker image\n",
    "conda_env.python.interpreter_path = '/opt/conda/bin/python'\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View run history details"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have tried various execution modes, we can find the best model from the last run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all metris logged in the run\n",
    "run.get_metrics()\n",
    "metrics = run.get_metrics()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the index where MSE is the smallest\n",
    "indices = list(range(0, len(metrics['mse'])))\n",
    "min_mse_index = min(indices, key=lambda x: metrics['mse'][x])\n",
    "\n",
    "print('When alpha is {1:0.2f}, we have min MSE {0:0.2f}.'.format(\n",
    "    metrics['mse'][min_mse_index], \n",
    "    metrics['alpha'][min_mse_index]\n",
    "))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up compute resource\n",
    "\n",
    "Use ```detach()``` to detach an existing DSVM from Workspace without deleting it. Use ```delete()``` if you created a new ```DsvmCompute``` and want to delete it."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dsvm_compute.detach()\n",
    "# dsvm_compute.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/training/train-on-local/train-on-local.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Train locally\n",
    "_**Train a model locally: Directly on your machine and within a Docker container**_\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Table of contents\n",
    "1. [Introduction](#intro)\n",
    "1. [Pre-requisites](#pre-reqs)\n",
    "1. [Initialize Workspace](#init)\n",
    "1. [Create An Experiment](#exp)\n",
    "1. [View training and auxiliary scripts](#view)\n",
    "1. [Configure & Run](#config-run)\n",
    "    1. User-managed environment\n",
    "        1. Set the environment up\n",
    "        1. Submit the script to run in the user-managed environment\n",
    "        1. Get run history details\n",
    "    1. System-managed environment\n",
    "        1. Set the environment up\n",
    "        1. Submit the script to run in the system-managed environment\n",
    "        1. Get run history details\n",
    "    1. Docker-based execution\n",
    "        1. Set the environment up\n",
    "        1. Submit the script to run in the system-managed environment\n",
    "        1. Get run history details\n",
    "        1. Use a custom Docker image\n",
    "1. [Query run metrics](#query)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction <a id='intro'></a>\n",
    "\n",
    "In this notebook, we will learn how to:\n",
    "\n",
    "* Connect to our AML workspace\n",
    "* Create or load a workspace\n",
    "* Configure & execute a local run in:\n",
    "    - a user-managed Python environment\n",
    "    - a system-managed Python environment\n",
    "    - a Docker environment\n",
    "* Query run metrics to find the best model trained in the run\n",
    "* Register that model for operationalization"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-requisites <a id='pre-reqs'></a>\n",
    "In this notebook, we assume that you have set your Azure Machine Learning workspace. If you have not, make sure you go through the [configuration notebook](../../../configuration.ipynb) first. In the end, you should have configuration file that contains the subscription ID, resource group and name of your workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Workspace <a id='init'></a>\n",
    "\n",
    "Initialize your workspace object from configuration file"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create An Experiment <a id='exp'></a>\n",
    "An experiment is a logical container in an Azure ML Workspace. It contains a series of trials called `Runs`. As such, it hosts run records such as run metrics, logs, and other output artifacts from your experiments."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "experiment_name = 'train-on-local'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. View training and auxiliary scripts <a id='view'></a>\n",
    "\n",
    "For convenience, we already created the training (`train.py`) script and supportive libraries (`mylib.py`) for you. Take a few minutes to examine both files."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./train.py', 'r') as f:\n",
    "    print(f.read())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./mylib.py', 'r') as f:\n",
    "    print(f.read())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configure & Run <a id='config-run'></a>\n",
    "### 6.A User-managed environment\n",
    "\n",
    "#### 6.A.a Set the environment up\n",
    "When using a user-managed environment, you are responsible for ensuring that all the necessary packages are available in the Python environment you choose to run the script in."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "msdoc": "how-to-track-experiments.md",
    "name": "user_managed_env"
   },
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "# Editing a run configuration property on-fly.\n",
    "user_managed_env = Environment(\"user-managed-env\")\n",
    "\n",
    "user_managed_env.python.user_managed_dependencies = True\n",
    "\n",
    "# You can choose a specific Python environment by pointing to a Python path \n",
    "#user_managed_env.python.interpreter_path = '/home/johndoe/miniconda3/envs/myenv/bin/python'"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.A.b Submit the script to run in the user-managed environment\n",
    "Whatever the way you manage your environment, you need to use the `ScriptRunConfig` class. It allows you to further configure your run by pointing to the `train.py` script and to the working directory, which also contains the `mylib.py` file. These inputs indeed provide the commands to execute in the run. Once the run is configured, you submit it to your experiment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "msdoc": "how-to-track-experiments.md",
    "name": "src"
   },
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "src = ScriptRunConfig(source_directory='./', script='train.py', environment=user_managed_env)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "msdoc": "how-to-track-experiments.md",
    "name": "run"
   },
   "outputs": [],
   "source": [
    "run = exp.submit(src)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.A.c Get run history details\n",
    "\n",
    "While all calculations were run on your machine (cf. below), by using a `run` you also captured the results of your calculations into your run and experiment. You can then see them on the Azure portal, through the link displayed as output of the following cell.\n",
    "\n",
    "**Note**: The recording of the computation results into your run was made possible by the `run.log()` commands in the `train.py` file."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you need to cancel a run, you can follow [these instructions](https://aka.ms/aml-docs-cancel-run)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Block any execution to wait until the run finishes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** All these calculations were run on your local machine, in the conda environment you defined above. You can find the results in:\n",
    "- `~/.azureml/envs/azureml_xxxx` for the conda environment you just created\n",
    "- `~/AppData/Local/Temp/azureml_runs/train-on-local_xxxx` for the machine learning models you trained (this path may differ depending on the platform you use). This folder also contains\n",
    "  - Logs (under azureml_logs/)\n",
    "  - Output pickled files (under outputs/)\n",
    "  - The configuration files (credentials, local and docker image setups)\n",
    "  - The train.py and mylib.py scripts\n",
    "  - The current notebook\n",
    "\n",
    "Take a few minutes to examine the output of the cell above. It shows the content of some of the log files, and extra information on the conda environment used."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.B System-managed environment\n",
    "#### 6.B.a Set the environment up\n",
    "Now, instead of managing the setup of the environment yourself, you can ask the system to build a new conda environment for you. The environment is built once, and will be reused in subsequent executions as long as the conda dependencies remain unchanged."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "system_managed_env = Environment(\"system-managed-env\")\n",
    "\n",
    "system_managed_env.python.user_managed_dependencies = False\n",
    "\n",
    "# Specify conda dependencies with scikit-learn\n",
    "cd = CondaDependencies.create(conda_packages=['scikit-learn'])\n",
    "system_managed_env.python.conda_dependencies = cd"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.B.b Submit the script to run in the system-managed environment\n",
    "A new conda environment is built based on the conda dependencies object. If you are running this for the first time, this might take up to 5 minutes.\n",
    "\n",
    "The commands used to execute the run are then the same as the ones you used above."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src.run_config.environment = system_managed_env\n",
    "run = exp.submit(src)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.B.c Get run history details"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output = True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.C Docker-based execution\n",
    "In this section, you will train the same models, but you will do so in a Docker container, on your local machine. For this, you then need to have the Docker engine installed locally. If you don't have it yet, please follow the instructions below.\n",
    "\n",
    "#### How to install Docker\n",
    "\n",
    "- [Linux](https://docs.docker.com/install/linux/docker-ce/ubuntu/)\n",
    "- [MacOs](https://docs.docker.com/docker-for-mac/install/)\n",
    "- [Windows](https://docs.docker.com/docker-for-windows/install/)\n",
    "\n",
    "    In case of issues, troubleshooting documentation can be found [here](https://docs.docker.com/docker-for-windows/troubleshoot/#running-docker-for-windows-in-nested-virtualization-scenarios). Additionally, you can follow the steps below, if Virtualization is not enabled on your machine:\n",
    "    - Go to Task Manager > Performance\n",
    "    - Check that Virtualization is enabled\n",
    "    - If it is not, go to `Start > Settings > Update and security > Recovery > Advanced Startup - Restart now > Troubleshoot > Advanced options > UEFI firmware settings - restart`\n",
    "    - In the BIOS, go to `Advanced > System options > Click the \"Virtualization Technology (VTx)\" only > Save > Exit > Save all changes` -- This will restart the machine\n",
    "\n",
    "**Notes**: \n",
    "- If your kernel is already running in a Docker container, such as **Azure Notebooks**, this mode will **NOT** work.\n",
    "- If you use a GPU base image, it needs to be used on Microsoft Azure Services such as ACI, AML Compute, Azure VMs, or AKS.\n",
    "\n",
    "You can also ask the system to pull down a Docker image and execute your scripts in it.\n",
    "\n",
    "#### 6.C.a Set the environment up\n",
    "\n",
    "In the cell below, you will configure your run to execute in a Docker container. It will:\n",
    "- run on a CPU\n",
    "- contain a conda environment in which the scikit-learn library will be installed.\n",
    "\n",
    "As before, you will finish configuring your run by pointing to the `train.py` and `mylib.py` files."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_env = Environment(\"docker-env\")\n",
    "\n",
    "docker_env.python.user_managed_dependencies = False\n",
    "docker_env.docker.enabled = True\n",
    "\n",
    "# use the default CPU-based Docker image from Azure ML\n",
    "print(docker_env.docker.base_image)\n",
    "\n",
    "# Specify conda dependencies with scikit-learn\n",
    "docker_env.python.conda_dependencies = cd"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  6.C.b Submit the script to run in the system-managed environment\n",
    "\n",
    "The run is now configured and ready to be executed in a Docker container. If you are running this for the first time, the Docker container will get created, as well as the conda environment inside it. This will take several minutes. Once all this is generated, however, this conda environment will be reused as long as you don't change the conda dependencies."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "src.run_config.environment = docker_env\n",
    "\n",
    "# Check if Docker is installed and Linux containers are enabled\n",
    "if subprocess.run(\"docker -v\", shell=True).returncode == 0:\n",
    "    out = subprocess.check_output(\"docker system info\", shell=True).decode('ascii')\n",
    "    if not \"OSType: linux\" in out:\n",
    "        print(\"Switch Docker engine to use Linux containers.\")\n",
    "    else:\n",
    "        run = exp.submit(src)\n",
    "else:\n",
    "    print(\"Docker engine is not installed.\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Potential issue on Windows and how to solve it\n",
    "\n",
    "If you are using a Windows machine, the creation of the Docker image may fail, and you may see the following error message\n",
    "`docker: Error response from daemon: Drive has not been shared. Failed to launch docker container. Check that docker is running and that C:\\ on Windows and /tmp elsewhere is shared.`\n",
    "\n",
    "This is because the process above tries to create a linux-based, i.e. non-windows-based, Docker image. To fix this, you can:\n",
    "- Open the Docker user interface\n",
    "- Navigate to Settings > Shared drives\n",
    "- Select C (or both C and D, if you have one)\n",
    "- Apply\n",
    "\n",
    "When this is done, you can try and re-run the command above.\n",
    "\n",
    "<img src=\"./docker_settings.png\" width=\"500\" align=\"left\">"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.C.c Get run history details"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get run history details\n",
    "run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results obtained here should be the same as those obtained before. However, take a look at the \"Execution summary\" section in the output of the cell above. Look for \"docker\". There, you should see the \"enabled\" field set to True. Compare this to the 2 prior runs (\"enabled\" was then set to False)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.C.d Use a custom Docker image\n",
    "\n",
    "You can also specify a custom Docker image, if you don't want to use the default image provided by Azure ML.\n",
    "```python\n",
    "custom_docker_env = Environment(\"custom-docker-env\")\n",
    "custom_docker_env.docker.enabled = True\n",
    "```\n",
    "\n",
    "You can either pull an image directly from Anaconda:\n",
    "```python\n",
    "# Use an image available in Docker Hub without authentication\n",
    "custom_docker_env.docker.base_image = \"continuumio/miniconda3\"\n",
    "```\n",
    "\n",
    "Or one of the images you may already have created:\n",
    "```python\n",
    "# or, use an image available in your private Azure Container Registry\n",
    "custom_docker_env.docker.base_image = \"mycustomimage:1.0\"\n",
    "custom_docker_env.docker.base_image_registry.address = \"myregistry.azurecr.io\"\n",
    "custom_docker_env.docker.base_image_registry.username = \"username\"\n",
    "custom_docker_env.docker.base_image_registry.password = \"password\"\n",
    "```\n",
    "\n",
    "##### Where to find my Docker image name and registry credentials\n",
    "    If you do not know what the name of your Docker image or container registry is, or if you don't know how to access the username and password needed above, proceed as follows:\n",
    "    - Docker image name:\n",
    "        - In the portal, under your resource group, click on your current workspace\n",
    "        - Click on Experiments\n",
    "        - Click on Images\n",
    "        - Click on the image of your choice\n",
    "        - Copy the \"ID\" string\n",
    "        - In this notebook, replace \"mycustomimage:1/0\" with that ID string\n",
    "    - Username and password:\n",
    "        - In the portal, under your resource group, click on the container registry associated with your workspace\n",
    "            - If you have several and don't know which one you need, click on your workspace, go to Overview and click on the \"Registry\" name on the upper right of the screen\n",
    "        - There, go to \"Access keys\"\n",
    "        - Copy the username and one of the passwords\n",
    "        - In this notebook, replace \"username\" and \"password\" by these values\n",
    "\n",
    "In any case, you will need to use the lines above in place of the line marked as `# Reference Docker image` in section 6.C.a. \n",
    "\n",
    "When you are using your custom Docker image, you might already have your Python environment properly set up. In that case, you can skip specifying conda dependencies, and just use the `user_managed_dependencies` option instead:\n",
    "```python\n",
    "custom_docker_env.python.user_managed_dependencies = True\n",
    "# path to the Python environment in the custom Docker image\n",
    "custom_docker_env.python.interpreter_path = '/opt/conda/bin/python'\n",
    "```\n",
    "\n",
    "Once you are done defining your environment, set that environment on your run configuration:\n",
    "```python\n",
    "src.run_config.environment = custom_docker_env\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Query run metrics <a id='query'></a>\n",
    "\n",
    "Once your run has completed, you can now extract the metrics you captured by using the `get_metrics` method. As shown in the `train.py` file, these metrics are \"alpha\" and \"mse\"."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "query history",
     "get metrics"
    ]
   },
   "outputs": [],
   "source": [
    "# Get all metris logged in the run\n",
    "run.get_metrics()\n",
    "metrics = run.get_metrics()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the model that has the lowest MSE value logged."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "best_alpha = metrics['alpha'][np.argmin(metrics['mse'])]\n",
    "\n",
    "print('When alpha is {1:0.2f}, we have min MSE {0:0.2f}.'.format(\n",
    "    min(metrics['mse']), \n",
    "    best_alpha\n",
    "))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare it to the others"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(metrics['alpha'], metrics['mse'], marker='o')\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Alpha\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also list all the files that are associated with this run record"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_file_names()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results obtained above, `ridge_0.40.pkl` is the best performing model. You can now register that particular model with the workspace. Once you have done so, go back to the portal and click on \"Models\". You should see it there."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supply a model name, and the full path to the serialized model file.\n",
    "model = run.register_model(model_name='best_ridge_model', model_path='./outputs/ridge_0.40.pkl')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Registered model:\\n --> Name: {}\\n --> Version: {}\\n --> URL: {}\".format(model.name, model.version, model.url))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now deploy your model by following [this example](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and explain models remotely via Azure Machine Learning Compute and deploy model and scoring explainer\n",
    "\n",
    "\n",
    "_**This notebook illustrates how to use the Azure Machine Learning Interpretability SDK to train and explain a classification model remotely on an Azure Machine Leanrning Compute Target (AMLCompute), and use Azure Container Instances (ACI) for deploying your model and its corresponding scoring explainer as a web service.**_\n",
    "\n",
    "Problem: IBM employee attrition classification with scikit-learn (train a model and run an explainer remotely via AMLCompute, and deploy model and its corresponding explainer.)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Run model explainer locally at training time](#Explain)\n",
    "    1. Apply feature transformations\n",
    "    1. Train a binary classification model\n",
    "    1. Explain the model on raw features\n",
    "        1. Generate global explanations\n",
    "        1. Generate local explanations\n",
    "1. [Visualize results](#Visualize)\n",
    "1. [Deploy model and scoring explainer](#Deploy)\n",
    "1. [Next steps](#Next)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook showcases how to train and explain a classification model remotely via Azure Machine Learning Compute (AMLCompute), download the calculated explanations locally for visualization and inspection, and deploy the final model and its corresponding explainer to Azure Container Instances (ACI).\n",
    "It demonstrates the API calls that you need to make to submit a run for training and explaining a model to AMLCompute, download the compute explanations remotely, and visualizing the global and local explanations via a visualization dashboard that provides an interactive way of discovering patterns in model predictions and downloaded explanations, and using Azure Machine Learning MLOps capabilities to deploy your model and its corresponding explainer.\n",
    "\n",
    "We will showcase one of the tabular data explainers: TabularExplainer (SHAP) and follow these steps:\n",
    "1.\tDevelop a machine learning script in Python which involves the training script and the explanation script.\n",
    "2.\tCreate and configure a compute target.\n",
    "3.\tSubmit the scripts to the configured compute target to run in that environment. During training, the scripts can read from or write to datastore. And the records of execution (e.g., model, metrics, prediction explanations) are saved as runs in the workspace and grouped under experiments.\n",
    "4.\tQuery the experiment for logged metrics and explanations from the current and past runs. Use the interpretability toolkit\u00e2\u20ac\u2122s visualization dashboard to visualize predictions and their explanation. If the metrics and explanations don't indicate a desired outcome, loop back to step 1 and iterate on your scripts.\n",
    "5.\tAfter a satisfactory run is found, create a scoring explainer and register the persisted model and its corresponding explainer in the model registry.\n",
    "6.\tDevelop a scoring script.\n",
    "7.\tCreate an image and register it in the image registry.\n",
    "8.\tDeploy the image as a web service in Azure.\n",
    "\n",
    "| ![azure-machine-learning-cycle](./img/azure-machine-learning-cycle.png) |\n",
    "|:--:|"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Make sure you go through the [configuration notebook](../../../../configuration.ipynb) first if you haven't."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create workspace"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain\n",
    "\n",
    "Create An Experiment: **Experiment** is a logical container in an Azure ML Workspace. It hosts run records which can include run metrics and output artifacts from your experiments."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "experiment_name = 'explainer-remote-run-on-amlcompute'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to AmlCompute\n",
    "\n",
    "Azure Machine Learning Compute is managed compute infrastructure that allows the user to easily create single to multi-node compute of the appropriate VM Family. It is created **within your workspace region** and is a resource that can be used by other users in your workspace. It autoscales by default to the max_nodes, when a job is submitted, and executes in a containerized environment packaging the dependencies as specified by the user. \n",
    "\n",
    "Since it is managed compute, job scheduling and cluster management are handled internally by Azure Machine Learning service. \n",
    "\n",
    "For more information on Azure Machine Learning Compute, please read [this article](https://docs.microsoft.com/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute)\n",
    "\n",
    "If you are an existing BatchAI customer who is migrating to Azure Machine Learning, please read [this article](https://aka.ms/batchai-retirement)\n",
    "\n",
    "**Note**: As with other Azure services, there are limits on certain resources (for eg. AmlCompute quota) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota.\n",
    "\n",
    "\n",
    "The training script `run_explainer.py` is already created for you. Let's have a look."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit an AmlCompute run\n",
    "\n",
    "First lets check which VM families are available in your region. Azure is a regional service and some specialized SKUs (especially GPUs) are only available in certain regions. Since AmlCompute is created in the region of your workspace, we will use the supported_vms () function to see if the VM family we want to use ('STANDARD_D2_V2') is supported.\n",
    "\n",
    "You can also pass a different region to check availability and then re-create your workspace in that region through the [configuration notebook](../../../configuration.ipynb)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "\n",
    "AmlCompute.supported_vmsizes(workspace=ws)\n",
    "# AmlCompute.supported_vmsizes(workspace=ws, location='southcentralus')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create project directory\n",
    "\n",
    "Create a directory that will contain all the necessary code from your local machine that you will need access to on the remote resource. This includes the training script, and any additional files your training script depends on"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "project_folder = './explainer-remote-run-on-amlcompute'\n",
    "os.makedirs(project_folder, exist_ok=True)\n",
    "shutil.copy('train_explain.py', project_folder)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provision a compute target\n",
    "\n",
    "You can provision an AmlCompute resource by simply defining two parameters thanks to smart defaults. By default it autoscales from 0 nodes and provisions dedicated VMs to run your job in a container. This is useful when you want to continously re-use the same target, debug it between jobs or simply share the resource with other users of your workspace.\n",
    "\n",
    "* `vm_size`: VM family of the nodes provisioned by AmlCompute. Simply choose from the supported_vmsizes() above\n",
    "* `max_nodes`: Maximum nodes to autoscale to while running a job on AmlCompute"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"cpu-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=4)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "cpu_cluster.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure & Run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "\n",
    "# Create a new runconfig object\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "# Set compute target to AmlCompute target created in previous step\n",
    "run_config.target = cpu_cluster.name\n",
    "\n",
    "# Enable Docker \n",
    "run_config.environment.docker.enabled = True\n",
    "\n",
    "# Set Docker base image to the default CPU-based image\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "\n",
    "# Use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "azureml_pip_packages = [\n",
    "    'azureml-defaults', 'azureml-contrib-interpret', 'azureml-telemetry', 'azureml-interpret'\n",
    "]\n",
    " \n",
    "\n",
    "\n",
    "# Note: this is to pin the scikit-learn version to be same as notebook.\n",
    "# In production scenario user would choose their dependencies\n",
    "import pkg_resources\n",
    "available_packages = pkg_resources.working_set\n",
    "sklearn_ver = None\n",
    "pandas_ver = None\n",
    "for dist in available_packages:\n",
    "    if dist.key == 'scikit-learn':\n",
    "        sklearn_ver = dist.version\n",
    "    elif dist.key == 'pandas':\n",
    "        pandas_ver = dist.version\n",
    "sklearn_dep = 'scikit-learn'\n",
    "pandas_dep = 'pandas'\n",
    "if sklearn_ver:\n",
    "    sklearn_dep = 'scikit-learn=={}'.format(sklearn_ver)\n",
    "if pandas_ver:\n",
    "    pandas_dep = 'pandas=={}'.format(pandas_ver)\n",
    "# Specify CondaDependencies obj\n",
    "# The CondaDependencies specifies the conda and pip packages that are installed in the environment\n",
    "# the submitted job is run in.  Note the remote environment(s) needs to be similar to the local\n",
    "# environment, otherwise if a model is trained or deployed in a different environment this can\n",
    "# cause errors.  Please take extra care when specifying your dependencies in a production environment.\n",
    "azureml_pip_packages.extend(['sklearn-pandas', 'pyyaml', sklearn_dep, pandas_dep])\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(pip_packages=azureml_pip_packages)\n",
    "# Now submit a run on AmlCompute\n",
    "from azureml.core.script_run_config import ScriptRunConfig\n",
    "\n",
    "script_run_config = ScriptRunConfig(source_directory=project_folder,\n",
    "                                    script='train_explain.py',\n",
    "                                    run_config=run_config)\n",
    "\n",
    "run = experiment.submit(script_run_config)\n",
    "\n",
    "# Show run details\n",
    "run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you need to cancel a run, you can follow [these instructions](https://aka.ms/aml-docs-cancel-run)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Shows output of the run on stdout.\n",
    "run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete () is used to deprovision and delete the AmlCompute target. Useful if you want to re-use the compute name \n",
    "# 'cpucluster' in this case but use a different VM family for instance.\n",
    "\n",
    "# cpu_cluster.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model Explanation, Model, and Data"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve model for visualization and deployment\n",
    "from azureml.core.model import Model\n",
    "import joblib\n",
    "original_model = Model(ws, 'amlcompute_deploy_model')\n",
    "model_path = original_model.download(exist_ok=True)\n",
    "original_svm_model = joblib.load(model_path)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve global explanation for visualization\n",
    "from azureml.interpret import ExplanationClient\n",
    "\n",
    "# get model explanation data\n",
    "client = ExplanationClient.from_run(run)\n",
    "global_explanation = client.download_model_explanation()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve x_test for visualization\n",
    "import joblib\n",
    "x_test_path = './x_test.pkl'\n",
    "run.download_file('x_test_ibm.pkl', output_file_path=x_test_path)\n",
    "x_test = joblib.load(x_test_path)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "Visualize the explanations"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret_community.widget import ExplanationDashboard"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExplanationDashboard(global_explanation, original_svm_model, datasetX=x_test)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy\n",
    "Deploy Model and ScoringExplainer"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "# WARNING: to install this, g++ needs to be available on the Docker image and is not by default (look at the next cell)\n",
    "azureml_pip_packages = [\n",
    "    'azureml-defaults', 'azureml-contrib-interpret', 'azureml-core', 'azureml-telemetry',\n",
    "    'azureml-interpret'\n",
    "]\n",
    " \n",
    "\n",
    "# Note: this is to pin the scikit-learn and pandas versions to be same as notebook.\n",
    "# In production scenario user would choose their dependencies\n",
    "import pkg_resources\n",
    "available_packages = pkg_resources.working_set\n",
    "sklearn_ver = None\n",
    "pandas_ver = None\n",
    "for dist in available_packages:\n",
    "    if dist.key == 'scikit-learn':\n",
    "        sklearn_ver = dist.version\n",
    "    elif dist.key == 'pandas':\n",
    "        pandas_ver = dist.version\n",
    "sklearn_dep = 'scikit-learn'\n",
    "pandas_dep = 'pandas'\n",
    "if sklearn_ver:\n",
    "    sklearn_dep = 'scikit-learn=={}'.format(sklearn_ver)\n",
    "if pandas_ver:\n",
    "    pandas_dep = 'pandas=={}'.format(pandas_ver)\n",
    "# Specify CondaDependencies obj\n",
    "# The CondaDependencies specifies the conda and pip packages that are installed in the environment\n",
    "# the submitted job is run in.  Note the remote environment(s) needs to be similar to the local\n",
    "# environment, otherwise if a model is trained or deployed in a different environment this can\n",
    "# cause errors.  Please take extra care when specifying your dependencies in a production environment.\n",
    "azureml_pip_packages.extend(['sklearn-pandas', 'pyyaml', sklearn_dep, pandas_dep])\n",
    "myenv = CondaDependencies.create(pip_packages=azureml_pip_packages)\n",
    "\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())\n",
    "\n",
    "with open(\"myenv.yml\",\"r\") as f:\n",
    "    print(f.read())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve scoring explainer for deployment\n",
    "scoring_explainer_model = Model(ws, 'IBM_attrition_explainer')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags={\"data\": \"IBM_Attrition\",  \n",
    "                                                     \"method\" : \"local_explanation\"}, \n",
    "                                               description='Get local explanations for IBM Employee Attrition data')\n",
    "\n",
    "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\n",
    "inference_config = InferenceConfig(entry_script=\"score_remote_explain.py\", environment=myenv)\n",
    "\n",
    "# Use configs and models generated above\n",
    "service = Model.deploy(ws, 'model-scoring-service', [scoring_explainer_model, original_model], inference_config, aciconfig)\n",
    "service.wait_for_deployment(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Create data to test service with\n",
    "examples = x_test[:4]\n",
    "input_data = examples.to_json()\n",
    "\n",
    "headers = {'Content-Type':'application/json'}\n",
    "\n",
    "# Send request to service\n",
    "print(\"POST to url\", service.scoring_uri)\n",
    "resp = requests.post(service.scoring_uri, input_data, headers=headers)\n",
    "\n",
    "# Can covert back to Python objects from json string if desired\n",
    "print(\"prediction:\", resp.text)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "Learn about other use cases of the explain package on a:\n",
    "1. [Training time: regression problem](https://github.com/interpretml/interpret-community/blob/master/notebooks/explain-regression-local.ipynb)       \n",
    "1. [Training time: binary classification problem](https://github.com/interpretml/interpret-community/blob/master/notebooks/explain-binary-classification-local.ipynb)\n",
    "1. [Training time: multiclass classification problem](https://github.com/interpretml/interpret-community/blob/master/notebooks/explain-multiclass-classification-local.ipynb)\n",
    "1. Explain models with engineered features:\n",
    "    1. [Simple feature transformations](https://github.com/interpretml/interpret-community/blob/master/notebooks/simple-feature-transformations-explain-local.ipynb)\n",
    "    1. [Advanced feature transformations](https://github.com/interpretml/interpret-community/blob/master/notebooks/advanced-feature-transformations-explain-local.ipynb)\n",
    "1. [Save model explanations via Azure Machine Learning Run History](../run-history/save-retrieve-explanations-run-history.ipynb)\n",
    "1. [Run explainers remotely on Azure Machine Learning Compute (AMLCompute)](../remote-explanation/explain-model-on-amlcompute.ipynb)\n",
    "1. [Inferencing time: deploy a locally-trained model and explainer](./train-explain-model-locally-and-deploy.ipynb)\n",
    "1. [Inferencing time: deploy a locally-trained keras model and explainer](./train-explain-model-keras-locally-and-deploy.ipynb)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-locally-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-locally-and-deploy.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-locally-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and explain models locally and deploy model and scoring explainer\n",
    "\n",
    "\n",
    "_**This notebook illustrates how to use the Azure Machine Learning Interpretability SDK to deploy a locally-trained model and its corresponding scoring explainer to Azure Container Instances (ACI) as a web service.**_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Problem: IBM employee attrition classification with scikit-learn (train and explain a model locally and use Azure Container Instances (ACI) for deploying your model and its corresponding scoring explainer as a web service.)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Run model explainer locally at training time](#Explain)\n",
    "    1. Apply feature transformations\n",
    "    1. Train a binary classification model\n",
    "    1. Explain the model on raw features\n",
    "        1. Generate global explanations\n",
    "        1. Generate local explanations\n",
    "1. [Visualize explanations](#Visualize)\n",
    "1. [Deploy model and scoring explainer](#Deploy)\n",
    "1. [Next steps](#Next)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-locally-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "This notebook showcases how to train and explain a classification model locally, and deploy the trained model and its corresponding explainer to Azure Container Instances (ACI).\n",
    "It demonstrates the API calls that you need to make to submit a run for training and explaining a model to AMLCompute, download the compute explanations remotely, and visualizing the global and local explanations via a visualization dashboard that provides an interactive way of discovering patterns in model predictions and downloaded explanations. It also demonstrates how to use Azure Machine Learning MLOps capabilities to deploy your model and its corresponding explainer.\n",
    "\n",
    "We will showcase one of the tabular data explainers: TabularExplainer (SHAP) and follow these steps:\n",
    "1.\tDevelop a machine learning script in Python which involves the training script and the explanation script.\n",
    "2.\tRun the script locally.\n",
    "3.\tUse the interpretability toolkit\u00e2\u20ac\u2122s visualization dashboard to visualize predictions and their explanation. If the metrics and explanations don't indicate a desired outcome, loop back to step 1 and iterate on your scripts.\n",
    "5.\tAfter a satisfactory run is found, create a scoring explainer and register the persisted model and its corresponding explainer in the model registry.\n",
    "6.\tDevelop a scoring script.\n",
    "7.\tCreate an image and register it in the image registry.\n",
    "8.\tDeploy the image as a web service in Azure.\n",
    "\n",
    "\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-locally-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Make sure you go through the [configuration notebook](../../../../configuration.ipynb) first if you haven't."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-locally-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-locally-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-locally-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create workspace"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-locally-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain\n",
    "Create An Experiment: **Experiment** is a logical container in an Azure ML Workspace. It hosts run records which can include run metrics and output artifacts from your experiments."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-locally-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "experiment_name = 'explain_model_at_scoring_time'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "run = experiment.start_logging()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-locally-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get IBM attrition data\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "outdirname = 'dataset.6.21.19'\n",
    "try:\n",
    "    from urllib import urlretrieve\n",
    "except ImportError:\n",
    "    from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "zipfilename = outdirname + '.zip'\n",
    "urlretrieve('https://publictestdatasets.blob.core.windows.net/data/' + zipfilename, zipfilename)\n",
    "with zipfile.ZipFile(zipfilename, 'r') as unzip:\n",
    "    unzip.extractall('.')\n",
    "attritionData = pd.read_csv('./WA_Fn-UseC_-HR-Employee-Attrition.csv')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-locally-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "from interpret.ext.blackbox import TabularExplainer\n",
    "\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "\n",
    "# Dropping Employee count as all values are 1 and hence attrition is independent of this feature\n",
    "attritionData = attritionData.drop(['EmployeeCount'], axis=1)\n",
    "# Dropping Employee Number since it is merely an identifier\n",
    "attritionData = attritionData.drop(['EmployeeNumber'], axis=1)\n",
    "attritionData = attritionData.drop(['Over18'], axis=1)\n",
    "# Since all values are 80\n",
    "attritionData = attritionData.drop(['StandardHours'], axis=1)\n",
    "\n",
    "# Converting target variables from string to numerical values\n",
    "target_map = {'Yes': 1, 'No': 0}\n",
    "attritionData[\"Attrition_numerical\"] = attritionData[\"Attrition\"].apply(lambda x: target_map[x])\n",
    "target = attritionData[\"Attrition_numerical\"]\n",
    "\n",
    "attritionXData = attritionData.drop(['Attrition_numerical', 'Attrition'], axis=1)\n",
    "\n",
    "# Creating dummy columns for each categorical feature\n",
    "categorical = []\n",
    "for col, value in attritionXData.iteritems():\n",
    "    if value.dtype == 'object':\n",
    "        categorical.append(col)\n",
    "\n",
    "# Store the numerical columns in a list numerical\n",
    "numerical = attritionXData.columns.difference(categorical)\n",
    "\n",
    "numeric_transformations = [([f], Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])) for f in numerical]\n",
    "\n",
    "categorical_transformations = [([f], OneHotEncoder(handle_unknown='ignore', sparse=False)) for f in categorical]\n",
    "\n",
    "transformations = numeric_transformations + categorical_transformations\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', DataFrameMapper(transformations)),\n",
    "                      ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Split data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(attritionXData,\n",
    "                                                    target,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=target)\n",
    "\n",
    "# Preprocess the data and fit the classification model\n",
    "clf.fit(x_train, y_train)\n",
    "model = clf.steps[-1][1]\n",
    "\n",
    "model_file_name = 'log_reg.pkl'\n",
    "\n",
    "# Save model in the outputs folder so it automatically get uploaded\n",
    "with open(model_file_name, 'wb') as file:\n",
    "    joblib.dump(value=clf, filename=os.path.join('./outputs/',\n",
    "                                                 model_file_name))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-locally-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain predictions on your local machine\n",
    "tabular_explainer = TabularExplainer(model, \n",
    "                                     initialization_examples=x_train, \n",
    "                                     features=attritionXData.columns, \n",
    "                                     classes=[\"Not leaving\", \"leaving\"], \n",
    "                                     transformations=transformations)\n",
    "\n",
    "# Explain overall model predictions (global explanation)\n",
    "# Passing in test dataset for evaluation examples - note it must be a representative sample of the original data\n",
    "# x_train can be passed as well, but with more examples explanations it will\n",
    "# take longer although they may be more accurate\n",
    "global_explanation = tabular_explainer.explain_global(x_test)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-locally-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.interpret.scoring.scoring_explainer import TreeScoringExplainer, save\n",
    "# ScoringExplainer\n",
    "scoring_explainer = TreeScoringExplainer(tabular_explainer)\n",
    "# Pickle scoring explainer locally\n",
    "save(scoring_explainer, exist_ok=True)\n",
    "\n",
    "# Register original model\n",
    "run.upload_file('original_model.pkl', os.path.join('./outputs/', model_file_name))\n",
    "original_model = run.register_model(model_name='local_deploy_model', \n",
    "                                    model_path='original_model.pkl')\n",
    "\n",
    "# Register scoring explainer\n",
    "run.upload_file('IBM_attrition_explainer.pkl', 'scoring_explainer.pkl')\n",
    "scoring_explainer_model = run.register_model(model_name='IBM_attrition_explainer', model_path='IBM_attrition_explainer.pkl')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-locally-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "Visualize the explanations"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-locally-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret_community.widget import ExplanationDashboard"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-locally-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExplanationDashboard(global_explanation, clf, datasetX=x_test)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-locally-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy \n",
    "\n",
    "Deploy Model and ScoringExplainer.\n",
    "\n",
    "Please note that you must indicate azureml-defaults with verion >= 1.0.45 as a pip dependency, because it contains the functionality needed to host the model as a web service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-locally-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "# azureml-defaults is required to host the model as a web service.\n",
    "azureml_pip_packages = [\n",
    "    'azureml-defaults', 'azureml-contrib-interpret', 'azureml-core', 'azureml-telemetry',\n",
    "    'azureml-interpret'\n",
    "]\n",
    " \n",
    "\n",
    "# Note: this is to pin the scikit-learn and pandas versions to be same as notebook.\n",
    "# In production scenario user would choose their dependencies\n",
    "import pkg_resources\n",
    "available_packages = pkg_resources.working_set\n",
    "sklearn_ver = None\n",
    "pandas_ver = None\n",
    "for dist in available_packages:\n",
    "    if dist.key == 'scikit-learn':\n",
    "        sklearn_ver = dist.version\n",
    "    elif dist.key == 'pandas':\n",
    "        pandas_ver = dist.version\n",
    "sklearn_dep = 'scikit-learn'\n",
    "pandas_dep = 'pandas'\n",
    "if sklearn_ver:\n",
    "    sklearn_dep = 'scikit-learn=={}'.format(sklearn_ver)\n",
    "if pandas_ver:\n",
    "    pandas_dep = 'pandas=={}'.format(pandas_ver)\n",
    "# Specify CondaDependencies obj\n",
    "# The CondaDependencies specifies the conda and pip packages that are installed in the environment\n",
    "# the submitted job is run in.  Note the remote environment(s) needs to be similar to the local\n",
    "# environment, otherwise if a model is trained or deployed in a different environment this can\n",
    "# cause errors.  Please take extra care when specifying your dependencies in a production environment.\n",
    "myenv = CondaDependencies.create(pip_packages=['sklearn-pandas', 'pyyaml', sklearn_dep, pandas_dep] + azureml_pip_packages,\n",
    "                                 pin_sdk_version=False)\n",
    "\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())\n",
    "\n",
    "with open(\"myenv.yml\",\"r\") as f:\n",
    "    print(f.read())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-locally-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "# Retrieve scoring explainer for deployment\n",
    "scoring_explainer_model = Model(ws, 'IBM_attrition_explainer')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-locally-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags={\"data\": \"IBM_Attrition\",  \n",
    "                                                     \"method\" : \"local_explanation\"}, \n",
    "                                               description='Get local explanations for IBM Employee Attrition data')\n",
    "\n",
    "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\n",
    "inference_config = InferenceConfig(entry_script=\"score_local_explain.py\", environment=myenv)\n",
    "\n",
    "# Use configs and models generated above\n",
    "service = Model.deploy(ws, 'model-scoring-deploy-local', [scoring_explainer_model, original_model], inference_config, aciconfig)\n",
    "service.wait_for_deployment(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-locally-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "# Create data to test service with\n",
    "sample_data = '{\"Age\":{\"899\":49},\"BusinessTravel\":{\"899\":\"Travel_Rarely\"},\"DailyRate\":{\"899\":1098},\"Department\":{\"899\":\"Research & Development\"},\"DistanceFromHome\":{\"899\":4},\"Education\":{\"899\":2},\"EducationField\":{\"899\":\"Medical\"},\"EnvironmentSatisfaction\":{\"899\":1},\"Gender\":{\"899\":\"Male\"},\"HourlyRate\":{\"899\":85},\"JobInvolvement\":{\"899\":2},\"JobLevel\":{\"899\":5},\"JobRole\":{\"899\":\"Manager\"},\"JobSatisfaction\":{\"899\":3},\"MaritalStatus\":{\"899\":\"Married\"},\"MonthlyIncome\":{\"899\":18711},\"MonthlyRate\":{\"899\":12124},\"NumCompaniesWorked\":{\"899\":2},\"OverTime\":{\"899\":\"No\"},\"PercentSalaryHike\":{\"899\":13},\"PerformanceRating\":{\"899\":3},\"RelationshipSatisfaction\":{\"899\":3},\"StockOptionLevel\":{\"899\":1},\"TotalWorkingYears\":{\"899\":23},\"TrainingTimesLastYear\":{\"899\":2},\"WorkLifeBalance\":{\"899\":4},\"YearsAtCompany\":{\"899\":1},\"YearsInCurrentRole\":{\"899\":0},\"YearsSinceLastPromotion\":{\"899\":0},\"YearsWithCurrManager\":{\"899\":0}}'\n",
    "\n",
    "\n",
    "\n",
    "headers = {'Content-Type':'application/json'}\n",
    "\n",
    "# Send request to service\n",
    "print(\"POST to url\", service.scoring_uri)\n",
    "resp = requests.post(service.scoring_uri, sample_data, headers=headers)\n",
    "\n",
    "# Can covert back to Python objects from json string if desired\n",
    "print(\"prediction:\", resp.text)\n",
    "result = json.loads(resp.text)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-locally-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importance for the prediction\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "\n",
    "labels = json.loads(sample_data)\n",
    "labels = labels.keys()\n",
    "objects = labels\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = result[\"local_importance_values\"][0][0]\n",
    "\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "locs, labels = plt.xticks()\n",
    "plt.setp(labels, rotation=90)\n",
    "plt.ylabel('Feature impact - leaving vs not leaving')\n",
    "plt.title('Local feature importance for prediction')\n",
    "\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-locally-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-locally-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "Learn about other use cases of the explain package on a:\n",
    "1. [Training time: regression problem](https://github.com/interpretml/interpret-community/blob/master/notebooks/explain-regression-local.ipynb)       \n",
    "1. [Training time: binary classification problem](https://github.com/interpretml/interpret-community/blob/master/notebooks/explain-binary-classification-local.ipynb)\n",
    "1. [Training time: multiclass classification problem](https://github.com/interpretml/interpret-community/blob/master/notebooks/explain-multiclass-classification-local.ipynb)\n",
    "1. Explain models with engineered features:\n",
    "    1. [Simple feature transformations](https://github.com/interpretml/interpret-community/blob/master/notebooks/simple-feature-transformations-explain-local.ipynb)\n",
    "    1. [Advanced feature transformations](https://github.com/interpretml/interpret-community/blob/master/notebooks/advanced-feature-transformations-explain-local.ipynb)\n",
    "1. [Save model explanations via Azure Machine Learning Run History](../run-history/save-retrieve-explanations-run-history.ipynb)\n",
    "1. [Run explainers remotely on Azure Machine Learning Compute (AMLCompute)](../remote-explanation/explain-model-on-amlcompute.ipynb)\n",
    "1. [Inferencing time: deploy a remotely-trained model and explainer](./train-explain-model-on-amlcompute-and-deploy.ipynb)\n",
    "1. [Inferencing time: deploy a locally-trained keras model and explainer](./train-explain-model-keras-locally-and-deploy.ipynb)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-locally-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/scoring-time/train-explain-model-locally-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and retrieve explanations via Azure Machine Learning Run History\n",
    "\n",
    "_**This notebook showcases how to use the Azure Machine Learning Interpretability SDK to save and retrieve classification model explanations to/from Azure Machine Learning Run History.**_\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Run model explainer locally at training time](#Explain)\n",
    "    1. Apply feature transformations\n",
    "    1. Train a binary classification model\n",
    "    1. Explain the model on raw features\n",
    "        1. Generate global explanations\n",
    "        1. Generate local explanations\n",
    "1. [Upload model explanations to Azure Machine Learning Run History](#Upload)\n",
    "1. [Download model explanations from Azure Machine Learning Run History](#Download)\n",
    "1. [Visualize explanations](#Visualize)\n",
    "1. [Next steps](#Next)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook showcases how to explain a classification model predictions locally at training time, upload explanations to the Azure Machine Learning's run history, and download previously-uploaded explanations from the Run History.\n",
    "It demonstrates the API calls that you need to make to upload/download the global and local explanations and a visualization dashboard that provides an interactive way of discovering patterns in data and downloaded explanations.\n",
    "\n",
    "We will showcase three tabular data explainers: TabularExplainer (SHAP), MimicExplainer (global surrogate), and PFIExplainer.\n",
    "\n",
    "\n",
    "\n",
    "Problem: IBM employee attrition classification with scikit-learn (run model explainer locally and upload explanation to the Azure Machine Learning Run History)\n",
    "\n",
    "1. Train a SVM classification model using Scikit-learn\n",
    "2. Run 'explain_model' with AML Run History, which leverages run history service to store and manage the explanation data\n",
    "---\n",
    "\n",
    "Setup: If you are using Jupyter notebooks, the extensions should be installed automatically with the package.\n",
    "If you are using Jupyter Labs run the following command:\n",
    "```\n",
    "(myenv) $ jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "```\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain\n",
    "\n",
    "### Run model explainer locally at training time"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Explainers:\n",
    "# 1. SHAP Tabular Explainer\n",
    "from interpret.ext.blackbox import TabularExplainer\n",
    "\n",
    "# OR\n",
    "\n",
    "# 2. Mimic Explainer\n",
    "from interpret.ext.blackbox import MimicExplainer\n",
    "# You can use one of the following four interpretable models as a global surrogate to the black box model\n",
    "from interpret.ext.glassbox import LGBMExplainableModel\n",
    "from interpret.ext.glassbox import LinearExplainableModel\n",
    "from interpret.ext.glassbox import SGDExplainableModel\n",
    "from interpret.ext.glassbox import DecisionTreeExplainableModel\n",
    "\n",
    "# OR\n",
    "\n",
    "# 3. PFI Explainer\n",
    "from interpret.ext.blackbox import PFIExplainer "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the IBM employee attrition data"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the IBM employee attrition dataset\n",
    "outdirname = 'dataset.6.21.19'\n",
    "try:\n",
    "    from urllib import urlretrieve\n",
    "except ImportError:\n",
    "    from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "zipfilename = outdirname + '.zip'\n",
    "urlretrieve('https://publictestdatasets.blob.core.windows.net/data/' + zipfilename, zipfilename)\n",
    "with zipfile.ZipFile(zipfilename, 'r') as unzip:\n",
    "    unzip.extractall('.')\n",
    "attritionData = pd.read_csv('./WA_Fn-UseC_-HR-Employee-Attrition.csv')\n",
    "\n",
    "# Dropping Employee count as all values are 1 and hence attrition is independent of this feature\n",
    "attritionData = attritionData.drop(['EmployeeCount'], axis=1)\n",
    "# Dropping Employee Number since it is merely an identifier\n",
    "attritionData = attritionData.drop(['EmployeeNumber'], axis=1)\n",
    "\n",
    "attritionData = attritionData.drop(['Over18'], axis=1)\n",
    "\n",
    "# Since all values are 80\n",
    "attritionData = attritionData.drop(['StandardHours'], axis=1)\n",
    "\n",
    "# Converting target variables from string to numerical values\n",
    "target_map = {'Yes': 1, 'No': 0}\n",
    "attritionData[\"Attrition_numerical\"] = attritionData[\"Attrition\"].apply(lambda x: target_map[x])\n",
    "target = attritionData[\"Attrition_numerical\"]\n",
    "\n",
    "attritionXData = attritionData.drop(['Attrition_numerical', 'Attrition'], axis=1)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(attritionXData, \n",
    "                                                    target, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=target)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummy columns for each categorical feature\n",
    "categorical = []\n",
    "for col, value in attritionXData.iteritems():\n",
    "    if value.dtype == 'object':\n",
    "        categorical.append(col)\n",
    "        \n",
    "# Store the numerical columns in a list numerical\n",
    "numerical = attritionXData.columns.difference(categorical)        "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform raw features"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explain raw features by either using a `sklearn.compose.ColumnTransformer` or a list of fitted transformer tuples. The cell below uses `sklearn.compose.ColumnTransformer`. In case you want to run the example with the list of fitted transformer tuples, comment the cell below and uncomment the cell that follows after. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "transformations = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical),\n",
    "        ('cat', categorical_transformer, categorical)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', transformations),\n",
    "                      ('classifier', SVC(C=1.0, probability=True))])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Uncomment below if sklearn-pandas is not installed\n",
    "#!pip install sklearn-pandas\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "# Impute, standardize the numeric features and one-hot encode the categorical features.    \n",
    "\n",
    "\n",
    "numeric_transformations = [([f], Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])) for f in numerical]\n",
    "\n",
    "categorical_transformations = [([f], OneHotEncoder(handle_unknown='ignore', sparse=False)) for f in categorical]\n",
    "\n",
    "transformations = numeric_transformations + categorical_transformations\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', transformations),\n",
    "                      ('classifier', SVC(C=1.0, probability=True))]) \n",
    "\n",
    "\n",
    "\n",
    "'''"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a SVM classification model, which you want to explain"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = clf.fit(x_train, y_train)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain predictions on your local machine"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Using SHAP TabularExplainer\n",
    "# clf.steps[-1][1] returns the trained classification model\n",
    "explainer = TabularExplainer(clf.steps[-1][1], \n",
    "                             initialization_examples=x_train, \n",
    "                             features=attritionXData.columns, \n",
    "                             classes=[\"Not leaving\", \"leaving\"], \n",
    "                             transformations=transformations)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. Using MimicExplainer\n",
    "# augment_data is optional and if true, oversamples the initialization examples to improve surrogate model accuracy to fit original model.  Useful for high-dimensional data where the number of rows is less than the number of columns. \n",
    "# max_num_of_augmentations is optional and defines max number of times we can increase the input data size.\n",
    "# LGBMExplainableModel can be replaced with LinearExplainableModel, SGDExplainableModel, or DecisionTreeExplainableModel\n",
    "# explainer = MimicExplainer(clf.steps[-1][1], \n",
    "#                            x_train, \n",
    "#                            LGBMExplainableModel, \n",
    "#                            augment_data=True, \n",
    "#                            max_num_of_augmentations=10, \n",
    "#                            features=attritionXData.columns, \n",
    "#                            classes=[\"Not leaving\", \"leaving\"], \n",
    "#                            transformations=transformations)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. Using PFIExplainer\n",
    "\n",
    "# Use the parameter \"metric\" to pass a metric name or function to evaluate the permutation. \n",
    "# Note that if a metric function is provided a higher value must be better.\n",
    "# Otherwise, take the negative of the function or set the parameter \"is_error_metric\" to True.\n",
    "# Default metrics: \n",
    "# F1 Score for binary classification, F1 Score with micro average for multiclass classification and\n",
    "# Mean absolute error for regression\n",
    "\n",
    "# explainer = PFIExplainer(clf.steps[-1][1], \n",
    "#                          features=x_train.columns, \n",
    "#                          transformations=transformations,\n",
    "#                          classes=[\"Not leaving\", \"leaving\"])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate global explanations\n",
    "Explain overall model predictions (global explanation)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing in test dataset for evaluation examples - note it must be a representative sample of the original data\n",
    "# x_train can be passed as well, but with more examples explanations will take longer although they may be more accurate\n",
    "global_explanation = explainer.explain_global(x_test)\n",
    "\n",
    "# Note: if you used the PFIExplainer in the previous step, use the next line of code instead\n",
    "# global_explanation = explainer.explain_global(x_test, true_labels=y_test)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted SHAP values\n",
    "print('ranked global importance values: {}'.format(global_explanation.get_ranked_global_values()))\n",
    "# Corresponding feature names\n",
    "print('ranked global importance names: {}'.format(global_explanation.get_ranked_global_names()))\n",
    "# Feature ranks (based on original order of features)\n",
    "print('global importance rank: {}'.format(global_explanation.global_importance_rank))\n",
    "\n",
    "# Note: PFIExplainer does not support per class explanations\n",
    "# Per class feature names\n",
    "print('ranked per class feature names: {}'.format(global_explanation.get_ranked_per_class_names()))\n",
    "# Per class feature importance values\n",
    "print('ranked per class feature values: {}'.format(global_explanation.get_ranked_per_class_values()))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out a dictionary that holds the sorted feature importance names and values\n",
    "print('global importance rank: {}'.format(global_explanation.get_feature_importance_dict()))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain overall model predictions as a collection of local (instance-level) explanations"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature shap values for all features and all data points in the training data\n",
    "print('local importance values: {}'.format(global_explanation.local_importance_values))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate local explanations\n",
    "Explain local data points (individual instances)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: PFIExplainer does not support local explanations\n",
    "# You can pass a specific data point or a group of data points to the explain_local function\n",
    "\n",
    "# E.g., Explain the first data point in the test set\n",
    "instance_num = 1\n",
    "local_explanation = explainer.explain_local(x_test[:instance_num])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prediction for the first member of the test set and explain why model made that prediction\n",
    "prediction_value = clf.predict(x_test)[instance_num]\n",
    "\n",
    "sorted_local_importance_values = local_explanation.get_ranked_local_values()[prediction_value]\n",
    "sorted_local_importance_names = local_explanation.get_ranked_local_names()[prediction_value]\n",
    "\n",
    "print('local importance values: {}'.format(sorted_local_importance_values))\n",
    "print('local importance names: {}'.format(sorted_local_importance_names))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload\n",
    "Upload explanations to Azure Machine Learning Run History"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.interpret import ExplanationClient\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'explain_model'\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "run = experiment.start_logging()\n",
    "client = ExplanationClient.from_run(run)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading model explanation data for storage or visualization in webUX\n",
    "# The explanation can then be downloaded on any compute\n",
    "# Multiple explanations can be uploaded\n",
    "client.upload_model_explanation(global_explanation, comment='global explanation: all features')\n",
    "# Or you can only upload the explanation object with the top k feature info\n",
    "#client.upload_model_explanation(global_explanation, top_k=2, comment='global explanation: Only top 2 features')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading model explanation data for storage or visualization in webUX\n",
    "# The explanation can then be downloaded on any compute\n",
    "# Multiple explanations can be uploaded\n",
    "client.upload_model_explanation(local_explanation, comment='local explanation for test point 1: all features')\n",
    "\n",
    "# Alterntively, you can only upload the local explanation object with the top k feature info\n",
    "#client.upload_model_explanation(local_explanation, top_k=2, comment='local explanation: top 2 features')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download\n",
    "Download explanations from Azure Machine Learning Run History"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List uploaded explanations\n",
    "client.list_model_explanations()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for explanation in client.list_model_explanations():\n",
    "    \n",
    "    if explanation['comment'] == 'local explanation for test point 1: all features':\n",
    "        downloaded_local_explanation = client.download_model_explanation(explanation_id=explanation['id'])\n",
    "        # You can pass a k value to only download the top k feature importance values\n",
    "        downloaded_local_explanation_top2 = client.download_model_explanation(top_k=2, explanation_id=explanation['id'])\n",
    "    \n",
    "    \n",
    "    elif explanation['comment'] == 'global explanation: all features':\n",
    "        downloaded_global_explanation = client.download_model_explanation(explanation_id=explanation['id'])\n",
    "        # You can pass a k value to only download the top k feature importance values\n",
    "        downloaded_global_explanation_top2 = client.download_model_explanation(top_k=2, explanation_id=explanation['id'])\n",
    "    "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "Load the visualization dashboard"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret_community.widget import ExplanationDashboard"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExplanationDashboard(downloaded_global_explanation, model, datasetX=x_test)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End\n",
    "Complete the run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.complete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "Learn about other use cases of the explain package on a:\n",
    "1. [Training time: regression problem](https://github.com/interpretml/interpret-community/blob/master/notebooks/explain-regression-local.ipynb)       \n",
    "1. [Training time: binary classification problem](https://github.com/interpretml/interpret-community/blob/master/notebooks/explain-binary-classification-local.ipynb)\n",
    "1. [Training time: multiclass classification problem](https://github.com/interpretml/interpret-community/blob/master/notebooks/explain-multiclass-classification-local.ipynb)\n",
    "1. Explain models with engineered features:\n",
    "    1. [Simple feature transformations](https://github.com/interpretml/interpret-community/blob/master/notebooks/simple-feature-transformations-explain-local.ipynb)\n",
    "    1. [Advanced feature transformations](https://github.com/interpretml/interpret-community/blob/master/notebooks/advanced-feature-transformations-explain-local.ipynb)\n",
    "1. [Run explainers remotely on Azure Machine Learning Compute (AMLCompute)](../remote-explanation/explain-model-on-amlcompute.ipynb)\n",
    "1. Inferencing time: deploy a classification model and explainer:\n",
    "    1. [Deploy a locally-trained model and explainer](../scoring-time/train-explain-model-locally-and-deploy.ipynb)\n",
    "    1. [Deploy a locally-trained keras model and explainer](../scoring-time/train-explain-model-keras-locally-and-deploy.ipynb)\n",
    "    1. [Deploy a remotely-trained model and explainer](../scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/run-history/save-retrieve-explanations-run-history.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and explain models remotely via Azure Machine Learning Compute\n",
    "\n",
    "\n",
    "_**This notebook showcases how to use the Azure Machine Learning Interpretability SDK to train and explain a regression model remotely on an Azure Machine Learning Compute Target (AMLCompute).**_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "    1. Initialize a Workspace\n",
    "    1. Create an Experiment\n",
    "    1. Introduction to AmlCompute\n",
    "    1. Submit an AmlCompute run\n",
    "1. Additional operations to perform on AmlCompute\n",
    "1. [Download model explanations from Azure Machine Learning Run History](#Download)\n",
    "1. [Visualize explanations](#Visualize)\n",
    "1. [Next steps](#Next)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook showcases how to train and explain a regression model remotely via Azure Machine Learning Compute (AMLCompute), and download the calculated explanations locally for visualization.\n",
    "It demonstrates the API calls that you need to make to submit a run for training and explaining a model to AMLCompute, download the compute explanations remotely, and visualizing the global and local explanations via a visualization dashboard that provides an interactive way of discovering patterns in model predictions and downloaded explanations.\n",
    "\n",
    "We will showcase one of the tabular data explainers: TabularExplainer (SHAP).\n",
    "\n",
    "Problem: Boston Housing Price Prediction with scikit-learn (train a model and run an explainer remotely via AMLCompute, and download and visualize the remotely-calculated explanations.)\n",
    "\n",
    "| ![explanations-run-history](./img/explanations-run-history.png) |\n",
    "|:--:|\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the [configuration notebook](../../../configuration.ipynb) first if you haven't.\n",
    "\n",
    "\n",
    "If you are using Jupyter notebooks, the extensions should be installed automatically with the package.\n",
    "If you are using Jupyter Labs run the following command:\n",
    "```\n",
    "(myenv) $ jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "```\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create workspace"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create An Experiment\n",
    "\n",
    "**Experiment** is a logical container in an Azure ML Workspace. It hosts run records which can include run metrics and output artifacts from your experiments."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "experiment_name = 'explainer-remote-run-on-amlcompute'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to AmlCompute\n",
    "\n",
    "Azure Machine Learning Compute is managed compute infrastructure that allows the user to easily create single to multi-node compute of the appropriate VM Family. It is created **within your workspace region** and is a resource that can be used by other users in your workspace. It autoscales by default to the max_nodes, when a job is submitted, and executes in a containerized environment packaging the dependencies as specified by the user. \n",
    "\n",
    "Since it is managed compute, job scheduling and cluster management are handled internally by Azure Machine Learning service. \n",
    "\n",
    "For more information on Azure Machine Learning Compute, please read [this article](https://docs.microsoft.com/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute)\n",
    "\n",
    "If you are an existing BatchAI customer who is migrating to Azure Machine Learning, please read [this article](https://aka.ms/batchai-retirement)\n",
    "\n",
    "**Note**: As with other Azure services, there are limits on certain resources (for eg. AmlCompute quota) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota.\n",
    "\n",
    "\n",
    "The training script `train_explain.py` is already created for you. Let's have a look."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit an AmlCompute run\n",
    "\n",
    "First lets check which VM families are available in your region. Azure is a regional service and some specialized SKUs (especially GPUs) are only available in certain regions. Since AmlCompute is created in the region of your workspace, we will use the supported_vms () function to see if the VM family we want to use ('STANDARD_D2_V2') is supported.\n",
    "\n",
    "You can also pass a different region to check availability and then re-create your workspace in that region through the [configuration notebook](../../../configuration.ipynb)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "\n",
    "AmlCompute.supported_vmsizes(workspace=ws)\n",
    "# AmlCompute.supported_vmsizes(workspace=ws, location='southcentralus')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create project directory\n",
    "\n",
    "Create a directory that will contain all the necessary code from your local machine that you will need access to on the remote resource. This includes the training script, and any additional files your training script depends on"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "project_folder = './explainer-remote-run-on-amlcompute'\n",
    "os.makedirs(project_folder, exist_ok=True)\n",
    "shutil.copy('train_explain.py', project_folder)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provision a compute target\n",
    "\n",
    "You can provision an AmlCompute resource by simply defining two parameters thanks to smart defaults. By default it autoscales from 0 nodes and provisions dedicated VMs to run your job in a container. This is useful when you want to continously re-use the same target, debug it between jobs or simply share the resource with other users of your workspace.\n",
    "\n",
    "* `vm_size`: VM family of the nodes provisioned by AmlCompute. Simply choose from the supported_vmsizes() above\n",
    "* `max_nodes`: Maximum nodes to autoscale to while running a job on AmlCompute"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"cpu-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=4)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "cpu_cluster.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure & Run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Create a new RunConfig object\n",
    "run_config = RunConfiguration(framework=\"python\")\n",
    "\n",
    "# Set compute target to AmlCompute target created in previous step\n",
    "run_config.target = cpu_cluster.name\n",
    "\n",
    "# Enable Docker \n",
    "run_config.environment.docker.enabled = True\n",
    "\n",
    "azureml_pip_packages = [\n",
    "    'azureml-defaults', 'azureml-contrib-interpret', 'azureml-telemetry', 'azureml-interpret'\n",
    "]\n",
    "\n",
    "# Note: this is to pin the scikit-learn and pandas versions to be same as notebook.\n",
    "# In production scenario user would choose their dependencies\n",
    "import pkg_resources\n",
    "available_packages = pkg_resources.working_set\n",
    "sklearn_ver = None\n",
    "pandas_ver = None\n",
    "for dist in available_packages:\n",
    "    if dist.key == 'scikit-learn':\n",
    "        sklearn_ver = dist.version\n",
    "    elif dist.key == 'pandas':\n",
    "        pandas_ver = dist.version\n",
    "sklearn_dep = 'scikit-learn'\n",
    "pandas_dep = 'pandas'\n",
    "if sklearn_ver:\n",
    "    sklearn_dep = 'scikit-learn=={}'.format(sklearn_ver)\n",
    "if pandas_ver:\n",
    "    pandas_dep = 'pandas=={}'.format(pandas_ver)\n",
    "# Specify CondaDependencies obj\n",
    "# The CondaDependencies specifies the conda and pip packages that are installed in the environment\n",
    "# the submitted job is run in.  Note the remote environment(s) needs to be similar to the local\n",
    "# environment, otherwise if a model is trained or deployed in a different environment this can\n",
    "# cause errors.  Please take extra care when specifying your dependencies in a production environment.\n",
    "azureml_pip_packages.extend([sklearn_dep, pandas_dep])\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(pip_packages=azureml_pip_packages)\n",
    "\n",
    "from azureml.core import Run\n",
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "src = ScriptRunConfig(source_directory=project_folder, \n",
    "                      script='train_explain.py', \n",
    "                      run_config=run_config) \n",
    "run = experiment.submit(config=src)\n",
    "run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you need to cancel a run, you can follow [these instructions](https://aka.ms/aml-docs-cancel-run)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Shows output of the run on stdout.\n",
    "run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_metrics()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download \n",
    "1. Download model explanation data."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.interpret import ExplanationClient\n",
    "\n",
    "# Get model explanation data\n",
    "client = ExplanationClient.from_run(run)\n",
    "global_explanation = client.download_model_explanation()\n",
    "local_importance_values = global_explanation.local_importance_values\n",
    "expected_values = global_explanation.expected_values\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or you can use the saved run.id to retrive the feature importance values\n",
    "client = ExplanationClient.from_run_id(ws, experiment_name, run.id)\n",
    "global_explanation = client.download_model_explanation()\n",
    "local_importance_values = global_explanation.local_importance_values\n",
    "expected_values = global_explanation.expected_values"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top k (e.g., 4) most important features with their importance values\n",
    "global_explanation_topk = client.download_model_explanation(top_k=4)\n",
    "global_importance_values = global_explanation_topk.get_ranked_global_values()\n",
    "global_importance_names = global_explanation_topk.get_ranked_global_names()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('global importance values: {}'.format(global_importance_values))\n",
    "print('global importance names: {}'.format(global_importance_names))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Download model file."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve model for visualization and deployment\n",
    "from azureml.core.model import Model\n",
    "import joblib\n",
    "original_model = Model(ws, 'model_explain_model_on_amlcomp')\n",
    "model_path = original_model.download(exist_ok=True)\n",
    "original_model = joblib.load(model_path)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Download test dataset."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve x_test for visualization\n",
    "import joblib\n",
    "x_test_path = './x_test_boston_housing.pkl'\n",
    "run.download_file('x_test_boston_housing.pkl', output_file_path=x_test_path)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = joblib.load('x_test_boston_housing.pkl')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "Load the visualization dashboard"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret_community.widget import ExplanationDashboard"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExplanationDashboard(global_explanation, original_model, datasetX=x_test)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "Learn about other use cases of the explain package on a:\n",
    "1. [Training time: regression problem](https://github.com/interpretml/interpret-community/blob/master/notebooks/explain-regression-local.ipynb)       \n",
    "1. [Training time: binary classification problem](https://github.com/interpretml/interpret-community/blob/master/notebooks/explain-binary-classification-local.ipynb)\n",
    "1. [Training time: multiclass classification problem](https://github.com/interpretml/interpret-community/blob/master/notebooks/explain-multiclass-classification-local.ipynb)\n",
    "1. Explain models with engineered features:\n",
    "    1. [Simple feature transformations](https://github.com/interpretml/interpret-community/blob/master/notebooks/simple-feature-transformations-explain-local.ipynb)\n",
    "    1. [Advanced feature transformations](https://github.com/interpretml/interpret-community/blob/master/notebooks/advanced-feature-transformations-explain-local.ipynb)\n",
    "1. [Save model explanations via Azure Machine Learning Run History](../run-history/save-retrieve-explanations-run-history.ipynb)\n",
    "1. Inferencing time: deploy a classification model and explainer:\n",
    "    1. [Deploy a locally-trained model and explainer](../scoring-time/train-explain-model-locally-and-deploy.ipynb)\n",
    "    1. [Deploy a locally-trained keras model and explainer](../scoring-time/train-explain-model-keras-locally-and-deploy.ipynb)\n",
    "    1. [Deploy a remotely-trained model and explainer](../scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/explain-model/azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register model and deploy as webservice in ACI\n",
    "\n",
    "Following this notebook, you will:\n",
    "\n",
    " - Learn how to register a model in your Azure Machine Learning Workspace.\n",
    " - Deploy your model as a web service in an Azure Container Instance."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the [configuration notebook](../../../configuration.ipynb) to install the Azure Machine Learning Python SDK and create a workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "\n",
    "\n",
    "# Check core SDK version number.\n",
    "print('SDK version:', azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "\n",
    "Create a [Workspace](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace%28class%29?view=azure-ml-py) object from your persisted configuration."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create workspace"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create trained model\n",
    "\n",
    "For this example, we will train a small model on scikit-learn's [diabetes dataset](https://scikit-learn.org/stable/datasets/index.html#diabetes-dataset). "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "dataset_x, dataset_y = load_diabetes(return_X_y=True)\n",
    "\n",
    "model = Ridge().fit(dataset_x, dataset_y)\n",
    "\n",
    "joblib.dump(model, 'sklearn_regression_model.pkl')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register input and output datasets\n",
    "\n",
    "Here, you will register the data used to create the model in your workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from azureml.core import Dataset\n",
    "\n",
    "\n",
    "np.savetxt('features.csv', dataset_x, delimiter=',')\n",
    "np.savetxt('labels.csv', dataset_y, delimiter=',')\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "datastore.upload_files(files=['./features.csv', './labels.csv'],\n",
    "                       target_path='sklearn_regression/',\n",
    "                       overwrite=True)\n",
    "\n",
    "input_dataset = Dataset.Tabular.from_delimited_files(path=[(datastore, 'sklearn_regression/features.csv')])\n",
    "output_dataset = Dataset.Tabular.from_delimited_files(path=[(datastore, 'sklearn_regression/labels.csv')])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model\n",
    "\n",
    "Register a file or folder as a model by calling [Model.register()](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none-).\n",
    "\n",
    "In addition to the content of the model file itself, your registered model will also store model metadata -- model description, tags, and framework information -- that will be useful when managing and deploying models in your workspace. Using tags, for instance, you can categorize your models and apply filters when listing models in your workspace. Also, marking this model with the scikit-learn framework will simplify deploying it as a web service, as we'll see later."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "register model from file",
     "sample-model-register"
    ]
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "from azureml.core import Model\n",
    "from azureml.core.resource_configuration import ResourceConfiguration\n",
    "\n",
    "\n",
    "model = Model.register(workspace=ws,\n",
    "                       model_name='my-sklearn-model',                # Name of the registered model in your workspace.\n",
    "                       model_path='./sklearn_regression_model.pkl',  # Local file to upload and register as a model.\n",
    "                       model_framework=Model.Framework.SCIKITLEARN,  # Framework used to create the model.\n",
    "                       model_framework_version=sklearn.__version__,  # Version of scikit-learn used to create the model.\n",
    "                       sample_input_dataset=input_dataset,\n",
    "                       sample_output_dataset=output_dataset,\n",
    "                       resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5),\n",
    "                       description='Ridge regression model to predict diabetes progression.',\n",
    "                       tags={'area': 'diabetes', 'type': 'regression'})\n",
    "\n",
    "print('Name:', model.name)\n",
    "print('Version:', model.version)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model\n",
    "\n",
    "Deploy your model as a web service using [Model.deploy()](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py#deploy-workspace--name--models--inference-config--deployment-config-none--deployment-target-none-). Web services take one or more models, load them in an environment, and run them on one of several supported deployment targets. For more information on all your options when deploying models, see the [next steps](#Next-steps) section at the end of this notebook.\n",
    "\n",
    "For this example, we will deploy your scikit-learn model to an Azure Container Instance (ACI)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a default environment (for supported models)\n",
    "\n",
    "The Azure Machine Learning service provides a default environment for supported model frameworks, including scikit-learn, based on the metadata you provided when registering your model. This is the easiest way to deploy your model.\n",
    "\n",
    "Even when you deploy your model to ACI with a default environment you can still customize the deploy configuration (i.e. the number of cores and amount of memory made available for the deployment) using the [AciWebservice.deploy_configuration()](https://docs.microsoft.com/python/api/azureml-core/azureml.core.webservice.aci.aciwebservice#deploy-configuration-cpu-cores-none--memory-gb-none--tags-none--properties-none--description-none--location-none--auth-enabled-none--ssl-enabled-none--enable-app-insights-none--ssl-cert-pem-file-none--ssl-key-pem-file-none--ssl-cname-none--dns-name-label-none--). Look at the \"Use a custom environment\" section of this notebook for more information on deploy configuration.\n",
    "\n",
    "**Note**: This step can take several minutes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_name = 'my-sklearn-service'\n",
    "\n",
    "service = Model.deploy(ws, service_name, [model], overwrite=True)\n",
    "service.wait_for_deployment(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After your model is deployed, perform a call to the web service using [service.run()](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice%28class%29?view=azure-ml-py#run-input-)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "input_payload = json.dumps({\n",
    "    'data': dataset_x[0:2].tolist(),\n",
    "    'method': 'predict'  # If you have a classification model, you can get probabilities by changing this to 'predict_proba'.\n",
    "})\n",
    "\n",
    "output = service.run(input_payload)\n",
    "\n",
    "print(output)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are finished testing your service, clean up the deployment with [service.delete()](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice%28class%29?view=azure-ml-py#delete--)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a custom environment\n",
    "\n",
    "If you want more control over how your model is run, if it uses another framework, or if it has special runtime requirements, you can instead specify your own environment and scoring method. Custom environments can be used for any model you want to deploy.\n",
    "\n",
    "Specify the model's runtime environment by creating an [Environment](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment%28class%29?view=azure-ml-py) object and providing the [CondaDependencies](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py) needed by your model."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "\n",
    "environment = Environment('my-sklearn-environment')\n",
    "environment.python.conda_dependencies = CondaDependencies.create(pip_packages=[\n",
    "    'azureml-defaults',\n",
    "    'inference-schema[numpy-support]',\n",
    "    'joblib',\n",
    "    'numpy',\n",
    "    'scikit-learn=={}'.format(sklearn.__version__)\n",
    "])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using a custom environment, you must also provide Python code for initializing and running your model. An example script is included with this notebook."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('score.py') as f:\n",
    "    print(f.read())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy your model in the custom environment by providing an [InferenceConfig](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.inferenceconfig?view=azure-ml-py) object to [Model.deploy()](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py#deploy-workspace--name--models--inference-config--deployment-config-none--deployment-target-none-). In this case we are also using the [AciWebservice.deploy_configuration()](https://docs.microsoft.com/python/api/azureml-core/azureml.core.webservice.aci.aciwebservice#deploy-configuration-cpu-cores-none--memory-gb-none--tags-none--properties-none--description-none--location-none--auth-enabled-none--ssl-enabled-none--enable-app-insights-none--ssl-cert-pem-file-none--ssl-key-pem-file-none--ssl-cname-none--dns-name-label-none--) method to generate a custom deploy configuration.\n",
    "\n",
    "**Note**: This step can take several minutes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "azuremlexception-remarks-sample",
     "sample-aciwebservice-deploy-config"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "\n",
    "service_name = 'my-custom-env-service'\n",
    "\n",
    "inference_config = InferenceConfig(entry_script='score.py', environment=environment)\n",
    "aci_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)\n",
    "\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name=service_name,\n",
    "                       models=[model],\n",
    "                       inference_config=inference_config,\n",
    "                       deployment_config=aci_config,\n",
    "                       overwrite=True)\n",
    "service.wait_for_deployment(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After your model is deployed, make a call to the web service using [service.run()](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice%28class%29?view=azure-ml-py#run-input-)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_payload = json.dumps({\n",
    "    'data': dataset_x[0:2].tolist()\n",
    "})\n",
    "\n",
    "output = service.run(input_payload)\n",
    "\n",
    "print(output)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are finished testing your service, clean up the deployment with [service.delete()](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice%28class%29?view=azure-ml-py#delete--)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Profiling\n",
    "\n",
    "Profile your model to understand how much CPU and memory the service, created as a result of its deployment, will need. Profiling returns information such as CPU usage, memory usage, and response latency. It also provides a CPU and memory recommendation based on the resource usage. You can profile your model (or more precisely the service built based on your model) on any CPU and/or memory combination where 0.1 <= CPU <= 3.5 and 0.1GB <= memory <= 15GB. If you do not provide a CPU and/or memory requirement, we will test it on the default configuration of 3.5 CPU and 15GB memory.\n",
    "\n",
    "In order to profile your model you will need:\n",
    "- a registered model\n",
    "- an entry script\n",
    "- an inference configuration\n",
    "- a single column tabular dataset, where each row contains a string representing sample request data sent to the service.\n",
    "\n",
    "Please, note that profiling is a long running operation and can take up to 25 minutes depending on the size of the dataset.\n",
    "\n",
    "At this point we only support profiling of services that expect their request data to be a string, for example: string serialized json, text, string serialized image, etc. The content of each row of the dataset (string) will be put into the body of the HTTP request and sent to the service encapsulating the model for scoring.\n",
    "\n",
    "Below is an example of how you can construct an input dataset to profile a service which expects its incoming requests to contain serialized json. In this case we created a dataset based one hundred instances of the same request data. In real world scenarios however, we suggest that you use larger datasets with various inputs, especially if your model resource usage/behavior is input dependent."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to register datasets using the register() method to your workspace so they can be shared with others, reused and referred to by name in your script.\n",
    "You can try get the dataset first to see if it's already registered."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.data import dataset_type_definitions\n",
    "\n",
    "dataset_name='diabetes_sample_request_data'\n",
    "\n",
    "dataset_registered = False\n",
    "try:\n",
    "    sample_request_data = Dataset.get_by_name(workspace = ws, name = dataset_name)\n",
    "    dataset_registered = True\n",
    "except:\n",
    "    print(\"The dataset {} is not registered in workspace yet.\".format(dataset_name))\n",
    "\n",
    "if not dataset_registered:\n",
    "    # create a string that can be utf-8 encoded and\n",
    "    # put in the body of the request\n",
    "    serialized_input_json = json.dumps({\n",
    "        'data': [\n",
    "            [ 0.03807591,  0.05068012,  0.06169621, 0.02187235, -0.0442235,\n",
    "            -0.03482076, -0.04340085, -0.00259226, 0.01990842, -0.01764613]\n",
    "        ]\n",
    "    })\n",
    "    dataset_content = []\n",
    "    for i in range(100):\n",
    "        dataset_content.append(serialized_input_json)\n",
    "    dataset_content = '\\n'.join(dataset_content)\n",
    "    file_name = \"{}.txt\".format(dataset_name)\n",
    "    f = open(file_name, 'w')\n",
    "    f.write(dataset_content)\n",
    "    f.close()\n",
    "\n",
    "    # upload the txt file created above to the Datastore and create a dataset from it\n",
    "    data_store = Datastore.get_default(ws)\n",
    "    data_store.upload_files(['./' + file_name], target_path='sample_request_data')\n",
    "    datastore_path = [(data_store, 'sample_request_data' +'/' + file_name)]\n",
    "    sample_request_data = Dataset.Tabular.from_delimited_files(\n",
    "        datastore_path,\n",
    "        separator='\\n',\n",
    "        infer_column_types=True,\n",
    "        header=dataset_type_definitions.PromoteHeadersBehavior.NO_HEADERS)\n",
    "    sample_request_data = sample_request_data.register(workspace=ws,\n",
    "                                                    name=dataset_name,\n",
    "                                                    create_new_version=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an input dataset we are ready to go ahead with profiling. In this case we are testing the previously introduced sklearn regression model on 1 CPU and 0.5 GB memory. The memory usage and recommendation presented in the result is measured in Gigabytes. The CPU usage and recommendation is measured in CPU cores."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "environment = Environment('my-sklearn-environment')\n",
    "environment.python.conda_dependencies = CondaDependencies.create(pip_packages=[\n",
    "    'azureml-defaults',\n",
    "    'inference-schema[numpy-support]',\n",
    "    'joblib',\n",
    "    'numpy',\n",
    "    'scikit-learn=={}'.format(sklearn.__version__)\n",
    "])\n",
    "inference_config = InferenceConfig(entry_script='score.py', environment=environment)\n",
    "# if cpu and memory_in_gb parameters are not provided\n",
    "# the model will be profiled on default configuration of\n",
    "# 3.5CPU and 15GB memory\n",
    "profile = Model.profile(ws,\n",
    "            'rgrsn-%s' % datetime.now().strftime('%m%d%Y-%H%M%S'),\n",
    "            [model],\n",
    "            inference_config,\n",
    "            input_dataset=sample_request_data,\n",
    "            cpu=1.0,\n",
    "            memory_in_gb=0.5)\n",
    "\n",
    "# profiling is a long running operation and may take up to 25 min\n",
    "profile.wait_for_completion(True)\n",
    "details = profile.get_details()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model packaging\n",
    "\n",
    "If you want to build a Docker image that encapsulates your model and its dependencies, you can use the model packaging option. The output image will be pushed to your workspace's ACR.\n",
    "\n",
    "You must include an Environment object in your inference configuration to use `Model.package()`.\n",
    "\n",
    "```python\n",
    "package = Model.package(ws, [model], inference_config)\n",
    "package.wait_for_creation(show_output=True)  # Or show_output=False to hide the Docker build logs.\n",
    "package.pull()\n",
    "```\n",
    "\n",
    "Instead of a fully-built image, you can also generate a Dockerfile and download all the assets needed to build an image on top of your Environment.\n",
    "\n",
    "```python\n",
    "package = Model.package(ws, [model], inference_config, generate_dockerfile=True)\n",
    "package.wait_for_creation(show_output=True)\n",
    "package.save(\"./local_context_dir\")\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    " - To run a production-ready web service, see the [notebook on deployment to Azure Kubernetes Service](../production-deploy-to-aks/production-deploy-to-aks.ipynb).\n",
    " - To run a local web service, see the [notebook on deployment to a local Docker container](../deploy-to-local/register-model-deploy-local.ipynb).\n",
    " - For more information on datasets, see the [notebook on training with datasets](../../work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb).\n",
    " - For more information on environments, see the [notebook on using environments](../../training/using-environments/using-environments.ipynb).\n",
    " - For information on all the available deployment targets, see [&ldquo;How and where to deploy models&rdquo;](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where#choose-a-compute-target)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/spark/model-register-and-deploy-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/spark/model-register-and-deploy-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register Spark Model and deploy as Webservice\n",
    "\n",
    "This example shows how to deploy a Webservice in step-by-step fashion:\n",
    "\n",
    " 1. Register Spark Model\n",
    " 2. Deploy Spark Model as Webservice"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/spark/model-register-and-deploy-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the [configuration](../../../configuration.ipynb) Notebook first if you haven't."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/spark/model-register-and-deploy-spark.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/spark/model-register-and-deploy-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/spark/model-register-and-deploy-spark.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create workspace"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/spark/model-register-and-deploy-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register Model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/spark/model-register-and-deploy-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add tags and descriptions to your Models. Note you need to have a `iris.model` file in the current directory. This model file is generated using [train in spark](../training/train-in-spark/train-in-spark.ipynb) notebook. The below call registers that file as a Model with the same name `iris.model` in the workspace.\n",
    "\n",
    "Using tags, you can track useful information such as the name and version of the machine learning library used to train the model. Note that tags must be alphanumeric."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/spark/model-register-and-deploy-spark.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "register model from file"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model = Model.register(model_path=\"iris.model\",\n",
    "                       model_name=\"iris.model\",\n",
    "                       tags={'type': \"regression\"},\n",
    "                       description=\"Logistic regression model to predict iris species\",\n",
    "                       workspace=ws)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/spark/model-register-and-deploy-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Environment"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/spark/model-register-and-deploy-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now create and/or use an Environment object when deploying a Webservice. The Environment can have been previously registered with your Workspace, or it will be registered with it as a part of the Webservice deployment.\n",
    "\n",
    "In this notebook, we will be using 'AzureML-PySpark-MmlSpark-0.15', a curated environment.\n",
    "\n",
    "More information can be found in our [using environments notebook](../training/using-environments/using-environments.ipynb)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/spark/model-register-and-deploy-spark.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "env = Environment.get(ws, name='AzureML-PySpark-MmlSpark-0.15')\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/spark/model-register-and-deploy-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Inference Configuration\n",
    "\n",
    "There is now support for a source directory, you can upload an entire folder from your local machine as dependencies for the Webservice.\n",
    "Note: in that case, your entry_script is relative path to the source_directory path.\n",
    "\n",
    "Sample code for using a source directory:\n",
    "\n",
    "```python\n",
    "inference_config = InferenceConfig(source_directory=\"C:/abc\",\n",
    "                                   entry_script=\"x/y/score.py\",\n",
    "                                   environment=environment)\n",
    "```\n",
    "\n",
    " - source_directory = holds source path as string, this entire folder gets added in image so its really easy to access any files within this folder or subfolder\n",
    " - entry_script = contains logic specific to initializing your model and running predictions\n",
    " - environment = An environment object to use for the deployment. Doesn't have to be registered"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/spark/model-register-and-deploy-spark.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create image"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=env)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/spark/model-register-and-deploy-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Model as Webservice on Azure Container Instance\n",
    "\n",
    "Note that the service creation can take few minutes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/spark/model-register-and-deploy-spark.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "azuremlexception-remarks-sample"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "from azureml.exceptions import WebserviceException\n",
    "\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)\n",
    "aci_service_name = 'aciservice1'\n",
    "\n",
    "try:\n",
    "    # if you want to get existing service below is the command\n",
    "    # since aci name needs to be unique in subscription deleting existing aci if any\n",
    "    # we use aci_service_name to create azure aci\n",
    "    service = Webservice(ws, name=aci_service_name)\n",
    "    if service:\n",
    "        service.delete()\n",
    "except WebserviceException as e:\n",
    "    print()\n",
    "\n",
    "service = Model.deploy(ws, aci_service_name, [model], inference_config, deployment_config)\n",
    "\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/spark/model-register-and-deploy-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test web service"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/spark/model-register-and-deploy-spark.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "test_sample = json.dumps({'features':{'type':1,'values':[4.3,3.0,1.1,0.1]},'label':2.0})\n",
    "\n",
    "test_sample_encoded = bytes(test_sample, encoding='utf8')\n",
    "prediction = service.run(input_data=test_sample_encoded)\n",
    "print(prediction)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/spark/model-register-and-deploy-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete ACI to clean up"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/spark/model-register-and-deploy-spark.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "deploy service",
     "aci"
    ]
   },
   "outputs": [],
   "source": [
    "service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/spark/model-register-and-deploy-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Profiling\n",
    "\n",
    "You can also take advantage of the profiling feature to estimate CPU and memory requirements for models.\n",
    "\n",
    "```python\n",
    "profile = Model.profile(ws, \"profilename\", [model], inference_config, test_sample)\n",
    "profile.wait_for_profiling(True)\n",
    "profiling_results = profile.get_results()\n",
    "print(profiling_results)\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/spark/model-register-and-deploy-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Packaging\n",
    "\n",
    "If you want to build a Docker image that encapsulates your model and its dependencies, you can use the model packaging option. The output image will be pushed to your workspace's ACR.\n",
    "\n",
    "You must include an Environment object in your inference configuration to use `Model.package()`.\n",
    "\n",
    "```python\n",
    "package = Model.package(ws, [model], inference_config)\n",
    "package.wait_for_creation(show_output=True)  # Or show_output=False to hide the Docker build logs.\n",
    "package.pull()\n",
    "```\n",
    "\n",
    "Instead of a fully-built image, you can also generate a Dockerfile and download all the assets needed to build an image on top of your Environment.\n",
    "\n",
    "```python\n",
    "package = Model.package(ws, [model], inference_config, generate_dockerfile=True)\n",
    "package.wait_for_creation(show_output=True)\n",
    "package.save(\"./local_context_dir\")\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/spark/model-register-and-deploy-spark.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Multiple Models as Webservice\n",
    "\n",
    "This example shows how to deploy a Webservice with multiple models in step-by-step fashion:\n",
    "\n",
    " 1. Register Models\n",
    " 2. Deploy Models as Webservice"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the [configuration](../../../configuration.ipynb) Notebook first if you haven't."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create workspace"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Models"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will be using and registering two models. \n",
    "\n",
    "First we will train two simple models on the [diabetes dataset](https://scikit-learn.org/stable/datasets/index.html#diabetes-dataset) included with scikit-learn, serializing them to files in the current directory."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import sklearn\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import BayesianRidge, Ridge\n",
    "\n",
    "x, y = load_diabetes(return_X_y=True)\n",
    "\n",
    "first_model = Ridge().fit(x, y)\n",
    "second_model = BayesianRidge().fit(x, y)\n",
    "\n",
    "joblib.dump(first_model, \"first_model.pkl\")\n",
    "joblib.dump(second_model, \"second_model.pkl\")\n",
    "\n",
    "print(\"Trained models using scikit-learn {}.\".format(sklearn.__version__))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our trained models locally, we will register them as Models with the names `my_first_model` and `my_second_model` in the workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "register model from file"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "my_model_1 = Model.register(model_path=\"first_model.pkl\",\n",
    "                            model_name=\"my_first_model\",\n",
    "                            workspace=ws)\n",
    "\n",
    "my_model_2 = Model.register(model_path=\"second_model.pkl\",\n",
    "                            model_name=\"my_second_model\",\n",
    "                            workspace=ws)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the Entry Script\n",
    "Write the script that will be used to predict on your models"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model.get_model_path()\n",
    "\n",
    "To get the paths of your models, use `Model.get_model_path(model_name, version=None, _workspace=None)` method. This method will find the path to a model using the name of the model registered under the workspace.\n",
    "\n",
    "In this example, we do not use the optional arguments `version` and `_workspace`.\n",
    "\n",
    "#### Using environment variable AZUREML_MODEL_DIR\n",
    "\n",
    "In other [examples](../deploy-to-cloud/score.py) with a single model deployment, we use the environment variable `AZUREML_MODEL_DIR` and model file name to get the model path. \n",
    "\n",
    "For single model deployments, this environment variable is the path to the model folder (`./azureml-models/$MODEL_NAME/$VERSION`). When we deploy multiple models, the environment variable is set to the folder containing all models (./azureml-models).\n",
    "\n",
    "If you're using multiple models and you know the versions of the models you deploy, you can use this method to get the model path:\n",
    "\n",
    "```python\n",
    "# Construct the model path using the registered model name, version, and model file name\n",
    "model_1_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'my_first_model', '1', 'first_model.pkl')\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from azureml.core.model import Model\n",
    "\n",
    "def init():\n",
    "    global model_1, model_2\n",
    "    # Here \"my_first_model\" is the name of the model registered under the workspace.\n",
    "    # This call will return the path to the .pkl file on the local disk.\n",
    "    model_1_path = Model.get_model_path(model_name='my_first_model')\n",
    "    model_2_path = Model.get_model_path(model_name='my_second_model')\n",
    "    \n",
    "    # Deserialize the model files back into scikit-learn models.\n",
    "    model_1 = joblib.load(model_1_path)\n",
    "    model_2 = joblib.load(model_2_path)\n",
    "\n",
    "# Note you can pass in multiple rows for scoring.\n",
    "def run(raw_data):\n",
    "    try:\n",
    "        data = json.loads(raw_data)['data']\n",
    "        data = np.array(data)\n",
    "        \n",
    "        # Call predict() on each model\n",
    "        result_1 = model_1.predict(data)\n",
    "        result_2 = model_2.predict(data)\n",
    "\n",
    "        # You can return any JSON-serializable value.\n",
    "        return {\"prediction1\": result_1.tolist(), \"prediction2\": result_2.tolist()}\n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        return result"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Environment"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now create and/or use an Environment object when deploying a Webservice. The Environment can have been previously registered with your Workspace, or it will be registered with it as a part of the Webservice deployment. Please note that your environment must include azureml-defaults with verion >= 1.0.45 as a pip dependency, because it contains the functionality needed to host the model as a web service.\n",
    "\n",
    "More information can be found in our [using environments notebook](../training/using-environments/using-environments.ipynb)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "env = Environment(\"deploytocloudenv\")\n",
    "env.python.conda_dependencies.add_pip_package(\"joblib\")\n",
    "env.python.conda_dependencies.add_pip_package(\"numpy\")\n",
    "env.python.conda_dependencies.add_pip_package(\"scikit-learn=={}\".format(sklearn.__version__))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Inference Configuration\n",
    "\n",
    "There is now support for a source directory, you can upload an entire folder from your local machine as dependencies for the Webservice.\n",
    "Note: in that case, environments's entry_script and file_path are relative paths to the source_directory path; myenv.docker.base_dockerfile is a string containing extra docker steps or contents of the docker file.\n",
    "\n",
    "Sample code for using a source directory:\n",
    "\n",
    "```python\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "myenv = Environment.from_conda_specification(name='myenv', file_path='env/myenv.yml')\n",
    "\n",
    "# explicitly set base_image to None when setting base_dockerfile\n",
    "myenv.docker.base_image = None\n",
    "# add extra docker commends to execute\n",
    "myenv.docker.base_dockerfile = \"FROM ubuntu\\n RUN echo \\\"hello\\\"\"\n",
    "\n",
    "inference_config = InferenceConfig(source_directory=\"C:/abc\",\n",
    "                                   entry_script=\"x/y/score.py\",\n",
    "                                   environment=myenv)\n",
    "```\n",
    "\n",
    " - file_path: input parameter to Environment constructor. Manages conda and python package dependencies.\n",
    " - env.docker.base_dockerfile: any extra steps you want to inject into docker file\n",
    " - source_directory: holds source path as string, this entire folder gets added in image so its really easy to access any files within this folder or subfolder\n",
    " - entry_script: contains logic specific to initializing your model and running predictions"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create image"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=env)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Model as Webservice on Azure Container Instance\n",
    "\n",
    "Note that the service creation can take few minutes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "azuremlexception-remarks-sample"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aci_service_name = \"aciservice-multimodel\"\n",
    "\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)\n",
    "\n",
    "service = Model.deploy(ws, aci_service_name, [my_model_1, my_model_2], inference_config, deployment_config, overwrite=True)\n",
    "service.wait_for_deployment(True)\n",
    "\n",
    "print(service.state)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test web service"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "test_sample = json.dumps({'data': x[0:2].tolist()})\n",
    "\n",
    "prediction = service.run(test_sample)\n",
    "\n",
    "print(prediction)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete ACI to clean up"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "deploy service",
     "aci"
    ]
   },
   "outputs": [],
   "source": [
    "service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks-ssl.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks-ssl.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying a web service to Azure Kubernetes Service (AKS)\n",
    "This notebook shows the steps for deploying a service: registering a model, provisioning a cluster with ssl (one time action), and deploying a service to it. \n",
    "We then test and delete the service, image and model."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks-ssl.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core.compute import AksCompute, ComputeTarget\n",
    "from azureml.core.webservice import Webservice, AksWebservice\n",
    "from azureml.core.model import Model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks-ssl.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "print(azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks-ssl.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get workspace\n",
    "Load existing workspace from the config file info."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks-ssl.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks-ssl.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register the model\n",
    "Register an existing trained model, add descirption and tags."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks-ssl.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Register the model\n",
    "from azureml.core.model import Model\n",
    "model = Model.register(model_path = \"sklearn_regression_model.pkl\", # this points to a local file\n",
    "                       model_name = \"sklearn_model\", # this is the name the model is registered as\n",
    "                       tags = {'area': \"diabetes\", 'type': \"regression\"},\n",
    "                       description = \"Ridge regression model to predict diabetes\",\n",
    "                       workspace = ws)\n",
    "\n",
    "print(model.name, model.description, model.version)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks-ssl.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Environment\n",
    "Create an environment that the model will be deployed with"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks-ssl.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "conda_deps = CondaDependencies.create(conda_packages=['numpy', 'scikit-learn==0.19.1', 'scipy'], pip_packages=['azureml-defaults', 'inference-schema'])\n",
    "myenv = Environment(name='myenv')\n",
    "myenv.python.conda_dependencies = conda_deps"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks-ssl.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a custom Docker image\n",
    "\n",
    "You can also specify a custom Docker image to be used as base image if you don't want to use the default base image provided by Azure ML. Please make sure the custom Docker image has Ubuntu >= 16.04, Conda >= 4.5.\\* and Python(3.5.\\* or 3.6.\\*).\n",
    "\n",
    "Only supported with `python` runtime.\n",
    "```python\n",
    "# use an image available in public Container Registry without authentication\n",
    "myenv.docker.base_image = \"mcr.microsoft.com/azureml/o16n-sample-user-base/ubuntu-miniconda\"\n",
    "\n",
    "# or, use an image available in a private Container Registry\n",
    "myenv.docker.base_image = \"myregistry.azurecr.io/mycustomimage:1.0\"\n",
    "myenv.docker.base_image_registry.address = \"myregistry.azurecr.io\"\n",
    "myenv.docker.base_image_registry.username = \"username\"\n",
    "myenv.docker.base_image_registry.password = \"password\"\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks-ssl.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the Entry Script\n",
    "Write the script that will be used to predict on your model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks-ssl.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score_ssl.py\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import Ridge\n",
    "from inference_schema.schema_decorators import input_schema, output_schema\n",
    "from inference_schema.parameter_types.standard_py_parameter_type import StandardPythonParameterType\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
    "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'sklearn_regression_model.pkl')\n",
    "    # deserialize the model file back into a sklearn model\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "\n",
    "standard_sample_input = {'a': 10, 'b': 9, 'c': 8, 'd': 7, 'e': 6, 'f': 5, 'g': 4, 'h': 3, 'i': 2, 'j': 1 }\n",
    "standard_sample_output = {'outcome': 1}\n",
    "\n",
    "@input_schema('param', StandardPythonParameterType(standard_sample_input))\n",
    "@output_schema(StandardPythonParameterType(standard_sample_output))\n",
    "def run(param):\n",
    "    try:\n",
    "        raw_data = [param['a'], param['b'], param['c'], param['d'], param['e'], param['f'], param['g'], param['h'], param['i'], param['j']]\n",
    "        data = numpy.array([raw_data])\n",
    "        result = model.predict(data)\n",
    "        return { 'outcome' : result[0] }\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        return error"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks-ssl.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the InferenceConfig\n",
    "Create the inference config that will be used when deploying the model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks-ssl.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inf_config = InferenceConfig(entry_script='score_ssl.py', environment=myenv)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks-ssl.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provision the AKS Cluster with SSL\n",
    "This is a one time setup. You can reuse this cluster for multiple deployments after it has been created. If you delete the cluster or the resource group that contains it, then you would have to recreate it.\n",
    "\n",
    "See code snippet below. Check the documentation [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-secure-web-service) for more details"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks-ssl.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the default configuration (can also provide parameters to customize)\n",
    "\n",
    "provisioning_config = AksCompute.provisioning_configuration()\n",
    "# Leaf domain label generates a name using the formula\n",
    "#  \"<leaf-domain-label>######.<azure-region>.cloudapp.azure.net\"\n",
    "#  where \"######\" is a random series of characters\n",
    "provisioning_config.enable_ssl(leaf_domain_label = \"contoso\", overwrite_existing_domain = True)\n",
    "\n",
    "aks_name = 'my-aks-ssl-1' \n",
    "# Create the cluster\n",
    "aks_target = ComputeTarget.create(workspace = ws, \n",
    "                                  name = aks_name, \n",
    "                                  provisioning_configuration = provisioning_config)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks-ssl.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "aks_target.wait_for_completion(show_output = True)\n",
    "print(aks_target.provisioning_state)\n",
    "print(aks_target.provisioning_errors)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks-ssl.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy web service to AKS"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks-ssl.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "sample-deploy-to-aks"
    ]
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "aks_config = AksWebservice.deploy_configuration()\n",
    "\n",
    "aks_service_name ='aks-service-ssl-1'\n",
    "\n",
    "aks_service = Model.deploy(workspace=ws,\n",
    "                           name=aks_service_name,\n",
    "                           models=[model],\n",
    "                           inference_config=inf_config,\n",
    "                           deployment_config=aks_config,\n",
    "                           deployment_target=aks_target,\n",
    "                          overwrite=True)\n",
    "\n",
    "aks_service.wait_for_deployment(show_output = True)\n",
    "print(aks_service.state)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks-ssl.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the web service using run method\n",
    "We test the web sevice by passing data.\n",
    "Run() method retrieves API keys behind the scenes to make sure that call is authenticated."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks-ssl.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "standard_sample_input = json.dumps({'param': {'a': 10, 'b': 9, 'c': 8, 'd': 7, 'e': 6, 'f': 5, 'g': 4, 'h': 3, 'i': 2, 'j': 1 }})\n",
    "\n",
    "aks_service.run(input_data=standard_sample_input)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks-ssl.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "Delete the service, image and model."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks-ssl.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "aks_service.delete()\n",
    "model.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks-ssl.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying a web service to Azure Kubernetes Service (AKS)\n",
    "This notebook shows the steps for deploying a service: registering a model, creating an image, provisioning a cluster (one time action), and deploying a service to it. \n",
    "We then test and delete the service, image and model."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core.compute import AksCompute, ComputeTarget\n",
    "from azureml.core.webservice import Webservice, AksWebservice\n",
    "from azureml.core.model import Model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "print(azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get workspace\n",
    "Load existing workspace from the config file info."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register the model\n",
    "Register an existing trained model, add descirption and tags."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Register the model\n",
    "from azureml.core.model import Model\n",
    "model = Model.register(model_path = \"sklearn_regression_model.pkl\", # this points to a local file\n",
    "                       model_name = \"sklearn_regression_model.pkl\", # this is the name the model is registered as\n",
    "                       tags = {'area': \"diabetes\", 'type': \"regression\"},\n",
    "                       description = \"Ridge regression model to predict diabetes\",\n",
    "                       workspace = ws)\n",
    "\n",
    "print(model.name, model.description, model.version)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Environment\n",
    "Create an environment that the model will be deployed with"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "conda_deps = CondaDependencies.create(conda_packages=['numpy','scikit-learn==0.19.1','scipy'], pip_packages=['azureml-defaults', 'inference-schema'])\n",
    "myenv = Environment(name='myenv')\n",
    "myenv.python.conda_dependencies = conda_deps"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a custom Docker image\n",
    "\n",
    "You can also specify a custom Docker image to be used as base image if you don't want to use the default base image provided by Azure ML. Please make sure the custom Docker image has Ubuntu >= 16.04, Conda >= 4.5.\\* and Python(3.5.\\* or 3.6.\\*).\n",
    "\n",
    "Only supported with `python` runtime.\n",
    "```python\n",
    "# use an image available in public Container Registry without authentication\n",
    "myenv.docker.base_image = \"mcr.microsoft.com/azureml/o16n-sample-user-base/ubuntu-miniconda\"\n",
    "\n",
    "# or, use an image available in a private Container Registry\n",
    "myenv.docker.base_image = \"myregistry.azurecr.io/mycustomimage:1.0\"\n",
    "myenv.docker.base_image_registry.address = \"myregistry.azurecr.io\"\n",
    "myenv.docker.base_image_registry.username = \"username\"\n",
    "myenv.docker.base_image_registry.password = \"password\"\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the Entry Script\n",
    "Write the script that will be used to predict on your model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
    "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'sklearn_regression_model.pkl')\n",
    "    # deserialize the model file back into a sklearn model\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "# note you can pass in multiple rows for scoring\n",
    "def run(raw_data):\n",
    "    try:\n",
    "        data = json.loads(raw_data)['data']\n",
    "        data = numpy.array(data)\n",
    "        result = model.predict(data)\n",
    "        # you can return any data type as long as it is JSON-serializable\n",
    "        return result.tolist()\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        return error"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the InferenceConfig\n",
    "Create the inference config that will be used when deploying the model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inf_config = InferenceConfig(entry_script='score.py', environment=myenv)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Profiling\n",
    "\n",
    "Profile your model to understand how much CPU and memory the service, created as a result of its deployment, will need. Profiling returns information such as CPU usage, memory usage, and response latency. It also provides a CPU and memory recommendation based on the resource usage. You can profile your model (or more precisely the service built based on your model) on any CPU and/or memory combination where 0.1 <= CPU <= 3.5 and 0.1GB <= memory <= 15GB. If you do not provide a CPU and/or memory requirement, we will test it on the default configuration of 3.5 CPU and 15GB memory.\n",
    "\n",
    "In order to profile your model you will need:\n",
    "- a registered model\n",
    "- an entry script\n",
    "- an inference configuration\n",
    "- a single column tabular dataset, where each row contains a string representing sample request data sent to the service.\n",
    "\n",
    "Please, note that profiling is a long running operation and can take up to 25 minutes depending on the size of the dataset.\n",
    "\n",
    "At this point we only support profiling of services that expect their request data to be a string, for example: string serialized json, text, string serialized image, etc. The content of each row of the dataset (string) will be put into the body of the HTTP request and sent to the service encapsulating the model for scoring.\n",
    "\n",
    "Below is an example of how you can construct an input dataset to profile a service which expects its incoming requests to contain serialized json. In this case we created a dataset based one hundred instances of the same request data. In real world scenarios however, we suggest that you use larger datasets with various inputs, especially if your model resource usage/behavior is input dependent."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to register datasets using the register() method to your workspace so they can be shared with others, reused and referred to by name in your script.\n",
    "You can try get the dataset first to see if it's already registered."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from azureml.core import Datastore\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.data import dataset_type_definitions\n",
    "\n",
    "dataset_name='sample_request_data'\n",
    "\n",
    "dataset_registered = False\n",
    "try:\n",
    "    sample_request_data = Dataset.get_by_name(workspace = ws, name = dataset_name)\n",
    "    dataset_registered = True\n",
    "except:\n",
    "    print(\"The dataset {} is not registered in workspace yet.\".format(dataset_name))\n",
    "\n",
    "if not dataset_registered:\n",
    "    input_json = {'data': [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "                        [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]]}\n",
    "    # create a string that can be put in the body of the request\n",
    "    serialized_input_json = json.dumps(input_json)\n",
    "    dataset_content = []\n",
    "    for i in range(100):\n",
    "        dataset_content.append(serialized_input_json)\n",
    "    sample_request_data = '\\n'.join(dataset_content)\n",
    "    file_name = \"{}.txt\".format(dataset_name)\n",
    "    f = open(file_name, 'w')\n",
    "    f.write(sample_request_data)\n",
    "    f.close()\n",
    "\n",
    "    # upload the txt file created above to the Datastore and create a dataset from it\n",
    "    data_store = Datastore.get_default(ws)\n",
    "    data_store.upload_files(['./' + file_name], target_path='sample_request_data')\n",
    "    datastore_path = [(data_store, 'sample_request_data' +'/' + file_name)]\n",
    "    sample_request_data = Dataset.Tabular.from_delimited_files(\n",
    "        datastore_path,\n",
    "        separator='\\n',\n",
    "        infer_column_types=True,\n",
    "        header=dataset_type_definitions.PromoteHeadersBehavior.NO_HEADERS)\n",
    "    sample_request_data = sample_request_data.register(workspace=ws,\n",
    "                                                    name=dataset_name,\n",
    "                                                    create_new_version=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an input dataset we are ready to go ahead with profiling. In this case we are testing the previously introduced sklearn regression model on 1 CPU and 0.5 GB memory. The memory usage and recommendation presented in the result is measured in Gigabytes. The CPU usage and recommendation is measured in CPU cores."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.model import Model, InferenceConfig\n",
    "\n",
    "\n",
    "environment = Environment('my-sklearn-environment')\n",
    "environment.python.conda_dependencies = CondaDependencies.create(pip_packages=[\n",
    "    'azureml-defaults',\n",
    "    'inference-schema[numpy-support]',\n",
    "    'joblib',\n",
    "    'numpy',\n",
    "    'scikit-learn==0.19.1',\n",
    "    'scipy'\n",
    "])\n",
    "inference_config = InferenceConfig(entry_script='score.py', environment=environment)\n",
    "# if cpu and memory_in_gb parameters are not provided\n",
    "# the model will be profiled on default configuration of\n",
    "# 3.5CPU and 15GB memory\n",
    "profile = Model.profile(ws,\n",
    "            'sklearn-%s' % datetime.now().strftime('%m%d%Y-%H%M%S'),\n",
    "            [model],\n",
    "            inference_config,\n",
    "            input_dataset=sample_request_data,\n",
    "            cpu=1.0,\n",
    "            memory_in_gb=0.5)\n",
    "\n",
    "# profiling is a long running operation and may take up to 25 min\n",
    "profile.wait_for_completion(True)\n",
    "details = profile.get_details()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provision the AKS Cluster\n",
    "This is a one time setup. You can reuse this cluster for multiple deployments after it has been created. If you delete the cluster or the resource group that contains it, then you would have to recreate it."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your AKS cluster\n",
    "aks_name = 'my-aks-9' \n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    aks_target = ComputeTarget(workspace=ws, name=aks_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # Use the default configuration (can also provide parameters to customize)\n",
    "    prov_config = AksCompute.provisioning_configuration()\n",
    "\n",
    "    # Create the cluster\n",
    "    aks_target = ComputeTarget.create(workspace = ws, \n",
    "                                    name = aks_name, \n",
    "                                    provisioning_configuration = prov_config)\n",
    "\n",
    "if aks_target.get_status() != \"Succeeded\":\n",
    "    aks_target.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create AKS Cluster in an existing virtual network (optional)\n",
    "See code snippet below. Check the documentation [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-enable-virtual-network#use-azure-kubernetes-service) for more details."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azureml.core.compute import ComputeTarget, AksCompute\n",
    "\n",
    "# # Create the compute configuration and set virtual network information\n",
    "# config = AksCompute.provisioning_configuration(location=\"eastus2\")\n",
    "# config.vnet_resourcegroup_name = \"mygroup\"\n",
    "# config.vnet_name = \"mynetwork\"\n",
    "# config.subnet_name = \"default\"\n",
    "# config.service_cidr = \"10.0.0.0/16\"\n",
    "# config.dns_service_ip = \"10.0.0.10\"\n",
    "# config.docker_bridge_cidr = \"172.17.0.1/16\"\n",
    "\n",
    "# # Create the compute target\n",
    "# aks_target = ComputeTarget.create(workspace = ws,\n",
    "#                                   name = \"myaks\",\n",
    "#                                   provisioning_configuration = config)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enable SSL on the AKS Cluster (optional)\n",
    "See code snippet below. Check the documentation [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-secure-web-service) for more details"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provisioning_config = AksCompute.provisioning_configuration(ssl_cert_pem_file=\"cert.pem\", ssl_key_pem_file=\"key.pem\", ssl_cname=\"www.contoso.com\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "aks_target.wait_for_completion(show_output = True)\n",
    "print(aks_target.provisioning_state)\n",
    "print(aks_target.provisioning_errors)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional step: Attach existing AKS cluster\n",
    "\n",
    "If you have existing AKS cluster in your Azure subscription, you can attach it to the Workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use the default configuration (can also provide parameters to customize)\n",
    "# resource_id = '/subscriptions/92c76a2f-0e1c-4216-b65e-abf7a3f34c1e/resourcegroups/raymondsdk0604/providers/Microsoft.ContainerService/managedClusters/my-aks-0605d37425356b7d01'\n",
    "\n",
    "# create_name='my-existing-aks' \n",
    "# # Create the cluster\n",
    "# attach_config = AksCompute.attach_configuration(resource_id=resource_id)\n",
    "# aks_target = ComputeTarget.attach(workspace=ws, name=create_name, attach_configuration=attach_config)\n",
    "# # Wait for the operation to complete\n",
    "# aks_target.wait_for_completion(True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy web service to AKS"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "sample-deploy-to-aks"
    ]
   },
   "outputs": [],
   "source": [
    "# Set the web service configuration (using default here)\n",
    "aks_config = AksWebservice.deploy_configuration()\n",
    "\n",
    "# # Enable token auth and disable (key) auth on the webservice\n",
    "# aks_config = AksWebservice.deploy_configuration(token_auth_enabled=True, auth_enabled=False)\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "sample-deploy-to-aks"
    ]
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "aks_service_name ='aks-service-1'\n",
    "\n",
    "aks_service = Model.deploy(workspace=ws,\n",
    "                           name=aks_service_name,\n",
    "                           models=[model],\n",
    "                           inference_config=inf_config,\n",
    "                           deployment_config=aks_config,\n",
    "                           deployment_target=aks_target)\n",
    "\n",
    "aks_service.wait_for_deployment(show_output = True)\n",
    "print(aks_service.state)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the web service using run method\n",
    "We test the web sevice by passing data.\n",
    "Run() method retrieves API keys behind the scenes to make sure that call is authenticated."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "test_sample = json.dumps({'data': [\n",
    "    [1,2,3,4,5,6,7,8,9,10], \n",
    "    [10,9,8,7,6,5,4,3,2,1]\n",
    "]})\n",
    "test_sample = bytes(test_sample,encoding = 'utf8')\n",
    "\n",
    "prediction = aks_service.run(input_data = test_sample)\n",
    "print(prediction)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the web service using raw HTTP request (optional)\n",
    "Alternatively you can construct a raw HTTP request and send it to the service. In this case you need to explicitly pass the HTTP header. This process is shown in the next 2 cells."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if (key) auth is enabled, retrieve the API keys. AML generates two keys.\n",
    "# key1, Key2 = aks_service.get_keys()\n",
    "# print(key1)\n",
    "\n",
    "# # if token auth is enabled, retrieve the token.\n",
    "# access_token, refresh_after = aks_service.get_token()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct raw HTTP request and send to the service\n",
    "# %%time\n",
    "\n",
    "# import requests\n",
    "\n",
    "# import json\n",
    "\n",
    "# test_sample = json.dumps({'data': [\n",
    "#     [1,2,3,4,5,6,7,8,9,10], \n",
    "#     [10,9,8,7,6,5,4,3,2,1]\n",
    "# ]})\n",
    "# test_sample = bytes(test_sample,encoding = 'utf8')\n",
    "\n",
    "# # If (key) auth is enabled, don't forget to add key to the HTTP header.\n",
    "# headers = {'Content-Type':'application/json', 'Authorization': 'Bearer ' + key1}\n",
    "\n",
    "# # If token auth is enabled, don't forget to add token to the HTTP header.\n",
    "# headers = {'Content-Type':'application/json', 'Authorization': 'Bearer ' + access_token}\n",
    "\n",
    "# resp = requests.post(aks_service.scoring_uri, test_sample, headers=headers)\n",
    "\n",
    "\n",
    "# print(\"prediction:\", resp.text)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "Delete the service, image and model."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "aks_service.delete()\n",
    "model.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/deployment/onnx/onnx-model-register-and-deploy.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register ONNX model and deploy as webservice\n",
    "\n",
    "Following this notebook, you will:\n",
    "\n",
    " - Learn how to register an ONNX in your Azure Machine Learning Workspace.\n",
    " - Deploy your model as a web service in an Azure Container Instance."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the [configuration notebook](../../../configuration.ipynb) to install the Azure Machine Learning Python SDK and create a workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "\n",
    "# Check core SDK version number.\n",
    "print('SDK version:', azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "\n",
    "Create a [Workspace](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace%28class%29?view=azure-ml-py) object from your persisted configuration."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create workspace"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model\n",
    "\n",
    "Register a file or folder as a model by calling [Model.register()](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none-). For this example, we have provided a trained ONNX MNIST model(`mnist-model.onnx` in the notebook's directory).\n",
    "\n",
    "In addition to the content of the model file itself, your registered model will also store model metadata -- model description, tags, and framework information -- that will be useful when managing and deploying models in your workspace. Using tags, for instance, you can categorize your models and apply filters when listing models in your workspace. Also, marking this model with the scikit-learn framework will simplify deploying it as a web service, as we'll see later."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "register model from file"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "model = Model.register(workspace=ws,\n",
    "                       model_name='mnist-sample',                  # Name of the registered model in your workspace.\n",
    "                       model_path='mnist-model.onnx',              # Local ONNX model to upload and register as a model.\n",
    "                       model_framework=Model.Framework.ONNX ,      # Framework used to create the model.\n",
    "                       model_framework_version='1.3',              # Version of ONNX used to create the model.\n",
    "                       description='Onnx MNIST model')\n",
    "\n",
    "print('Name:', model.name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model\n",
    "\n",
    "Deploy your model as a web service using [Model.deploy()](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py#deploy-workspace--name--models--inference-config--deployment-config-none--deployment-target-none-). Web services take one or more models, load them in an environment, and run them on one of several supported deployment targets.\n",
    "\n",
    "For this example, we will deploy the ONNX model to an Azure Container Instance (ACI)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a default environment (for supported models)\n",
    "\n",
    "The Azure Machine Learning service provides a default environment for supported model frameworks, including ONNX, based on the metadata you provided when registering your model. This is the easiest way to deploy your model.\n",
    "\n",
    "**Note**: This step can take several minutes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Webservice\n",
    "from azureml.exceptions import WebserviceException\n",
    "\n",
    "service_name = 'onnx-mnist-service'\n",
    "\n",
    "# Remove any existing service under the same name.\n",
    "try:\n",
    "    Webservice(ws, service_name).delete()\n",
    "except WebserviceException:\n",
    "    pass\n",
    "\n",
    "service = Model.deploy(ws, service_name, [model])\n",
    "service.wait_for_deployment(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After your model is deployed, perform a call to the web service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {'Content-Type': 'application/json', 'Accept': 'application/json'}\n",
    "\n",
    "if service.auth_enabled:\n",
    "    headers['Authorization'] = 'Bearer '+ service.get_keys()[0]\n",
    "elif service.token_auth_enabled:\n",
    "    headers['Authorization'] = 'Bearer '+ service.get_token()[0]\n",
    "\n",
    "scoring_uri = service.scoring_uri\n",
    "print(scoring_uri)\n",
    "with open('onnx-mnist-predict-input.json', 'rb') as data_file:\n",
    "    response = requests.post(\n",
    "        scoring_uri, data=data_file, headers=headers)\n",
    "print(response.status_code)\n",
    "print(response.elapsed)\n",
    "print(response.json())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are finished testing your service, clean up the deployment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-model-register-and-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Real-time Object Detection using ONNX on AzureML\n",
    "\n",
    "This example shows how to convert the TinyYOLO model from CoreML to ONNX and operationalize it as a web service using Azure Machine Learning services and the ONNX Runtime.\n",
    "\n",
    "## What is ONNX\n",
    "ONNX is an open format for representing machine learning and deep learning models. ONNX enables open and interoperable AI by enabling data scientists and developers to use the tools of their choice without worrying about lock-in and flexibility to deploy to a variety of platforms. ONNX is developed and supported by a community of partners including Microsoft, Facebook, and Amazon. For more information, explore the [ONNX website](http://onnx.ai).\n",
    "\n",
    "## YOLO Details\n",
    "You Only Look Once (YOLO) is a state-of-the-art, real-time object detection system. For more information about YOLO, please visit the [YOLO website](https://pjreddie.com/darknet/yolo/)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To make the best use of your time, make sure you have done the following:\n",
    "\n",
    "* Understand the [architecture and terms](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning\n",
    "* If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration](../../../configuration.ipynb) notebook to:\n",
    "    * install the AML SDK\n",
    "    * create a workspace and its configuration file (config.json)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install necessary packages\n",
    "\n",
    "You'll need to run the following commands to use this tutorial:\n",
    "\n",
    "```sh\n",
    "pip install onnxmltools\n",
    "pip install coremltools  # use this on Linux and Mac\n",
    "pip install git+https://github.com/apple/coremltools  # use this on Windows\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert model to ONNX\n",
    "\n",
    "First we download the CoreML model. We use the CoreML model from [Matthijs Hollemans's tutorial](https://github.com/hollance/YOLO-CoreML-MPSNNGraph). This may take a few minutes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "coreml_model_url = \"https://github.com/hollance/YOLO-CoreML-MPSNNGraph/raw/master/TinyYOLO-CoreML/TinyYOLO-CoreML/TinyYOLO.mlmodel\"\n",
    "urllib.request.urlretrieve(coreml_model_url, filename=\"TinyYOLO.mlmodel\")\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use ONNXMLTools to convert the model."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxmltools\n",
    "import coremltools\n",
    "\n",
    "# Load a CoreML model\n",
    "coreml_model = coremltools.utils.load_spec('TinyYOLO.mlmodel')\n",
    "\n",
    "# Convert from CoreML into ONNX\n",
    "onnx_model = onnxmltools.convert_coreml(coreml_model, 'TinyYOLOv2')\n",
    "\n",
    "# Fix the preprocessor bias in the ImageScaler\n",
    "for init in onnx_model.graph.initializer:\n",
    "    if init.name == 'scalerPreprocessor_bias':\n",
    "        init.dims[1] = 1\n",
    "\n",
    "# Save ONNX model\n",
    "onnxmltools.utils.save_model(onnx_model, 'tinyyolov2.onnx')\n",
    "\n",
    "import os\n",
    "print(os.path.getsize('tinyyolov2.onnx'))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying as a web service with Azure ML\n",
    "\n",
    "### Load Azure ML workspace\n",
    "\n",
    "We begin by instantiating a workspace object from the existing workspace created earlier in the configuration notebook."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering your model with Azure ML\n",
    "\n",
    "Now we upload the model and register it in the workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model = Model.register(model_path = \"tinyyolov2.onnx\",\n",
    "                       model_name = \"tinyyolov2\",\n",
    "                       tags = {\"onnx\": \"demo\"},\n",
    "                       description = \"TinyYOLO\",\n",
    "                       workspace = ws)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying your registered models\n",
    "\n",
    "You can optionally list out all the models that you have registered in this workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ws.models\n",
    "for name, m in models.items():\n",
    "    print(\"Name:\", name,\"\\tVersion:\", m.version, \"\\tDescription:\", m.description, m.tags)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write scoring file\n",
    "\n",
    "We are now going to deploy our ONNX model on Azure ML using the ONNX Runtime. We begin by writing a score.py file that will be invoked by the web service call. The `init()` function is called once when the container is started so we load the model using the ONNX Runtime into a global session object."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "from azureml.core.model import Model\n",
    "import numpy as np    # we're going to use numpy to process input and output data\n",
    "import onnxruntime    # to inference ONNX models, we use the ONNX Runtime\n",
    "\n",
    "def init():\n",
    "    global session\n",
    "    model = Model.get_model_path(model_name = 'tinyyolov2')\n",
    "    session = onnxruntime.InferenceSession(model)\n",
    "\n",
    "def preprocess(input_data_json):\n",
    "    # convert the JSON data into the tensor input\n",
    "    return np.array(json.loads(input_data_json)['data']).astype('float32')\n",
    "\n",
    "def postprocess(result):\n",
    "    return np.array(result).tolist()\n",
    "\n",
    "def run(input_data_json):\n",
    "    try:\n",
    "        start = time.time()   # start timer\n",
    "        input_data = preprocess(input_data_json)\n",
    "        input_name = session.get_inputs()[0].name  # get the id of the first input of the model   \n",
    "        result = session.run([], {input_name: input_data})\n",
    "        end = time.time()     # stop timer\n",
    "        return {\"result\": postprocess(result),\n",
    "                \"time\": end - start}\n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        return {\"error\": result}"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up inference configuration\n",
    "First we create a YAML file that specifies which dependencies we would like to see in our container. Please note that you must include azureml-defaults with verion >= 1.0.45 as a pip dependency, because it contains the functionality needed to host the model as a web service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "myenv = CondaDependencies.create(pip_packages=[\"numpy\", \"onnxruntime\", \"azureml-core\", \"azureml-defaults\"])\n",
    "\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create the inference configuration."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "\n",
    "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n",
    "                                               memory_gb = 1, \n",
    "                                               tags = {'demo': 'onnx'}, \n",
    "                                               description = 'web service for TinyYOLO ONNX model')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will take a few minutes to run as the model gets packaged up and deployed to ACI."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_service_name = 'my-aci-service-tiny-yolo'\n",
    "print(\"Service\", aci_service_name)\n",
    "aci_service = Model.deploy(ws, aci_service_name, [model], inference_config, aciconfig)\n",
    "aci_service.wait_for_deployment(True)\n",
    "print(aci_service.state)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case the deployment fails, you can check the logs. Make sure to delete your aci_service before trying again."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if aci_service.state != 'Healthy':\n",
    "    # run this command for debugging.\n",
    "    print(aci_service.get_logs())\n",
    "    aci_service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success!\n",
    "\n",
    "If you've made it this far, you've deployed a working web service that does object detection using an ONNX model. You can get the URL for the webservice with the code below."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aci_service.scoring_uri)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are eventually done using the web service, remember to delete it."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-convert-aml-deploy-tinyyolo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet50 Image Classification using ONNX and AzureML\n",
    "\n",
    "This example shows how to deploy the ResNet50 ONNX model as a web service using Azure Machine Learning services and the ONNX Runtime.\n",
    "\n",
    "## What is ONNX\n",
    "ONNX is an open format for representing machine learning and deep learning models. ONNX enables open and interoperable AI by enabling data scientists and developers to use the tools of their choice without worrying about lock-in and flexibility to deploy to a variety of platforms. ONNX is developed and supported by a community of partners including Microsoft, Facebook, and Amazon. For more information, explore the [ONNX website](http://onnx.ai).\n",
    "\n",
    "## ResNet50 Details\n",
    "ResNet classifies the major object in an input image into a set of 1000 pre-defined classes. For more information about the ResNet50 model and how it was created can be found on the [ONNX Model Zoo github](https://github.com/onnx/models/tree/master/vision/classification/resnet). "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To make the best use of your time, make sure you have done the following:\n",
    "\n",
    "* Understand the [architecture and terms](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning\n",
    "* If you are using an Azure Machine Learning Notebook VM, you are all set.  Otherwise, go through the [configuration notebook](../../../configuration.ipynb) to:\n",
    "    * install the AML SDK\n",
    "    * create a workspace and its configuration file (config.json)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download pre-trained ONNX model from ONNX Model Zoo.\n",
    "\n",
    "Download the [ResNet50v2 model and test data](https://s3.amazonaws.com/onnx-model-zoo/resnet/resnet50v2/resnet50v2.tar.gz) and extract it in the same folder as this tutorial notebook.\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "onnx_model_url = \"https://s3.amazonaws.com/onnx-model-zoo/resnet/resnet50v2/resnet50v2.tar.gz\"\n",
    "urllib.request.urlretrieve(onnx_model_url, filename=\"resnet50v2.tar.gz\")\n",
    "\n",
    "!tar xvzf resnet50v2.tar.gz"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying as a web service with Azure ML"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your Azure ML workspace\n",
    "\n",
    "We begin by instantiating a workspace object from the existing workspace created earlier in the configuration notebook."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register your model with Azure ML\n",
    "\n",
    "Now we upload the model and register it in the workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model = Model.register(model_path = \"resnet50v2/resnet50v2.onnx\",\n",
    "                       model_name = \"resnet50v2\",\n",
    "                       tags = {\"onnx\": \"demo\"},\n",
    "                       description = \"ResNet50v2 from ONNX Model Zoo\",\n",
    "                       workspace = ws)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying your registered models\n",
    "\n",
    "You can optionally list out all the models that you have registered in this workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ws.models\n",
    "for name, m in models.items():\n",
    "    print(\"Name:\", name,\"\\tVersion:\", m.version, \"\\tDescription:\", m.description, m.tags)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write scoring file\n",
    "\n",
    "We are now going to deploy our ONNX model on Azure ML using the ONNX Runtime. We begin by writing a score.py file that will be invoked by the web service call. The `init()` function is called once when the container is started so we load the model using the ONNX Runtime into a global session object."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import numpy as np    # we're going to use numpy to process input and output data\n",
    "import onnxruntime    # to inference ONNX models, we use the ONNX Runtime\n",
    "\n",
    "def softmax(x):\n",
    "    x = x.reshape(-1)\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def init():\n",
    "    global session\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
    "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\n",
    "    model = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'resnet50v2.onnx')\n",
    "    session = onnxruntime.InferenceSession(model, None)\n",
    "\n",
    "def preprocess(input_data_json):\n",
    "    # convert the JSON data into the tensor input\n",
    "    img_data = np.array(json.loads(input_data_json)['data']).astype('float32')\n",
    "    \n",
    "    #normalize\n",
    "    mean_vec = np.array([0.485, 0.456, 0.406])\n",
    "    stddev_vec = np.array([0.229, 0.224, 0.225])\n",
    "    norm_img_data = np.zeros(img_data.shape).astype('float32')\n",
    "    for i in range(img_data.shape[0]):\n",
    "        norm_img_data[i,:,:] = (img_data[i,:,:]/255 - mean_vec[i]) / stddev_vec[i]\n",
    "\n",
    "    return norm_img_data\n",
    "\n",
    "def postprocess(result):\n",
    "    return softmax(np.array(result)).tolist()\n",
    "\n",
    "def run(input_data_json):\n",
    "    try:\n",
    "        start = time.time()\n",
    "        # load in our data which is expected as NCHW 224x224 image\n",
    "        input_data = preprocess(input_data_json)\n",
    "        input_name = session.get_inputs()[0].name  # get the id of the first input of the model   \n",
    "        result = session.run([], {input_name: input_data})\n",
    "        end = time.time()     # stop timer\n",
    "        return {\"result\": postprocess(result),\n",
    "                \"time\": end - start}\n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        return {\"error\": result}"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create inference configuration"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a YAML file that specifies which dependencies we would like to see in our container."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "\n",
    "myenv = CondaDependencies.create(pip_packages=[\"numpy\", \"onnxruntime\", \"azureml-core\", \"azureml-defaults\"])\n",
    "\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the inference configuration object. Please note that you must indicate azureml-defaults with verion >= 1.0.45 as a pip dependency, because it contains the functionality needed to host the model as a web service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "\n",
    "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n",
    "                                               memory_gb = 1, \n",
    "                                               tags = {'demo': 'onnx'}, \n",
    "                                               description = 'web service for ResNet50 ONNX model')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will likely take a few minutes to run as well."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "aci_service_name = 'onnx-demo-resnet50'+str(randint(0,100))\n",
    "print(\"Service\", aci_service_name)\n",
    "aci_service = Model.deploy(ws, aci_service_name, [model], inference_config, aciconfig)\n",
    "aci_service.wait_for_deployment(True)\n",
    "print(aci_service.state)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case the deployment fails, you can check the logs. Make sure to delete your aci_service before trying again."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if aci_service.state != 'Healthy':\n",
    "    # run this command for debugging.\n",
    "    print(aci_service.get_logs())\n",
    "    aci_service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success!\n",
    "\n",
    "If you've made it this far, you've deployed a working web service that does image classification using an ONNX model. You can get the URL for the webservice with the code below."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aci_service.scoring_uri)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are eventually done using the web service, remember to delete it."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-modelzoo-aml-deploy-resnet50.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Digit Classification (MNIST) using ONNX Runtime on Azure ML\n",
    "\n",
    "This example shows how to deploy an image classification neural network using the Modified National Institute of Standards and Technology ([MNIST](http://yann.lecun.com/exdb/mnist/)) dataset and Open Neural Network eXchange format ([ONNX](http://aka.ms/onnxdocarticle)) on the Azure Machine Learning platform. MNIST is a popular dataset consisting of 70,000 grayscale images. Each image is a handwritten digit of 28x28 pixels, representing number from 0 to 9. This tutorial will show you how to deploy a MNIST model from the [ONNX model zoo](https://github.com/onnx/models), use it to make predictions using ONNX Runtime Inference, and deploy it as a web service in Azure.\n",
    "\n",
    "Throughout this tutorial, we will be referring to ONNX, a neural network exchange format used to represent deep learning models. With ONNX, AI developers can more easily move models between state-of-the-art tools (CNTK, PyTorch, Caffe, MXNet, TensorFlow) and choose the combination that is best for them. ONNX is developed and supported by a community of partners including Microsoft AI, Facebook, and Amazon. For more information, explore the [ONNX website](http://onnx.ai) and [open source files](https://github.com/onnx).\n",
    "\n",
    "[ONNX Runtime](https://aka.ms/onnxruntime-python) is the runtime engine that enables evaluation of trained machine learning (Traditional ML and Deep Learning) models with high performance and low resource utilization.\n",
    "\n",
    "#### Tutorial Objectives:\n",
    "\n",
    "- Describe the MNIST dataset and pretrained Convolutional Neural Net ONNX model, stored in the ONNX model zoo.\n",
    "- Deploy and run the pretrained MNIST ONNX model on an Azure Machine Learning instance\n",
    "- Predict labels for test set data points in the cloud using ONNX Runtime and Azure ML"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### 1. Install Azure ML SDK and create a new workspace\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, please follow [Azure ML configuration notebook](../../../configuration.ipynb) to set up your environment.\n",
    "\n",
    "### 2. Install additional packages needed for this tutorial notebook\n",
    "You need to install the popular plotting library `matplotlib`, the image manipulation library `opencv`, and the `onnx` library in the conda environment where Azure Maching Learning SDK is installed. \n",
    "\n",
    "```sh\n",
    "(myenv) $ pip install matplotlib onnx opencv-python\n",
    "```\n",
    "\n",
    "**Debugging tip**: Make sure that you run the \"jupyter notebook\" command to launch this notebook after activating your virtual environment. Choose the respective Python kernel for your new virtual environment using the `Kernel > Change Kernel` menu above. If you have completed the steps correctly, the upper right corner of your screen should state `Python [conda env:myenv]` instead of `Python [default]`.\n",
    "\n",
    "### 3. Download sample data and pre-trained ONNX model from ONNX Model Zoo.\n",
    "\n",
    "In the following lines of code, we download [the trained ONNX MNIST model and corresponding test data](https://github.com/onnx/models/tree/master/vision/classification/mnist) and place them in the same folder as this tutorial notebook. For more information about the MNIST dataset, please visit [Yan LeCun's website](http://yann.lecun.com/exdb/mnist/)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urllib is a built-in Python library to download files from URLs\n",
    "\n",
    "# Objective: retrieve the latest version of the ONNX MNIST model files from the\n",
    "# ONNX Model Zoo and save it in the same folder as this tutorial\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "onnx_model_url = \"https://github.com/onnx/models/blob/master/vision/classification/mnist/model/mnist-7.tar.gz?raw=true\"\n",
    "\n",
    "urllib.request.urlretrieve(onnx_model_url, filename=\"mnist-7.tar.gz\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the ! magic command tells our jupyter notebook kernel to run the following line of \n",
    "# code from the command line instead of the notebook kernel\n",
    "\n",
    "# We use tar and xvcf to unzip the files we just retrieved from the ONNX model zoo\n",
    "\n",
    "!tar xvzf mnist-7.tar.gz"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a VM with your ONNX model in the Cloud\n",
    "\n",
    "### Load Azure ML workspace\n",
    "\n",
    "We begin by instantiating a workspace object from the existing workspace created earlier in the configuration notebook."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering your model with Azure ML"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"mnist\" # replace this with the location of your model files\n",
    "\n",
    "# leave as is if it's in the same folder as this notebook"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model = Model.register(workspace = ws,\n",
    "                       model_path = model_dir + \"/\" + \"model.onnx\",\n",
    "                       model_name = \"mnist_1\",\n",
    "                       tags = {\"onnx\": \"demo\"},\n",
    "                       description = \"MNIST image classification CNN from ONNX Model Zoo\",)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Displaying your registered models\n",
    "\n",
    "This step is not required, so feel free to skip it."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ws.models\n",
    "for name, m in models.items():\n",
    "    print(\"Name:\", name,\"\\tVersion:\", m.version, \"\\tDescription:\", m.description, m.tags)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c3f2f57c-7454-4d3e-b38d-b0946cf066ea"
    }
   },
   "source": [
    "### ONNX MNIST Model Methodology\n",
    "\n",
    "The image classification model we are using is pre-trained using Microsoft's deep learning cognitive toolkit, [CNTK](https://github.com/Microsoft/CNTK), from the [ONNX model zoo](http://github.com/onnx/models). The model zoo has many other models that can be deployed on cloud providers like AzureML without any additional training. To ensure that our cloud deployed model works, we use testing data from the famous MNIST data set, provided as part of the [trained MNIST model](https://github.com/onnx/models/tree/master/vision/classification/mnist) in the ONNX model zoo.\n",
    "\n",
    "***Input: Handwritten Images from MNIST Dataset***\n",
    "\n",
    "***Task: Classify each MNIST image into an appropriate digit***\n",
    "\n",
    "***Output: Digit prediction for input image***\n",
    "\n",
    "Run the cell below to look at some of the sample images from the MNIST dataset that we used to train this ONNX model. Remember, once the application is deployed in Azure ML, you can use your own images as input for the model to classify!"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for images and plots in this notebook\n",
    "import matplotlib.pyplot as plt  \n",
    "from IPython.display import Image\n",
    "\n",
    "# display images inline\n",
    "%matplotlib inline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url=\"http://3.bp.blogspot.com/_UpN7DfJA0j4/TJtUBWPk0SI/AAAAAAAAABY/oWPMtmqJn3k/s1600/mnist_originals.png\", width=200, height=200)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify our Score and Environment Files"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to deploy our ONNX Model on AML with inference in ONNX Runtime. We begin by writing a score.py file, which will help us run the model in our Azure ML virtual machine (VM), and then specify our environment by writing a yml file. You will also notice that we import the onnxruntime library to do runtime inference on our ONNX models (passing in input and evaluating out model's predicted output). More information on the API and commands can be found in the [ONNX Runtime documentation](https://aka.ms/onnxruntime).\n",
    "\n",
    "### Write Score File\n",
    "\n",
    "A score file is what tells our Azure cloud service what to do. After initializing our model using azureml.core.model, we start an ONNX Runtime inference session to evaluate the data passed in on our function calls."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import json\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def init():\n",
    "    global session, input_name, output_name\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
    "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\n",
    "    model = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'model.onnx')\n",
    "    session = onnxruntime.InferenceSession(model, None)\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name \n",
    "    \n",
    "\n",
    "def preprocess(input_data_json):\n",
    "    # convert the JSON data into the tensor input\n",
    "    return np.array(json.loads(input_data_json)['data']).astype('float32')\n",
    "\n",
    "def postprocess(result):\n",
    "    # We use argmax to pick the highest confidence label\n",
    "    return int(np.argmax(np.array(result).squeeze(), axis=0))\n",
    "    \n",
    "def run(input_data):\n",
    "\n",
    "    try:\n",
    "        # load in our data, convert to readable format\n",
    "        data = preprocess(input_data)\n",
    "        \n",
    "        # start timer\n",
    "        start = time.time()\n",
    "        \n",
    "        r = session.run([output_name], {input_name: data})\n",
    "        \n",
    "        #end timer\n",
    "        end = time.time()\n",
    "        \n",
    "        result = postprocess(r)\n",
    "        result_dict = {\"result\": result,\n",
    "                      \"time_in_sec\": end - start}\n",
    "    except Exception as e:\n",
    "        result_dict = {\"error\": str(e)}\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "def choose_class(result_prob):\n",
    "    \"\"\"We use argmax to determine the right label to choose from our output\"\"\"\n",
    "    return int(np.argmax(result_prob, axis=0))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Environment File\n",
    "\n",
    "This step creates a YAML environment file that specifies which dependencies we would like to see in our Linux Virtual Machine. Please note that you must indicate azureml-defaults with verion >= 1.0.45 as a pip dependency, because it contains the functionality needed to host the model as a web service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "myenv = CondaDependencies.create(pip_packages=[\"numpy\", \"onnxruntime\", \"azureml-core\", \"azureml-defaults\"])\n",
    "\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Inference Configuration"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "\n",
    "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n",
    "                                               memory_gb = 1, \n",
    "                                               tags = {'demo': 'onnx'}, \n",
    "                                               description = 'ONNX for mnist model')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will likely take a few minutes to run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_service_name = 'onnx-demo-mnist'\n",
    "print(\"Service\", aci_service_name)\n",
    "aci_service = Model.deploy(ws, aci_service_name, [model], inference_config, aciconfig)\n",
    "aci_service.wait_for_deployment(True)\n",
    "print(aci_service.state)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if aci_service.state != 'Healthy':\n",
    "    # run this command for debugging.\n",
    "    print(aci_service.get_logs())\n",
    "\n",
    "    # If your deployment fails, make sure to delete your aci_service or rename your service before trying again!\n",
    "    # aci_service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Success!\n",
    "\n",
    "If you've made it this far, you've deployed a working VM with a handwritten digit classifier running in the cloud using Azure ML. Congratulations!\n",
    "\n",
    "You can get the URL for the webservice with the code below. Let's now see how well our model deals with our test images."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aci_service.scoring_uri)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Evaluation\n",
    "\n",
    "### Load Test Data\n",
    "\n",
    "These are already in your directory from your ONNX model download (from the model zoo).\n",
    "\n",
    "Notice that our Model Zoo files have a .pb extension. This is because they are [protobuf files (Protocol Buffers)](https://developers.google.com/protocol-buffers/docs/pythontutorial), so we need to read in our data through our ONNX TensorProto reader into a format we can work with, like numerical arrays."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to manipulate our arrays\n",
    "import numpy as np \n",
    "\n",
    "# read in test data protobuf files included with the model\n",
    "import onnx\n",
    "from onnx import numpy_helper\n",
    "\n",
    "# to use parsers to read in our model/data\n",
    "import json\n",
    "import os\n",
    "\n",
    "test_inputs = []\n",
    "test_outputs = []\n",
    "\n",
    "# read in 3 testing images from .pb files\n",
    "test_data_size = 3\n",
    "\n",
    "for i in np.arange(test_data_size):\n",
    "    input_test_data = os.path.join(model_dir, 'test_data_set_{0}'.format(i), 'input_0.pb')\n",
    "    output_test_data = os.path.join(model_dir, 'test_data_set_{0}'.format(i), 'output_0.pb')\n",
    "    \n",
    "    # convert protobuf tensors to np arrays using the TensorProto reader from ONNX\n",
    "    tensor = onnx.TensorProto()\n",
    "    with open(input_test_data, 'rb') as f:\n",
    "        tensor.ParseFromString(f.read())\n",
    "    \n",
    "    input_data = numpy_helper.to_array(tensor)\n",
    "    test_inputs.append(input_data)\n",
    "    \n",
    "    with open(output_test_data, 'rb') as f:\n",
    "        tensor.ParseFromString(f.read())\n",
    "    \n",
    "    output_data = numpy_helper.to_array(tensor)\n",
    "    test_outputs.append(output_data)\n",
    "    \n",
    "if len(test_inputs) == test_data_size:\n",
    "    print('Test data loaded successfully.')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c3f2f57c-7454-4d3e-b38d-b0946cf066ea"
    }
   },
   "source": [
    "### Show some sample images\n",
    "We use `matplotlib` to plot 3 test images from the dataset."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "396d478b-34aa-4afa-9898-cdce8222a516"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 6))\n",
    "for test_image in np.arange(3):\n",
    "    plt.subplot(1, 15, test_image+1)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    plt.imshow(test_inputs[test_image].reshape(28, 28), cmap = plt.cm.Greys)\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluation / prediction"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 6))\n",
    "plt.subplot(1, 8, 1)\n",
    "\n",
    "plt.text(x = 0, y = -30, s = \"True Label: \", fontsize = 13, color = 'black')\n",
    "plt.text(x = 0, y = -20, s = \"Result: \", fontsize = 13, color = 'black')\n",
    "plt.text(x = 0, y = -10, s = \"Inference Time: \", fontsize = 13, color = 'black')\n",
    "plt.text(x = 3, y = 14, s = \"Model Input\", fontsize = 12, color = 'black')\n",
    "plt.text(x = 6, y = 18, s = \"(28 x 28)\", fontsize = 12, color = 'black')\n",
    "plt.imshow(np.ones((28,28)), cmap=plt.cm.Greys)    \n",
    "\n",
    "\n",
    "for i in np.arange(test_data_size):\n",
    "    \n",
    "    input_data = json.dumps({'data': test_inputs[i].tolist()})\n",
    "    \n",
    "    # predict using the deployed model\n",
    "    r = aci_service.run(input_data)\n",
    "    \n",
    "    if \"error\" in r:\n",
    "        print(r['error'])\n",
    "        break\n",
    "        \n",
    "    result = r['result']\n",
    "    time_ms = np.round(r['time_in_sec'] * 1000, 2)\n",
    "    \n",
    "    ground_truth = int(np.argmax(test_outputs[i]))\n",
    "    \n",
    "    # compare actual value vs. the predicted values:\n",
    "    plt.subplot(1, 8, i+2)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "\n",
    "    # use different color for misclassified sample\n",
    "    font_color = 'red' if ground_truth != result else 'black'\n",
    "    clr_map = plt.cm.gray if ground_truth != result else plt.cm.Greys\n",
    "\n",
    "    # ground truth labels are in blue\n",
    "    plt.text(x = 10, y = -30, s = ground_truth, fontsize = 18, color = 'blue')\n",
    "    \n",
    "    # predictions are in black if correct, red if incorrect\n",
    "    plt.text(x = 10, y = -20, s = result, fontsize = 18, color = font_color)\n",
    "    plt.text(x = 5, y = -10, s = str(time_ms) + ' ms', fontsize = 14, color = font_color)\n",
    "\n",
    "    \n",
    "    plt.imshow(test_inputs[i].reshape(28, 28), cmap = clr_map)\n",
    "\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try classifying your own images!\n",
    "\n",
    "Create your own handwritten image and pass it into the model."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions take your image and format it so it can be passed\n",
    "# as input into our ONNX model\n",
    "\n",
    "import cv2\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    \"\"\"Convert the input image into grayscale\"\"\"\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "def resize_img(img_to_resize):\n",
    "    \"\"\"Resize image to MNIST model input dimensions\"\"\"\n",
    "    r_img = cv2.resize(img_to_resize, dsize=(28, 28), interpolation=cv2.INTER_AREA)\n",
    "    r_img.resize((1, 1, 28, 28))\n",
    "    return r_img\n",
    "\n",
    "def preprocess(img_to_preprocess):\n",
    "    \"\"\"Resize input images and convert them to grayscale.\"\"\"\n",
    "    if img_to_preprocess.shape == (28, 28):\n",
    "        img_to_preprocess.resize((1, 1, 28, 28))\n",
    "        return img_to_preprocess\n",
    "    \n",
    "    grayscale = rgb2gray(img_to_preprocess)\n",
    "    processed_img = resize_img(grayscale)\n",
    "    return processed_img"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this string with your own path/test image\n",
    "# Make sure your image is square and the dimensions are equal (i.e. 100 * 100 pixels or 28 * 28 pixels)\n",
    "\n",
    "# Any PNG or JPG image file should work\n",
    "\n",
    "your_test_image = \"<path to file>\"\n",
    "\n",
    "# e.g. your_test_image = \"C:/Users/vinitra.swamy/Pictures/handwritten_digit.png\"\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "if your_test_image != \"<path to file>\":\n",
    "    img = mpimg.imread(your_test_image)\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(img, cmap = plt.cm.Greys)\n",
    "    print(\"Old Dimensions: \", img.shape)\n",
    "    img = preprocess(img)\n",
    "    print(\"New Dimensions: \", img.shape)\n",
    "else:\n",
    "    img = None"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if img is None:\n",
    "    print(\"Add the path for your image data.\")\n",
    "else:\n",
    "    input_data = json.dumps({'data': img.tolist()})\n",
    "\n",
    "    try:\n",
    "        r = aci_service.run(input_data)\n",
    "        result = r['result']\n",
    "        time_ms = np.round(r['time_in_sec'] * 1000, 2)\n",
    "    except KeyError as e:\n",
    "        print(str(e))\n",
    "\n",
    "    plt.figure(figsize = (16, 6))\n",
    "    plt.subplot(1, 15,1)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    plt.text(x = -100, y = -20, s = \"Model prediction: \", fontsize = 14)\n",
    "    plt.text(x = -100, y = -10, s = \"Inference time: \", fontsize = 14)\n",
    "    plt.text(x = 0, y = -20, s = str(result), fontsize = 14)\n",
    "    plt.text(x = 0, y = -10, s = str(time_ms) + \" ms\", fontsize = 14)\n",
    "    plt.text(x = -100, y = 14, s = \"Input image: \", fontsize = 14)\n",
    "    plt.imshow(img.reshape(28, 28), cmap = plt.cm.gray)    "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: How does our  ONNX MNIST model work? \n",
    "#### A brief explanation of Convolutional Neural Networks\n",
    "\n",
    "A [convolutional neural network](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNN, or ConvNet) is a type of [feed-forward](https://en.wikipedia.org/wiki/Feedforward_neural_network) artificial neural network made up of neurons that have learnable weights and biases. The CNNs take advantage of the spatial nature of the data. In nature, we perceive different objects by their shapes, size and colors. For example, objects in a natural scene are typically edges, corners/vertices (defined by two of more edges), color patches etc. These primitives are often identified using different detectors (e.g., edge detection, color detector) or combination of detectors interacting to facilitate image interpretation (object classification, region of interest detection, scene description etc.) in real world vision related tasks. These detectors are also known as filters. Convolution is a mathematical operator that takes an image and a filter as input and produces a filtered output (representing say edges, corners, or colors in the input image).  \n",
    "\n",
    "Historically, these filters are a set of weights that were often hand crafted or modeled with mathematical functions (e.g., [Gaussian](https://en.wikipedia.org/wiki/Gaussian_filter) / [Laplacian](http://homepages.inf.ed.ac.uk/rbf/HIPR2/log.htm) / [Canny](https://en.wikipedia.org/wiki/Canny_edge_detector) filter).  The filter outputs are mapped through non-linear activation functions mimicking human brain cells called [neurons](https://en.wikipedia.org/wiki/Neuron). Popular deep CNNs or ConvNets (such as [AlexNet](https://en.wikipedia.org/wiki/AlexNet), [VGG](https://arxiv.org/abs/1409.1556), [Inception](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf), [ResNet](https://arxiv.org/pdf/1512.03385v1.pdf)) that are used for various [computer vision](https://en.wikipedia.org/wiki/Computer_vision) tasks have many of these architectural primitives (inspired from biology).  \n",
    "\n",
    "### Convolution Layer\n",
    "\n",
    "A convolution layer is a set of filters. Each filter is defined by a weight (**W**) matrix, and  bias ($b$).\n",
    "\n",
    "These filters are scanned across the image performing the dot product between the weights and corresponding input value ($x$). The bias value is added to the output of the dot product and the resulting sum is optionally mapped through an activation function."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Description\n",
    "\n",
    "The MNIST model from the ONNX Model Zoo uses maxpooling to update the weights in its convolutions, summarized by the graphic below. You can see the entire workflow of our pre-trained model in the following image, with our input images and our output probabilities of each of our 10 labels. If you're interested in exploring the logic behind creating a Deep Learning model further, please look at the [training tutorial for our ONNX MNIST Convolutional Neural Network](https://github.com/Microsoft/CNTK/blob/master/Tutorials/CNTK_103D_MNIST_ConvolutionalNeuralNetwork.ipynb). "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember to delete your service after you are done using it!\n",
    "\n",
    "aci_service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations!\n",
    "\n",
    "In this tutorial, you have:\n",
    "- familiarized yourself with ONNX Runtime inference and the pretrained models in the ONNX model zoo\n",
    "- understood a state-of-the-art convolutional neural net image classification model (MNIST in ONNX) and deployed it in Azure ML cloud\n",
    "- ensured that your deep learning model is working perfectly (in the cloud) on test data, and checked it against some of your own!\n",
    "\n",
    "Next steps:\n",
    "- Check out another interesting application based on a Microsoft Research computer vision paper that lets you set up a [facial emotion recognition model](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb) in the cloud! This tutorial deploys a pre-trained ONNX Computer Vision model in an Azure ML virtual machine.\n",
    "- Contribute to our [open source ONNX repository on github](http://github.com/onnx/onnx) and/or add to our [ONNX model zoo](http://github.com/onnx/models)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Handwritten Digit Classification using ONNX and AzureML\n",
    "\n",
    "This example shows how to train a model on the MNIST data using PyTorch, save it as an ONNX model, and deploy it as a web service using Azure Machine Learning services and the ONNX Runtime.\n",
    "\n",
    "## What is ONNX\n",
    "ONNX is an open format for representing machine learning and deep learning models. ONNX enables open and interoperable AI by enabling data scientists and developers to use the tools of their choice without worrying about lock-in and flexibility to deploy to a variety of platforms. ONNX is developed and supported by a community of partners including Microsoft, Facebook, and Amazon. For more information, explore the [ONNX website](http://onnx.ai).\n",
    "\n",
    "## MNIST Details\n",
    "The Modified National Institute of Standards and Technology (MNIST) dataset consists of 70,000 grayscale images. Each image is a handwritten digit of 28x28 pixels, representing numbers from 0 to 9. For more information about the MNIST dataset, please visit [Yan LeCun's website](http://yann.lecun.com/exdb/mnist/). For more information about the MNIST model and how it was created can be found on the [ONNX Model Zoo github](https://github.com/onnx/models/tree/master/vision/classification/mnist). "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* Understand the [architecture and terms](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning\n",
    "* If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration notebook](../../../configuration.ipynb) to:\n",
    "    * install the AML SDK\n",
    "    * create a workspace and its configuration file (`config.json`)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "### Create a remote compute target\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) to execute your training script on. In this tutorial, you create an [Azure Batch AI](https://docs.microsoft.com/azure/batch-ai/overview) cluster as your training compute resource. This code creates a cluster for you if it does not already exist in your workspace.\n",
    "\n",
    "**Creation of the cluster takes approximately 5 minutes.** If the cluster is already in your workspace this code will skip the cluster creation process."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"gpu-cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', \n",
    "                                                                max_nodes=6)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# Use the 'status' property to get a detailed status for the current cluster. \n",
    "print(compute_target.status.serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code creates a GPU cluster. If you instead want to create a CPU cluster, provide a different VM size to the `vm_size` parameter, such as `STANDARD_D2_V2`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a project directory\n",
    "Create a directory that will contain all the necessary code from your local machine that you will need access to on the remote resource. This includes the training script and any additional files your training script depends on."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "project_folder = './pytorch-mnist'\n",
    "os.makedirs(project_folder, exist_ok=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the training script [`mnist.py`](mnist.py) into your project directory. Make sure the training script has the following code to create an ONNX file:\n",
    "```python\n",
    "dummy_input = torch.randn(1, 1, 28, 28, device=device)\n",
    "model_path = os.path.join(output_dir, 'mnist.onnx')\n",
    "torch.onnx.export(model, dummy_input, model_path)\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.copy('mnist.py', project_folder)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiment\n",
    "Create an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment) to track all the runs in your workspace for this transfer learning PyTorch tutorial. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'pytorch1-mnist'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a PyTorch estimator\n",
    "The AML SDK's PyTorch estimator enables you to easily submit PyTorch training jobs for both single-node and distributed runs. For more information on the PyTorch estimator, refer [here](https://docs.microsoft.com/azure/machine-learning/service/how-to-train-pytorch). The following code will define a single-node PyTorch job."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import PyTorch\n",
    "\n",
    "estimator = PyTorch(source_directory=project_folder, \n",
    "                    script_params={'--output-dir': './outputs'},\n",
    "                    compute_target=compute_target,\n",
    "                    entry_script='mnist.py',\n",
    "                    use_gpu=True)\n",
    "\n",
    "# upgrade to PyTorch 1.0 Preview, which has better support for ONNX\n",
    "estimator.conda_dependencies.remove_conda_package('pytorch=0.4.0')\n",
    "estimator.conda_dependencies.add_conda_package('pytorch-nightly')\n",
    "estimator.conda_dependencies.add_channel('pytorch')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `script_params` parameter is a dictionary containing the command-line arguments to your training script `entry_script`. Please note the following:\n",
    "- We specified the output directory as `./outputs`. The `outputs` directory is specially treated by AML in that all the content in this directory gets uploaded to your workspace as part of your run history. The files written to this directory are therefore accessible even once your remote run is over. In this tutorial, we will save our trained model to this output directory.\n",
    "\n",
    "To leverage the Azure VM's GPU for training, we set `use_gpu=True`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job\n",
    "Run your experiment by submitting your estimator object. Note that this call is asynchronous."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(estimator)\n",
    "print(run.get_details())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor your run\n",
    "You can monitor the progress of the run with a Jupyter widget. Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can block until the script has completed training before running more code."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the model (optional)\n",
    "\n",
    "Once the run completes, you can choose to download the ONNX model."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the files from the run\n",
    "run.get_file_names()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('outputs', 'mnist.onnx')\n",
    "run.download_file(model_path, output_file_path=model_path)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the model\n",
    "You can also register the model from your run to your workspace. The `model_path` parameter takes in the relative path on the remote VM to the model file in your `outputs` directory. You can then deploy this registered model as a web service through the AML SDK."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run.register_model(model_name='mnist', model_path=model_path)\n",
    "print(model.name, model.id, model.version, sep = '\\t')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying your registered models (optional)\n",
    "\n",
    "You can optionally list out all the models that you have registered in this workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ws.models\n",
    "for name, m in models.items():\n",
    "    print(\"Name:\", name,\"\\tVersion:\", m.version, \"\\tDescription:\", m.description, m.tags)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying as a web service\n",
    "\n",
    "### Write scoring file\n",
    "\n",
    "We are now going to deploy our ONNX model on Azure ML using the ONNX Runtime. We begin by writing a score.py file that will be invoked by the web service call. The `init()` function is called once when the container is started so we load the model using the ONNX Runtime into a global session object."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "from azureml.core.model import Model\n",
    "import numpy as np    # we're going to use numpy to process input and output data\n",
    "import onnxruntime    # to inference ONNX models, we use the ONNX Runtime\n",
    "\n",
    "def init():\n",
    "    global session\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
    "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\n",
    "    model = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'mnist.onnx')\n",
    "    session = onnxruntime.InferenceSession(model)\n",
    "\n",
    "def preprocess(input_data_json):\n",
    "    # convert the JSON data into the tensor input\n",
    "    return np.array(json.loads(input_data_json)['data']).astype('float32')\n",
    "\n",
    "def postprocess(result):\n",
    "    # We use argmax to pick the highest confidence label\n",
    "    return int(np.argmax(np.array(result).squeeze(), axis=0))\n",
    "\n",
    "def run(input_data_json):\n",
    "    try:\n",
    "        start = time.time()   # start timer\n",
    "        input_data = preprocess(input_data_json)\n",
    "        input_name = session.get_inputs()[0].name  # get the id of the first input of the model   \n",
    "        result = session.run([], {input_name: input_data})\n",
    "        end = time.time()     # stop timer\n",
    "        return {\"result\": postprocess(result),\n",
    "                \"time\": end - start}\n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        return {\"error\": result}"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create inference configuration\n",
    "First we create a YAML file that specifies which dependencies we would like to see in our container. Please note that you must indicate azureml-defaults with verion >= 1.0.45 as a pip dependency, because it contains the functionality needed to host the model as a web service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "myenv = CondaDependencies.create(pip_packages=[\"numpy\",\"onnxruntime\",\"azureml-core\", \"azureml-defaults\"])\n",
    "\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we setup the inference configuration "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "\n",
    "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n",
    "                                               memory_gb = 1, \n",
    "                                               tags = {'demo': 'onnx'}, \n",
    "                                               description = 'web service for MNIST ONNX model')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will likely take a few minutes to run as well."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "from random import randint\n",
    "\n",
    "aci_service_name = 'onnx-demo-mnist'+str(randint(0,100))\n",
    "print(\"Service\", aci_service_name)\n",
    "aci_service = Model.deploy(ws, aci_service_name, [model], inference_config, aciconfig)\n",
    "aci_service.wait_for_deployment(True)\n",
    "print(aci_service.state)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case the deployment fails, you can check the logs. Make sure to delete your aci_service before trying again."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if aci_service.state != 'Healthy':\n",
    "    # run this command for debugging.\n",
    "    print(aci_service.get_logs())\n",
    "    aci_service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success!\n",
    "\n",
    "If you've made it this far, you've deployed a working web service that does handwritten digit classification using an ONNX model. You can get the URL for the webservice with the code below."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aci_service.scoring_uri)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are eventually done using the web service, remember to delete it."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Expression Recognition (FER+) using ONNX Runtime on Azure ML\n",
    "\n",
    "This example shows how to deploy an image classification neural network using the Facial Expression Recognition ([FER](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data)) dataset and Open Neural Network eXchange format ([ONNX](http://aka.ms/onnxdocarticle)) on the Azure Machine Learning platform. This tutorial will show you how to deploy a FER+ model from the [ONNX model zoo](https://github.com/onnx/models), use it to make predictions using ONNX Runtime Inference, and deploy it as a web service in Azure.\n",
    "\n",
    "Throughout this tutorial, we will be referring to ONNX, a neural network exchange format used to represent deep learning models. With ONNX, AI developers can more easily move models between state-of-the-art tools (CNTK, PyTorch, Caffe, MXNet, TensorFlow) and choose the combination that is best for them. ONNX is developed and supported by a community of partners including Microsoft AI, Facebook, and Amazon. For more information, explore the [ONNX website](http://onnx.ai) and [open source files](https://github.com/onnx).\n",
    "\n",
    "[ONNX Runtime](https://aka.ms/onnxruntime-python) is the runtime engine that enables evaluation of trained machine learning (Traditional ML and Deep Learning) models with high performance and low resource utilization. We use the CPU version of ONNX Runtime in this tutorial, but will soon be releasing an additional tutorial for deploying this model using ONNX Runtime GPU.\n",
    "\n",
    "#### Tutorial Objectives:\n",
    "\n",
    "1. Describe the FER+ dataset and pretrained Convolutional Neural Net ONNX model for Emotion Recognition, stored in the ONNX model zoo.\n",
    "2. Deploy and run the pretrained FER+ ONNX model on an Azure Machine Learning instance\n",
    "3. Predict labels for test set data points in the cloud using ONNX Runtime and Azure ML"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### 1. Install Azure ML SDK and create a new workspace\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, please follow [Azure ML configuration notebook](../../../configuration.ipynb) to set up your environment.\n",
    "\n",
    "### 2. Install additional packages needed for this Notebook\n",
    "You need to install the popular plotting library `matplotlib`, the image manipulation library `opencv`, and the `onnx` library in the conda environment where Azure Maching Learning SDK is installed.\n",
    "\n",
    "```sh\n",
    "(myenv) $ pip install matplotlib onnx opencv-python\n",
    "```\n",
    "\n",
    "**Debugging tip**: Make sure that to activate your virtual environment (myenv) before you re-launch this notebook using the `jupyter notebook` comand. Choose the respective Python kernel for your new virtual environment using the `Kernel > Change Kernel` menu above. If you have completed the steps correctly, the upper right corner of your screen should state `Python [conda env:myenv]` instead of `Python [default]`.\n",
    "\n",
    "### 3. Download sample data and pre-trained ONNX model from ONNX Model Zoo.\n",
    "\n",
    "In the following lines of code, we download [the trained ONNX Emotion FER+ model and corresponding test data](https://github.com/onnx/models/tree/master/vision/body_analysis/emotion_ferplus) and place them in the same folder as this tutorial notebook. For more information about the FER+ dataset, please visit Microsoft Researcher Emad Barsoum's [FER+ source data repository](https://github.com/ebarsoum/FERPlus)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urllib is a built-in Python library to download files from URLs\n",
    "\n",
    "# Objective: retrieve the latest version of the ONNX Emotion FER+ model files from the\n",
    "# ONNX Model Zoo and save it in the same folder as this tutorial\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "onnx_model_url = \"https://github.com/onnx/models/blob/master/vision/body_analysis/emotion_ferplus/model/emotion-ferplus-7.tar.gz?raw=true\"\n",
    "\n",
    "urllib.request.urlretrieve(onnx_model_url, filename=\"emotion-ferplus-7.tar.gz\")\n",
    "\n",
    "# the ! magic command tells our jupyter notebook kernel to run the following line of \n",
    "# code from the command line instead of the notebook kernel\n",
    "\n",
    "# We use tar and xvcf to unzip the files we just retrieved from the ONNX model zoo\n",
    "\n",
    "!tar xvzf emotion-ferplus-7.tar.gz"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a VM with your ONNX model in the Cloud\n",
    "\n",
    "### Load Azure ML workspace\n",
    "\n",
    "We begin by instantiating a workspace object from the existing workspace created earlier in the configuration notebook."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering your model with Azure ML"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"emotion_ferplus\" # replace this with the location of your model files\n",
    "\n",
    "# leave as is if it's in the same folder as this notebook"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model = Model.register(model_path = model_dir + \"/\" + \"model.onnx\",\n",
    "                       model_name = \"onnx_emotion\",\n",
    "                       tags = {\"onnx\": \"demo\"},\n",
    "                       description = \"FER+ emotion recognition CNN from ONNX Model Zoo\",\n",
    "                       workspace = ws)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Displaying your registered models\n",
    "\n",
    "This step is not required, so feel free to skip it."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ws.models\n",
    "for name, m in models.items():\n",
    "    print(\"Name:\", name,\"\\tVersion:\", m.version, \"\\tDescription:\", m.description, m.tags)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX FER+ Model Methodology\n",
    "\n",
    "The image classification model we are using is pre-trained using Microsoft's deep learning cognitive toolkit, [CNTK](https://github.com/Microsoft/CNTK), from the [ONNX model zoo](http://github.com/onnx/models). The model zoo has many other models that can be deployed on cloud providers like AzureML without any additional training. To ensure that our cloud deployed model works, we use testing data from the well-known FER+ data set, provided as part of the [trained Emotion Recognition model](https://github.com/onnx/models/tree/master/vision/body_analysis/emotion_ferplus) in the ONNX model zoo.\n",
    "\n",
    "The original Facial Emotion Recognition (FER) Dataset was released in 2013 by Pierre-Luc Carrier and Aaron Courville as part of a [Kaggle Competition](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data), but some of the labels are not entirely appropriate for the expression. In the FER+ Dataset, each photo was evaluated by at least 10 croud sourced reviewers, creating a more accurate basis for ground truth. \n",
    "\n",
    "You can see the difference of label quality in the sample model input below. The FER labels are the first word below each image, and the FER+ labels are the second word below each image.\n",
    "\n",
    "![](https://raw.githubusercontent.com/Microsoft/FERPlus/master/FER+vsFER.png)\n",
    "\n",
    "***Input: Photos of cropped faces from FER+ Dataset***\n",
    "\n",
    "***Task: Classify each facial image into its appropriate emotions in the emotion table***\n",
    "\n",
    "```    emotion_table = {'neutral':0, 'happiness':1, 'surprise':2, 'sadness':3, 'anger':4, 'disgust':5, 'fear':6, 'contempt':7} ```\n",
    "\n",
    "***Output: Emotion prediction for input image***\n",
    "\n",
    "\n",
    "Remember, once the application is deployed in Azure ML, you can use your own images as input for the model to classify."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for images and plots in this notebook\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "# display images inline\n",
    "%matplotlib inline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Description\n",
    "\n",
    "The FER+ model from the ONNX Model Zoo is summarized by the graphic below. You can see the entire workflow of our pre-trained model in the following image from Barsoum et. al's paper [\"Training Deep Networks for Facial Expression Recognition\n",
    "with Crowd-Sourced Label Distribution\"](https://arxiv.org/pdf/1608.01041.pdf), with our (64 x 64) input images and our output probabilities for each of the labels."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/vinitra/FERPlus/master/emotion_model_img.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify our Score and Environment Files"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to deploy our ONNX Model on AML with inference in ONNX Runtime. We begin by writing a score.py file, which will help us run the model in our Azure ML virtual machine (VM), and then specify our environment by writing a yml file. You will also notice that we import the onnxruntime library to do runtime inference on our ONNX models (passing in input and evaluating out model's predicted output). More information on the API and commands can be found in the [ONNX Runtime documentation](https://aka.ms/onnxruntime).\n",
    "\n",
    "### Write Score File\n",
    "\n",
    "A score file is what tells our Azure cloud service what to do. After initializing our model using azureml.core.model, we start an ONNX Runtime inference session to evaluate the data passed in on our function calls."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import json\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "def init():\n",
    "    global session, input_name, output_name\n",
    "    model = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'model.onnx')\n",
    "    session = onnxruntime.InferenceSession(model, None)\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name \n",
    "    \n",
    "def run(input_data):\n",
    "    '''Purpose: evaluate test input in Azure Cloud using onnxruntime.\n",
    "        We will call the run function later from our Jupyter Notebook \n",
    "        so our azure service can evaluate our model input in the cloud. '''\n",
    "\n",
    "    try:\n",
    "        # load in our data, convert to readable format\n",
    "        data = np.array(json.loads(input_data)['data']).astype('float32')\n",
    "        \n",
    "        start = time.time()\n",
    "        r = session.run([output_name], {input_name : data})\n",
    "        end = time.time()\n",
    "        \n",
    "        result = emotion_map(postprocess(r[0]))\n",
    "        \n",
    "        result_dict = {\"result\": result,\n",
    "                      \"time_in_sec\": [end - start]}\n",
    "    except Exception as e:\n",
    "        result_dict = {\"error\": str(e)}\n",
    "    \n",
    "    return json.dumps(result_dict)\n",
    "\n",
    "def emotion_map(classes, N=1):\n",
    "    \"\"\"Take the most probable labels (output of postprocess) and returns the \n",
    "    top N emotional labels that fit the picture.\"\"\"\n",
    "    \n",
    "    emotion_table = {'neutral':0, 'happiness':1, 'surprise':2, 'sadness':3, \n",
    "                     'anger':4, 'disgust':5, 'fear':6, 'contempt':7}\n",
    "    \n",
    "    emotion_keys = list(emotion_table.keys())\n",
    "    emotions = []\n",
    "    for i in range(N):\n",
    "        emotions.append(emotion_keys[classes[i]])\n",
    "    return emotions\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values (probabilities from 0 to 1) for each possible label.\"\"\"\n",
    "    x = x.reshape(-1)\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def postprocess(scores):\n",
    "    \"\"\"This function takes the scores generated by the network and \n",
    "    returns the class IDs in decreasing order of probability.\"\"\"\n",
    "    prob = softmax(scores)\n",
    "    prob = np.squeeze(prob)\n",
    "    classes = np.argsort(prob)[::-1]\n",
    "    return classes"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Environment File\n",
    "Please note that you must indicate azureml-defaults with verion >= 1.0.45 as a pip dependency, because it contains the functionality needed to host the model as a web service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "\n",
    "myenv = CondaDependencies.create(pip_packages=[\"numpy\", \"onnxruntime\", \"azureml-core\", \"azureml-defaults\"])\n",
    "\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup inference configuration"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "\n",
    "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n",
    "                                               memory_gb = 1, \n",
    "                                               tags = {'demo': 'onnx'}, \n",
    "                                               description = 'ONNX for emotion recognition model')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will likely take a few minutes to run as well."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_service_name = 'onnx-demo-emotion'\n",
    "print(\"Service\", aci_service_name)\n",
    "aci_service = Model.deploy(ws, aci_service_name, [model], inference_config, aciconfig)\n",
    "aci_service.wait_for_deployment(True)\n",
    "print(aci_service.state)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if aci_service.state != 'Healthy':\n",
    "    # run this command for debugging.\n",
    "    print(aci_service.get_logs())\n",
    "\n",
    "    # If your deployment fails, make sure to delete your aci_service before trying again!\n",
    "    # aci_service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Success!\n",
    "\n",
    "If you've made it this far, you've deployed a working VM with a facial emotion recognition model running in the cloud using Azure ML. Congratulations!\n",
    "\n",
    "Let's see how well our model deals with our test images."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Evaluation\n",
    "\n",
    "### Useful Helper Functions\n",
    "\n",
    "We preprocess and postprocess our data (see score.py file) using the helper functions specified in the [ONNX FER+ Model page in the Model Zoo repository](https://github.com/onnx/models/tree/master/vision/body_analysis/emotion_ferplus)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_map(classes, N=1):\n",
    "    \"\"\"Take the most probable labels (output of postprocess) and returns the \n",
    "    top N emotional labels that fit the picture.\"\"\"\n",
    "    \n",
    "    emotion_table = {'neutral':0, 'happiness':1, 'surprise':2, 'sadness':3, \n",
    "                     'anger':4, 'disgust':5, 'fear':6, 'contempt':7}\n",
    "    \n",
    "    emotion_keys = list(emotion_table.keys())\n",
    "    emotions = []\n",
    "    for c in range(N):\n",
    "        emotions.append(emotion_keys[classes[c]])\n",
    "    return emotions\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values (probabilities from 0 to 1) for each possible label.\"\"\"\n",
    "    x = x.reshape(-1)\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def postprocess(scores):\n",
    "    \"\"\"This function takes the scores generated by the network and \n",
    "    returns the class IDs in decreasing order of probability.\"\"\"\n",
    "    prob = softmax(scores)\n",
    "    prob = np.squeeze(prob)\n",
    "    classes = np.argsort(prob)[::-1]\n",
    "    return classes"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test Data\n",
    "\n",
    "These are already in your directory from your ONNX model download (from the model zoo).\n",
    "\n",
    "Notice that our Model Zoo files have a .pb extension. This is because they are [protobuf files (Protocol Buffers)](https://developers.google.com/protocol-buffers/docs/pythontutorial), so we need to read in our data through our ONNX TensorProto reader into a format we can work with, like numerical arrays."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to manipulate our arrays\n",
    "import numpy as np \n",
    "\n",
    "# read in test data protobuf files included with the model\n",
    "import onnx\n",
    "from onnx import numpy_helper\n",
    "\n",
    "# to use parsers to read in our model/data\n",
    "import json\n",
    "import os\n",
    "\n",
    "test_inputs = []\n",
    "test_outputs = []\n",
    "\n",
    "# read in 3 testing images from .pb files\n",
    "test_data_size = 3\n",
    "\n",
    "for num in np.arange(test_data_size):\n",
    "    input_test_data = os.path.join(model_dir, 'test_data_set_{0}'.format(num), 'input_0.pb')\n",
    "    output_test_data = os.path.join(model_dir, 'test_data_set_{0}'.format(num), 'output_0.pb')\n",
    "    \n",
    "    # convert protobuf tensors to np arrays using the TensorProto reader from ONNX\n",
    "    tensor = onnx.TensorProto()\n",
    "    with open(input_test_data, 'rb') as f:\n",
    "        tensor.ParseFromString(f.read())\n",
    "    \n",
    "    input_data = numpy_helper.to_array(tensor)\n",
    "    test_inputs.append(input_data)\n",
    "    \n",
    "    with open(output_test_data, 'rb') as f:\n",
    "        tensor.ParseFromString(f.read())\n",
    "    \n",
    "    output_data = numpy_helper.to_array(tensor)\n",
    "    output_processed = emotion_map(postprocess(output_data[0]))[0]\n",
    "    test_outputs.append(output_processed)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c3f2f57c-7454-4d3e-b38d-b0946cf066ea"
    }
   },
   "source": [
    "### Show some sample images\n",
    "We use `matplotlib` to plot 3 test images from the dataset."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "396d478b-34aa-4afa-9898-cdce8222a516"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 20))\n",
    "for test_image in np.arange(3):\n",
    "    test_inputs[test_image].reshape(1, 64, 64)\n",
    "    plt.subplot(1, 8, test_image+1)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    plt.text(x = 10, y = -10, s = test_outputs[test_image], fontsize = 18)\n",
    "    plt.imshow(test_inputs[test_image].reshape(64, 64), cmap = plt.cm.gray)\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluation / prediction"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 6))\n",
    "plt.subplot(1, 8, 1)\n",
    "\n",
    "plt.text(x = 0, y = -30, s = \"True Label: \", fontsize = 13, color = 'black')\n",
    "plt.text(x = 0, y = -20, s = \"Result: \", fontsize = 13, color = 'black')\n",
    "plt.text(x = 0, y = -10, s = \"Inference Time: \", fontsize = 13, color = 'black')\n",
    "plt.text(x = 3, y = 14, s = \"Model Input\", fontsize = 12, color = 'black')\n",
    "plt.text(x = 6, y = 18, s = \"(64 x 64)\", fontsize = 12, color = 'black')\n",
    "plt.imshow(np.ones((28,28)), cmap=plt.cm.Greys)    \n",
    "\n",
    "\n",
    "for i in np.arange(test_data_size):\n",
    "    \n",
    "    input_data = json.dumps({'data': test_inputs[i].tolist()})\n",
    "\n",
    "    # predict using the deployed model\n",
    "    r = json.loads(aci_service.run(input_data))\n",
    "    \n",
    "    if \"error\" in r:\n",
    "        print(r['error'])\n",
    "        break\n",
    "        \n",
    "    result = r['result'][0]\n",
    "    time_ms = np.round(r['time_in_sec'][0] * 1000, 2)\n",
    "    \n",
    "    ground_truth = test_outputs[i]\n",
    "    \n",
    "    # compare actual value vs. the predicted values:\n",
    "    plt.subplot(1, 8, i+2)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "\n",
    "    # use different color for misclassified sample\n",
    "    font_color = 'red' if ground_truth != result else 'black'\n",
    "    clr_map = plt.cm.Greys if ground_truth != result else plt.cm.gray\n",
    "\n",
    "    # ground truth labels are in blue\n",
    "    plt.text(x = 10, y = -70, s = ground_truth, fontsize = 18, color = 'blue')\n",
    "    \n",
    "    # predictions are in black if correct, red if incorrect\n",
    "    plt.text(x = 10, y = -45, s = result, fontsize = 18, color = font_color)\n",
    "    plt.text(x = 5, y = -22, s = str(time_ms) + ' ms', fontsize = 14, color = font_color)\n",
    "\n",
    "    \n",
    "    plt.imshow(test_inputs[i].reshape(64, 64), cmap = clr_map)\n",
    "\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try classifying your own images!"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions take your image and format it so it can be passed\n",
    "# as input into our ONNX model\n",
    "\n",
    "import cv2\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    \"\"\"Convert the input image into grayscale\"\"\"\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "def resize_img(img_to_resize):\n",
    "    \"\"\"Resize image to FER+ model input dimensions\"\"\"\n",
    "    r_img = cv2.resize(img_to_resize, dsize=(64, 64), interpolation=cv2.INTER_AREA)\n",
    "    r_img.resize((1, 1, 64, 64))\n",
    "    return r_img\n",
    "\n",
    "def preprocess(img_to_preprocess):\n",
    "    \"\"\"Resize input images and convert them to grayscale.\"\"\"\n",
    "    if img_to_preprocess.shape == (64, 64):\n",
    "        img_to_preprocess.resize((1, 1, 64, 64))\n",
    "        return img_to_preprocess\n",
    "    \n",
    "    grayscale = rgb2gray(img_to_preprocess)\n",
    "    processed_img = resize_img(grayscale)\n",
    "    return processed_img"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the following string with your own path/test image\n",
    "# Make sure your image is square and the dimensions are equal (i.e. 100 * 100 pixels or 28 * 28 pixels)\n",
    "\n",
    "# Any PNG or JPG image file should work\n",
    "# Make sure to include the entire path with // instead of /\n",
    "\n",
    "# e.g. your_test_image = \"C:/Users/vinitra.swamy/Pictures/face.png\"\n",
    "\n",
    "your_test_image = \"<path to file>\"\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "if your_test_image != \"<path to file>\":\n",
    "    img = mpimg.imread(your_test_image)\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(img, cmap = plt.cm.Greys)\n",
    "    print(\"Old Dimensions: \", img.shape)\n",
    "    img = preprocess(img)\n",
    "    print(\"New Dimensions: \", img.shape)\n",
    "else:\n",
    "    img = None"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if img is None:\n",
    "    print(\"Add the path for your image data.\")\n",
    "else:\n",
    "    input_data = json.dumps({'data': img.tolist()})\n",
    "\n",
    "    try:\n",
    "        r = json.loads(aci_service.run(input_data))\n",
    "        result = r['result'][0]\n",
    "        time_ms = np.round(r['time_in_sec'][0] * 1000, 2)\n",
    "    except KeyError as e:\n",
    "        print(str(e))\n",
    "\n",
    "    plt.figure(figsize = (16, 6))\n",
    "    plt.subplot(1,8,1)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    plt.text(x = -10, y = -40, s = \"Model prediction: \", fontsize = 14)\n",
    "    plt.text(x = -10, y = -25, s = \"Inference time: \", fontsize = 14)\n",
    "    plt.text(x = 100, y = -40, s = str(result), fontsize = 14)\n",
    "    plt.text(x = 100, y = -25, s = str(time_ms) + \" ms\", fontsize = 14)\n",
    "    plt.text(x = -10, y = -10, s = \"Model Input image: \", fontsize = 14)\n",
    "    plt.imshow(img.reshape((64, 64)), cmap = plt.cm.gray)    \n",
    "     "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember to delete your service after you are done using it!\n",
    "\n",
    "aci_service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations!\n",
    "\n",
    "In this tutorial, you have:\n",
    "- familiarized yourself with ONNX Runtime inference and the pretrained models in the ONNX model zoo\n",
    "- understood a state-of-the-art convolutional neural net image classification model (FER+ in ONNX) and deployed it in the Azure ML cloud\n",
    "- ensured that your deep learning model is working perfectly (in the cloud) on test data, and checked it against some of your own!\n",
    "\n",
    "Next steps:\n",
    "- If you have not already, check out another interesting ONNX/AML application that lets you set up a state-of-the-art [handwritten image classification model (MNIST)](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/deployment/onnx/onnx-inference-mnist-deploy.ipynb) in the cloud! This tutorial deploys a pre-trained ONNX Computer Vision model for handwritten digit classification in an Azure ML virtual machine.\n",
    "- Keep an eye out for an updated version of this tutorial that uses ONNX Runtime GPU.\n",
    "- Contribute to our [open source ONNX repository on github](http://github.com/onnx/onnx) and/or add to our [ONNX model zoo](http://github.com/onnx/models)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/onnx/onnx-inference-facial-expression-recognition-deploy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enabling App Insights for Services in Production\n",
    "With this notebook, you can learn how to enable App Insights for standard service monitoring, plus, we provide examples for doing custom logging within a scoring files in a model.\n",
    "\n",
    "\n",
    "## What does Application Insights monitor?\n",
    "It monitors request rates, response times, failure rates, etc. For more information visit [App Insights docs.](https://docs.microsoft.com/en-us/azure/application-insights/app-insights-overview)\n",
    "\n",
    "\n",
    "## What is different compared to standard production deployment process?\n",
    "If you want to enable generic App Insights for a service run:\n",
    "```python\n",
    "aks_service= Webservice(ws, \"aks-w-dc2\")\n",
    "aks_service.update(enable_app_insights=True)```\n",
    "Where \"aks-w-dc2\" is your service name. You can also do this from the Azure Portal under your Workspace--> deployments--> Select deployment--> Edit--> Advanced Settings--> Select \"Enable AppInsights diagnostics\"\n",
    "\n",
    "If you want to log custom traces, you will follow the standard deplyment process for AKS and you will:\n",
    "1. Update scoring file.\n",
    "2. Update aks configuration.\n",
    "3. Deploy the model with this new configuration. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import your dependencies"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "import json\n",
    "\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.compute import AksCompute, ComputeTarget\n",
    "from azureml.core.webservice import AksWebservice\n",
    "\n",
    "print(azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set up your configuration and create a workspace\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Register Model\n",
    "Register an existing trained model, add descirption and tags."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "model = Model.register(model_path=\"sklearn_regression_model.pkl\", # This points to a local file.\n",
    "                       model_name=\"sklearn_regression_model.pkl\", # This is the name the model is registered as.\n",
    "                       tags={'area': \"diabetes\", 'type': \"regression\"},\n",
    "                       description=\"Ridge regression model to predict diabetes\",\n",
    "                       workspace=ws)\n",
    "\n",
    "print(model.name, model.description, model.version)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. *Update your scoring file with custom print statements*\n",
    "Here is an example:\n",
    "### a. In your init function add:\n",
    "```python\n",
    "print (\"model initialized\" + time.strftime(\"%H:%M:%S\"))```\n",
    "\n",
    "### b. In your run function add:\n",
    "```python\n",
    "print (\"Prediction created\" + time.strftime(\"%H:%M:%S\"))```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import Ridge\n",
    "import time\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    #Print statement for appinsights custom traces:\n",
    "    print (\"model initialized\" + time.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
    "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'sklearn_regression_model.pkl')\n",
    "\n",
    "    # deserialize the model file back into a sklearn model\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "\n",
    "# note you can pass in multiple rows for scoring\n",
    "def run(raw_data):\n",
    "    try:\n",
    "        data = json.loads(raw_data)['data']\n",
    "        data = numpy.array(data)\n",
    "        result = model.predict(data)\n",
    "        print (\"Prediction created\" + time.strftime(\"%H:%M:%S\"))\n",
    "        # you can return any datatype as long as it is JSON-serializable\n",
    "        return result.tolist()\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        print (error + time.strftime(\"%H:%M:%S\"))\n",
    "        return error"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. *Create myenv.yml file*\n",
    "Please note that you must indicate azureml-defaults with verion >= 1.0.45 as a pip dependency, because it contains the functionality needed to host the model as a web service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "myenv = CondaDependencies.create(conda_packages=['numpy','scikit-learn==0.20.3'],\n",
    "                                 pip_packages=['azureml-defaults'])\n",
    "\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Inference Configuration"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.environment import Environment\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy to ACI (Optional)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aci_deployment_config = AciWebservice.deploy_configuration(cpu_cores=1,\n",
    "                                                           memory_gb=1,\n",
    "                                                           tags={'area': \"diabetes\", 'type': \"regression\"},\n",
    "                                                           description=\"Predict diabetes using regression model\",\n",
    "                                                           enable_app_insights=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_service_name = \"aci-service-appinsights\"\n",
    "\n",
    "aci_service = Model.deploy(ws, aci_service_name, [model], inference_config, aci_deployment_config, overwrite=True)\n",
    "aci_service.wait_for_deployment(show_output=True)\n",
    "\n",
    "print(aci_service.state)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if aci_service.state == \"Healthy\":\n",
    "    test_sample = json.dumps({\n",
    "        \"data\": [\n",
    "            [1,28,13,45,54,6,57,8,8,10],\n",
    "            [101,9,8,37,6,45,4,3,2,41]\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    prediction = aci_service.run(test_sample)\n",
    "\n",
    "    print(prediction)\n",
    "else:\n",
    "    raise ValueError(\"Service deployment isn't healthy, can't call the service. Error: \", aci_service.error)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deploy to AKS service"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AKS compute if you haven't done so."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AksCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "aks_name = \"my-aks-insights\"\n",
    "\n",
    "creating_compute = False\n",
    "try:\n",
    "    aks_target = ComputeTarget(ws, aks_name)\n",
    "    print(\"Using existing AKS compute target {}.\".format(aks_name))\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating a new AKS compute target {}.\".format(aks_name))\n",
    "\n",
    "    # Use the default configuration (can also provide parameters to customize).\n",
    "    prov_config = AksCompute.provisioning_configuration()\n",
    "    aks_target = ComputeTarget.create(workspace=ws,\n",
    "                                      name=aks_name,\n",
    "                                      provisioning_configuration=prov_config)\n",
    "    creating_compute = True"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if creating_compute and aks_target.provisioning_state != \"Succeeded\":\n",
    "    aks_target.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aks_target.provisioning_state)\n",
    "print(aks_target.provisioning_errors)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already have a cluster you can attach the service to it:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "%%time\n",
    "resource_id = '/subscriptions/<subscriptionid>/resourcegroups/<resourcegroupname>/providers/Microsoft.ContainerService/managedClusters/<aksservername>'\n",
    "create_name= 'myaks4'\n",
    "attach_config = AksCompute.attach_configuration(resource_id=resource_id)\n",
    "aks_target = ComputeTarget.attach(workspace=ws,\n",
    "                                  name=create_name,\n",
    "                                  attach_configuration=attach_config)\n",
    "## Wait for the operation to complete\n",
    "aks_target.wait_for_provisioning(True)```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. *Activate App Insights through updating AKS Webservice configuration*\n",
    "In order to enable App Insights in your service you will need to update your AKS configuration file:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the web service configuration.\n",
    "aks_deployment_config = AksWebservice.deploy_configuration(enable_app_insights=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Deploy your service"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if aks_target.provisioning_state == \"Succeeded\":\n",
    "    aks_service_name = \"aks-service-appinsights\"\n",
    "    aks_service = Model.deploy(ws,\n",
    "                               aks_service_name,\n",
    "                               [model],\n",
    "                               inference_config,\n",
    "                               aks_deployment_config,\n",
    "                               deployment_target=aks_target,\n",
    "                               overwrite=True)\n",
    "    aks_service.wait_for_deployment(show_output=True)\n",
    "    print(aks_service.state)\n",
    "else:\n",
    "    raise ValueError(\"AKS cluster provisioning failed. Error: \", aks_target.provisioning_errors)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test your service "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if aks_service.state == \"Healthy\":\n",
    "    test_sample = json.dumps({\n",
    "        \"data\": [\n",
    "            [1,28,13,45,54,6,57,8,8,10],\n",
    "            [101,9,8,37,6,45,4,3,2,41]\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    prediction = aks_service.run(input_data=test_sample)\n",
    "    print(prediction)\n",
    "else:\n",
    "    raise ValueError(\"Service deployment isn't healthy, can't call the service. Error: \", aks_service.error)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. See your service telemetry in App Insights\n",
    "1. Go to the [Azure Portal](https://portal.azure.com/)\n",
    "2. All resources--> Select the subscription/resource group where you created your Workspace--> Select the App Insights type\n",
    "3. Click on the AppInsights resource. You'll see a highlevel dashboard with information on Requests, Server response time and availability.\n",
    "4. Click on the top banner \"Analytics\"\n",
    "5. In the \"Schema\" section select \"traces\" and run your query.\n",
    "6. Voila! All your custom traces should be there."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disable App Insights"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aks_service.update(enable_app_insights=False)\n",
    "aks_service.wait_for_deployment(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "aks_service.delete()\n",
    "aci_service.delete()\n",
    "model.delete()\n",
    "if creating_compute:\n",
    "    aks_target.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy models to Azure Kubernetes Service (AKS) using controlled roll out\n",
    "This notebook will show you how to deploy mulitple AKS webservices with the same scoring endpoint and how to roll out your models in a controlled manner by configuring % of scoring traffic going to each webservice. If you are using a Notebook VM, you are all set. Otherwise, go through the [configuration notebook](../../../configuration.ipynb) to install the Azure Machine Learning Python SDK and create an Azure ML Workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for latest version\n",
    "import azureml.core\n",
    "print(azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "Create a [Workspace](https://docs.microsoft.com/python/api/azureml-core/azureml.core.workspace%28class%29?view=azure-ml-py) object from your persisted configuration."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the model\n",
    "Register a file or folder as a model by calling [Model.register()](https://docs.microsoft.com/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none-).\n",
    "In addition to the content of the model file itself, your registered model will also store model metadata -- model description, tags, and framework information -- that will be useful when managing and deploying models in your workspace. Using tags, for instance, you can categorize your models and apply filters when listing models in your workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "model = Model.register(workspace=ws,\n",
    "                       model_name='sklearn_regression_model.pkl',                # Name of the registered model in your workspace.\n",
    "                       model_path='./sklearn_regression_model.pkl',  # Local file to upload and register as a model.\n",
    "                       model_framework=Model.Framework.SCIKITLEARN,  # Framework used to create the model.\n",
    "                       model_framework_version='0.19.1',             # Version of scikit-learn used to create the model.\n",
    "                       description='Ridge regression model to predict diabetes progression.',\n",
    "                       tags={'area': 'diabetes', 'type': 'regression'})\n",
    "\n",
    "print('Name:', model.name)\n",
    "print('Version:', model.version)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register an environment (for all models)\n",
    "\n",
    "If you control over how your model is run, or if it has special runtime requirements, you can specify your own environment and scoring method.\n",
    "\n",
    "Specify the model's runtime environment by creating an [Environment](https://docs.microsoft.com/python/api/azureml-core/azureml.core.environment%28class%29?view=azure-ml-py) object and providing the [CondaDependencies](https://docs.microsoft.com/python/api/azureml-core/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py) needed by your model."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "environment=Environment('my-sklearn-environment')\n",
    "environment.python.conda_dependencies = CondaDependencies.create(pip_packages=[\n",
    "    'azureml-defaults',\n",
    "    'inference-schema[numpy-support]',\n",
    "    'numpy',\n",
    "    'scikit-learn==0.19.1',\n",
    "    'scipy'\n",
    "])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using a custom environment, you must also provide Python code for initializing and running your model. An example script is included with this notebook."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('score.py') as f:\n",
    "    print(f.read())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the InferenceConfig\n",
    "Create the inference configuration to reference your environment and entry script during deployment"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inference_config = InferenceConfig(entry_script='score.py', \n",
    "                                   source_directory='.',\n",
    "                                   environment=environment)\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provision the AKS Cluster\n",
    "If you already have an AKS cluster attached to this workspace, skip the step below and provide the name of the cluster."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AksCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "# Use the default configuration (can also provide parameters to customize)\n",
    "prov_config = AksCompute.provisioning_configuration()\n",
    "\n",
    "aks_name = 'my-aks' \n",
    "# Create the cluster\n",
    "aks_target = ComputeTarget.create(workspace = ws, \n",
    "                                  name = aks_name, \n",
    "                                  provisioning_configuration = prov_config) \n",
    "aks_target.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Endpoint and add a version (AKS service)\n",
    "This creates a new endpoint and adds a version behind it. By default the first version added is the default version. You can specify the traffic percentile a version takes behind an endpoint. \n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploying the model and create a new endpoint\n",
    "from azureml.core.webservice import AksEndpoint\n",
    "# from azureml.core.compute import ComputeTarget\n",
    "\n",
    "#select a created compute\n",
    "compute = ComputeTarget(ws, 'my-aks')\n",
    "namespace_name=\"endpointnamespace\"\n",
    "# define the endpoint name\n",
    "endpoint_name = \"myendpoint1\"\n",
    "# define the service name\n",
    "version_name= \"versiona\"\n",
    "\n",
    "endpoint_deployment_config = AksEndpoint.deploy_configuration(tags = {'modelVersion':'firstversion', 'department':'finance'}, \n",
    "                                                              description = \"my first version\", namespace = namespace_name, \n",
    "                                                              version_name = version_name, traffic_percentile = 40)\n",
    "\n",
    "endpoint = Model.deploy(ws, endpoint_name, [model], inference_config, endpoint_deployment_config, compute)\n",
    "endpoint.wait_for_deployment(True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.get_logs()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add another version of the service to an existing endpoint\n",
    "This adds another version behind an existing endpoint. You can specify the traffic percentile the new version takes. If no traffic_percentile is specified then it defaults to 0. All the unspecified traffic percentile (in this example 50) across all versions goes to default version."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new version to an existing Endpoint.\n",
    "version_name_add=\"versionb\" \n",
    "\n",
    "endpoint.create_version(version_name = version_name_add, inference_config=inference_config, models=[model], tags = {'modelVersion':'secondversion', 'department':'finance'}, \n",
    "                        description = \"my second version\", traffic_percentile = 10)\n",
    "endpoint.wait_for_deployment(True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update an existing version in an endpoint\n",
    "There are two types of versions: control and treatment. An endpoint contains one or more treatment versions but only one control version. This categorization helps compare the different versions against the defined control version."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.update_version(version_name=endpoint.versions[version_name_add].name, description=\"my second version update\", traffic_percentile=40, is_default=True, is_control_version_type=True)\n",
    "endpoint.wait_for_deployment(True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the web service using run method\n",
    "Test the web sevice by passing in data. Run() method retrieves API keys behind the scenes to make sure that call is authenticated."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring on endpoint\n",
    "import json\n",
    "test_sample = json.dumps({'data': [\n",
    "    [1,2,3,4,5,6,7,8,9,10], \n",
    "    [10,9,8,7,6,5,4,3,2,1]\n",
    "]})\n",
    "\n",
    "test_sample_encoded = bytes(test_sample, encoding='utf8')\n",
    "prediction = endpoint.run(input_data=test_sample_encoded)\n",
    "print(prediction)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Resources"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting a version in an endpoint\n",
    "endpoint.delete_version(version_name=version_name)\n",
    "endpoint.wait_for_deployment(True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting an endpoint, this will delete all versions in the endpoint and the endpoint itself\n",
    "endpoint.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-with-controlled-rollout/deploy-aks-with-controlled-rollout.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks-gpu/production-deploy-to-aks-gpu.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/automated-machine-learning/production-deploy-to-aks-gpu/production-deploy-to-aks-gpu.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks-gpu/production-deploy-to-aks-gpu.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying a web service to Azure Kubernetes Service (AKS)\n",
    "This notebook shows the steps for deploying a service: registering a model, creating an image, provisioning a cluster (one time action), and deploying a service to it. \n",
    "We then test and delete the service, image and model."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks-gpu/production-deploy-to-aks-gpu.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "print(azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks-gpu/production-deploy-to-aks-gpu.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get workspace\n",
    "Load existing workspace from the config file info."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks-gpu/production-deploy-to-aks-gpu.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks-gpu/production-deploy-to-aks-gpu.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the model\n",
    "\n",
    "Prior to registering the model, you should have a TensorFlow [Saved Model](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md) in the `resnet50` directory. This cell will download a [pretrained resnet50](http://download.tensorflow.org/models/official/20181001_resnet/savedmodels/resnet_v1_fp32_savedmodel_NCHW_jpg.tar.gz) and unpack it to that directory."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks-gpu/production-deploy-to-aks-gpu.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import shutil\n",
    "import tarfile\n",
    "import tempfile\n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "model_url = \"http://download.tensorflow.org/models/official/20181001_resnet/savedmodels/resnet_v1_fp32_savedmodel_NCHW_jpg.tar.gz\"\n",
    "\n",
    "archive_prefix = \"./resnet_v1_fp32_savedmodel_NCHW_jpg/1538686758/\"\n",
    "target_folder = \"resnet50\"\n",
    "\n",
    "if not os.path.exists(target_folder):\n",
    "    response = requests.get(model_url)\n",
    "    archive = tarfile.open(fileobj=BytesIO(response.content))\n",
    "    with tempfile.TemporaryDirectory() as temp_folder:\n",
    "        archive.extractall(temp_folder)\n",
    "        shutil.copytree(os.path.join(temp_folder, archive_prefix), target_folder)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks-gpu/production-deploy-to-aks-gpu.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register the model\n",
    "Register an existing trained model, add description and tags."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks-gpu/production-deploy-to-aks-gpu.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model = Model.register(model_path=\"resnet50\", # This points to the local directory to upload.\n",
    "                       model_name=\"resnet50\", # This is the name the model is registered as.\n",
    "                       tags={'area': \"Image classification\", 'type': \"classification\"},\n",
    "                       description=\"Image classification trained on Imagenet Dataset\",\n",
    "                       workspace=ws)\n",
    "\n",
    "print(model.name, model.description, model.version)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks-gpu/production-deploy-to-aks-gpu.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provision the AKS Cluster\n",
    "This is a one time setup. You can reuse this cluster for multiple deployments after it has been created. If you delete the cluster or the resource group that contains it, then you would have to recreate it."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks-gpu/production-deploy-to-aks-gpu.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AksCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your GPU cluster\n",
    "gpu_cluster_name = \"aks-gpu-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    gpu_cluster = ComputeTarget(workspace=ws, name=gpu_cluster_name)\n",
    "    print(\"Found existing gpu cluster\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new gpu-cluster\")\n",
    "    \n",
    "    # Specify the configuration for the new cluster\n",
    "    compute_config = AksCompute.provisioning_configuration(cluster_purpose=AksCompute.ClusterPurpose.DEV_TEST,\n",
    "                                                           agent_count=1,\n",
    "                                                           vm_size=\"Standard_NV6\")\n",
    "    # Create the cluster with the specified name and configuration\n",
    "    gpu_cluster = ComputeTarget.create(ws, gpu_cluster_name, compute_config)\n",
    "\n",
    "    # Wait for the cluster to complete, show the output log\n",
    "    gpu_cluster.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks-gpu/production-deploy-to-aks-gpu.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the model as a web service to AKS\n",
    "\n",
    "First create a scoring script"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks-gpu/production-deploy-to-aks-gpu.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from azureml.contrib.services.aml_request import AMLRequest, rawhttp\n",
    "from azureml.contrib.services.aml_response import AMLResponse\n",
    "\n",
    "def init():\n",
    "    global session\n",
    "    global input_name\n",
    "    global output_name\n",
    "    \n",
    "    session = tf.Session()\n",
    "\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
    "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'resnet50')\n",
    "    model = tf.saved_model.loader.load(session, ['serve'], model_path)\n",
    "    if len(model.signature_def['serving_default'].inputs) > 1:\n",
    "        raise ValueError(\"This score.py only supports one input\")\n",
    "    input_name = [tensor.name for tensor in model.signature_def['serving_default'].inputs.values()][0]\n",
    "    output_name = [tensor.name for tensor in model.signature_def['serving_default'].outputs.values()]\n",
    "    \n",
    "\n",
    "@rawhttp\n",
    "def run(request):\n",
    "    if request.method == 'POST':\n",
    "        reqBody = request.get_data(False)\n",
    "        resp = score(reqBody)\n",
    "        return AMLResponse(resp, 200)\n",
    "    if request.method == 'GET':\n",
    "        respBody = str.encode(\"GET is not supported\")\n",
    "        return AMLResponse(respBody, 405)\n",
    "    return AMLResponse(\"bad request\", 500)\n",
    "\n",
    "def score(data):\n",
    "    result = session.run(output_name, {input_name: [data]})\n",
    "    return json.dumps(result[1].tolist())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    init()\n",
    "    with open(\"test_image.jpg\", 'rb') as f:\n",
    "        content = f.read()\n",
    "        print(score(content))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks-gpu/production-deploy-to-aks-gpu.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the deployment configuration objects and deploy the model as a webservice."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks-gpu/production-deploy-to-aks-gpu.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the web service configuration (using default here)\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AksWebservice\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.environment import Environment, DEFAULT_GPU_IMAGE\n",
    "\n",
    "env = Environment('deploytocloudenv')\n",
    "# Please see [Azure ML Containers repository](https://github.com/Azure/AzureML-Containers#featured-tags)\n",
    "# for open-sourced GPU base images.\n",
    "env.docker.base_image = DEFAULT_GPU_IMAGE\n",
    "env.python.conda_dependencies = CondaDependencies.create(conda_packages=['tensorflow-gpu==1.12.0','numpy'],\n",
    "                                 pip_packages=['azureml-contrib-services', 'azureml-defaults'])\n",
    "\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=env)\n",
    "aks_config = AksWebservice.deploy_configuration()\n",
    "\n",
    "# # Enable token auth and disable (key) auth on the webservice\n",
    "# aks_config = AksWebservice.deploy_configuration(token_auth_enabled=True, auth_enabled=False)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks-gpu/production-deploy-to-aks-gpu.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "aks_service_name ='gpu-rn50'\n",
    "\n",
    "aks_service = Model.deploy(workspace=ws,\n",
    "                           name=aks_service_name,\n",
    "                           models=[model],\n",
    "                           inference_config=inference_config,\n",
    "                           deployment_config=aks_config,\n",
    "                           deployment_target=gpu_cluster)\n",
    "\n",
    "aks_service.wait_for_deployment(show_output = True)\n",
    "print(aks_service.state)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks-gpu/production-deploy-to-aks-gpu.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the web service\n",
    "We test the web sevice by passing the test images content."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks-gpu/production-deploy-to-aks-gpu.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import requests\n",
    "\n",
    "# if (key) auth is enabled, fetch keys and include in the request\n",
    "key1, key2 = aks_service.get_keys()\n",
    "\n",
    "headers = {'Content-Type':'application/json', 'Authorization': 'Bearer ' + key1}\n",
    "\n",
    "# # if token auth is enabled, fetch token and include in the request\n",
    "# access_token, fetch_after = aks_service.get_token()\n",
    "# headers = {'Content-Type':'application/json', 'Authorization': 'Bearer ' + access_token}\n",
    "\n",
    "test_sample = open('snowleopardgaze.jpg', 'rb').read()\n",
    "resp = requests.post(aks_service.scoring_uri, test_sample, headers=headers)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks-gpu/production-deploy-to-aks-gpu.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "Delete the service, image, model and compute target"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks-gpu/production-deploy-to-aks-gpu.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "aks_service.delete()\n",
    "model.delete()\n",
    "gpu_cluster.delete()\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/production-deploy-to-aks-gpu/production-deploy-to-aks-gpu.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register model and deploy locally with advanced usages\n",
    "\n",
    "This example shows how to deploy a web service in step-by-step fashion:\n",
    "\n",
    " 1. Register model\n",
    " 2. Deploy the image as a web service in a local Docker container.\n",
    " 3. Quickly test changes to your entry script by reloading the local service.\n",
    " 4. Optionally, you can also make changes to model, conda or extra_docker_file_steps and update local service"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the [configuration](../../../configuration.ipynb) Notebook first if you haven't."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create workspace"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create trained model\n",
    "\n",
    "For this example, we will train a small model on scikit-learn's [diabetes dataset](https://scikit-learn.org/stable/datasets/index.html#diabetes-dataset). "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "dataset_x, dataset_y = load_diabetes(return_X_y=True)\n",
    "\n",
    "sk_model = Ridge().fit(dataset_x, dataset_y)\n",
    "\n",
    "joblib.dump(sk_model, \"sklearn_regression_model.pkl\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add tags and descriptions to your models. we are using `sklearn_regression_model.pkl` file in the current directory as a model with the name `sklearn_regression_model` in the workspace.\n",
    "\n",
    "Using tags, you can track useful information such as the name and version of the machine learning library used to train the model, framework, category, target customer etc. Note that tags must be alphanumeric."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "register model from file",
     "sample-model-register"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model = Model.register(model_path=\"sklearn_regression_model.pkl\",\n",
    "                       model_name=\"sklearn_regression_model\",\n",
    "                       tags={'area': \"diabetes\", 'type': \"regression\"},\n",
    "                       description=\"Ridge regression model to predict diabetes\",\n",
    "                       workspace=ws)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manage your dependencies in a folder"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "source_directory = \"source_directory\"\n",
    "\n",
    "os.makedirs(source_directory, exist_ok=True)\n",
    "os.makedirs(os.path.join(source_directory, \"x/y\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(source_directory, \"env\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(source_directory, \"dockerstep\"), exist_ok=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show `score.py`. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile source_directory/x/y/score.py\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from inference_schema.schema_decorators import input_schema, output_schema\n",
    "from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment. Join this path with the filename of the model file.\n",
    "    # It holds the path to the directory that contains the deployed model (./azureml-models/$MODEL_NAME/$VERSION)\n",
    "    # If there are multiple models, this value is the path to the directory containing all deployed models (./azureml-models)\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'sklearn_regression_model.pkl')\n",
    "    # Deserialize the model file back into a sklearn model.\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "    global name\n",
    "    # Note here, the entire source directory from inference config gets added into image.\n",
    "    # Below is an example of how you can use any extra files in image.\n",
    "    with open('./source_directory/extradata.json') as json_file:\n",
    "        data = json.load(json_file)\n",
    "        name = data[\"people\"][0][\"name\"]\n",
    "\n",
    "input_sample = np.array([[10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0]])\n",
    "output_sample = np.array([3726.995])\n",
    "\n",
    "@input_schema('data', NumpyParameterType(input_sample))\n",
    "@output_schema(NumpyParameterType(output_sample))\n",
    "def run(data):\n",
    "    try:\n",
    "        result = model.predict(data)\n",
    "        # You can return any JSON-serializable object.\n",
    "        return \"Hello \" + name + \" here is your result = \" + str(result)\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        return error"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile source_directory/extradata.json\n",
    "{\n",
    "    \"people\": [\n",
    "        {\n",
    "            \"website\": \"microsoft.com\", \n",
    "            \"from\": \"Seattle\", \n",
    "            \"name\": \"Mrudula\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Inference Configuration\n",
    "\n",
    " - file_path: input parameter to Environment constructor. Manages conda and python package dependencies.\n",
    " - env.docker.base_dockerfile: any extra steps you want to inject into docker file\n",
    " - source_directory: holds source path as string, this entire folder gets added in image so its really easy to access any files within this folder or subfolder\n",
    " - entry_script: contains logic specific to initializing your model and running predictions"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "\n",
    "myenv = Environment('myenv')\n",
    "myenv.python.conda_dependencies.add_pip_package(\"inference-schema[numpy-support]\")\n",
    "myenv.python.conda_dependencies.add_pip_package(\"joblib\")\n",
    "myenv.python.conda_dependencies.add_pip_package(\"scikit-learn=={}\".format(sklearn.__version__))\n",
    "\n",
    "# explicitly set base_image to None when setting base_dockerfile\n",
    "myenv.docker.base_image = None\n",
    "myenv.docker.base_dockerfile = \"FROM mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04\\nRUN echo \\\"this is test\\\"\"\n",
    "myenv.inferencing_stack_version = \"latest\"\n",
    "\n",
    "inference_config = InferenceConfig(source_directory=source_directory,\n",
    "                                   entry_script=\"x/y/score.py\",\n",
    "                                   environment=myenv)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Model as a Local Docker Web Service\n",
    "\n",
    "*Make sure you have Docker installed and running.*\n",
    "\n",
    "Note that the service creation can take few minutes.\n",
    "\n",
    "NOTE:\n",
    "\n",
    "The Docker image runs as a Linux container. If you are running Docker for Windows, you need to ensure the Linux Engine is running:\n",
    "\n",
    "    # PowerShell command to switch to Linux engine\n",
    "    & 'C:\\Program Files\\Docker\\Docker\\DockerCli.exe' -SwitchLinuxEngine"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "deploy service",
     "aci"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.webservice import LocalWebservice\n",
    "\n",
    "# This is optional, if not provided Docker will choose a random unused port.\n",
    "deployment_config = LocalWebservice.deploy_configuration(port=6789)\n",
    "\n",
    "local_service = Model.deploy(ws, \"test\", [model], inference_config, deployment_config)\n",
    "\n",
    "local_service.wait_for_deployment()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Local service port: {}'.format(local_service.port))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Status and Get Container Logs\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(local_service.get_logs())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Web Service"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the web service with some input data to get a prediction."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sample_input = json.dumps({\n",
    "    'data': dataset_x[0:2].tolist()\n",
    "})\n",
    "\n",
    "print(local_service.run(sample_input))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload Service\n",
    "\n",
    "You can update your score.py file and then call `reload()` to quickly restart the service. This will only reload your execution script and dependency files, it will not rebuild the underlying Docker image. As a result, `reload()` is fast, but if you do need to rebuild the image -- to add a new Conda or pip package, for instance -- you will have to call `update()`, instead (see below)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile source_directory/x/y/score.py\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from inference_schema.schema_decorators import input_schema, output_schema\n",
    "from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
    "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'sklearn_regression_model.pkl')\n",
    "    # Deserialize the model file back into a sklearn model.\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "    global name, from_location\n",
    "    # Note here, the entire source directory from inference config gets added into image.\n",
    "    # Below is an example of how you can use any extra files in image.\n",
    "    with open('source_directory/extradata.json') as json_file:  \n",
    "        data = json.load(json_file)\n",
    "        name = data[\"people\"][0][\"name\"]\n",
    "        from_location = data[\"people\"][0][\"from\"]\n",
    "\n",
    "input_sample = np.array([[10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0]])\n",
    "output_sample = np.array([3726.995])\n",
    "\n",
    "@input_schema('data', NumpyParameterType(input_sample))\n",
    "@output_schema(NumpyParameterType(output_sample))\n",
    "def run(data):\n",
    "    try:\n",
    "        result = model.predict(data)\n",
    "        # You can return any JSON-serializable object.\n",
    "        return \"Hello \" + name + \" from \" + from_location + \" here is your result = \" + str(result)\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        return error"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_service.reload()\n",
    "print(\"--------------------------------------------------------------\")\n",
    "\n",
    "# After calling reload(), run() will return the updated message.\n",
    "local_service.run(sample_input)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Service\n",
    "\n",
    "If you want to change your model(s), Conda dependencies, or deployment configuration, call `update()` to rebuild the Docker image.\n",
    "\n",
    "```python\n",
    "\n",
    "local_service.update(models=[SomeOtherModelObject],\n",
    "                     deployment_config=local_config,\n",
    "                     inference_config=inference_config)\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Service"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local-advanced.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register model and deploy locally\n",
    "\n",
    "This example shows how to deploy a web service in step-by-step fashion:\n",
    "\n",
    " 1. Register model\n",
    " 2. Deploy the image as a web service in a local Docker container.\n",
    " 3. Quickly test changes to your entry script by reloading the local service.\n",
    " 4. Optionally, you can also make changes to model, conda or extra_docker_file_steps and update local service"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the [configuration](../../../configuration.ipynb) Notebook first if you haven't."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create trained model\n",
    "\n",
    "For this example, we will train a small model on scikit-learn's [diabetes dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset). "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "dataset_x, dataset_y = load_diabetes(return_X_y=True)\n",
    "\n",
    "sk_model = Ridge().fit(dataset_x, dataset_y)\n",
    "\n",
    "joblib.dump(sk_model, \"sklearn_regression_model.pkl\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are registering the serialized file `sklearn_regression_model.pkl` in the current directory as a model with the name `sklearn_regression_model` in the workspace.\n",
    "\n",
    "You can add tags and descriptions to your models. Using tags, you can track useful information such as the name and version of the machine learning library used to train the model, framework, category, target customer etc. Note that tags must be alphanumeric."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "register model from file"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model = Model.register(model_path=\"sklearn_regression_model.pkl\",\n",
    "                       model_name=\"sklearn_regression_model\",\n",
    "                       tags={'area': \"diabetes\", 'type': \"regression\"},\n",
    "                       description=\"Ridge regression model to predict diabetes\",\n",
    "                       workspace=ws)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Environment"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "environment = Environment(\"LocalDeploy\")\n",
    "environment.python.conda_dependencies.add_pip_package(\"inference-schema[numpy-support]\")\n",
    "environment.python.conda_dependencies.add_pip_package(\"joblib\")\n",
    "environment.python.conda_dependencies.add_pip_package(\"scikit-learn=={}\".format(sklearn.__version__))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide the Scoring Script\n",
    "\n",
    "This Python script handles the model execution inside the service container. The `init()` method loads the model file, and `run(data)` is called for every input to the service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from inference_schema.schema_decorators import input_schema, output_schema\n",
    "from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
    "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'sklearn_regression_model.pkl')\n",
    "    # Deserialize the model file back into a sklearn model.\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "input_sample = np.array([[10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0]])\n",
    "output_sample = np.array([3726.995])\n",
    "\n",
    "@input_schema('data', NumpyParameterType(input_sample))\n",
    "@output_schema(NumpyParameterType(output_sample))\n",
    "def run(data):\n",
    "    try:\n",
    "        result = model.predict(data)\n",
    "        # You can return any JSON-serializable object.\n",
    "        return result.tolist()\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        return error"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Inference Configuration"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\",\n",
    "                                   environment=environment)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Model as a Local Docker Web Service\n",
    "\n",
    "*Make sure you have Docker installed and running.*\n",
    "\n",
    "Note that the service creation can take few minutes.\n",
    "\n",
    "NOTE:\n",
    "\n",
    "The Docker image runs as a Linux container. If you are running Docker for Windows, you need to ensure the Linux Engine is running:\n",
    "\n",
    "    # PowerShell command to switch to Linux engine\n",
    "    & 'C:\\Program Files\\Docker\\Docker\\DockerCli.exe' -SwitchLinuxEngine"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "sample-localwebservice-deploy"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.webservice import LocalWebservice\n",
    "\n",
    "# This is optional, if not provided Docker will choose a random unused port.\n",
    "deployment_config = LocalWebservice.deploy_configuration(port=6789)\n",
    "\n",
    "local_service = Model.deploy(ws, \"test\", [model], inference_config, deployment_config)\n",
    "\n",
    "local_service.wait_for_deployment()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Local service port: {}'.format(local_service.port))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Status and Get Container Logs\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(local_service.get_logs())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Web Service"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the web service with some input data to get a prediction."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sample_input = json.dumps({\n",
    "    'data': dataset_x[0:2].tolist()\n",
    "})\n",
    "\n",
    "local_service.run(sample_input)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload Service\n",
    "\n",
    "You can update your score.py file and then call `reload()` to quickly restart the service. This will only reload your execution script and dependency files, it will not rebuild the underlying Docker image. As a result, `reload()` is fast, but if you do need to rebuild the image -- to add a new Conda or pip package, for instance -- you will have to call `update()`, instead (see below)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from inference_schema.schema_decorators import input_schema, output_schema\n",
    "from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
    "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'sklearn_regression_model.pkl')\n",
    "    # Deserialize the model file back into a sklearn model.\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "input_sample = np.array([[10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0]])\n",
    "output_sample = np.array([3726.995])\n",
    "\n",
    "@input_schema('data', NumpyParameterType(input_sample))\n",
    "@output_schema(NumpyParameterType(output_sample))\n",
    "def run(data):\n",
    "    try:\n",
    "        result = model.predict(data)\n",
    "        # You can return any JSON-serializable object.\n",
    "        return 'Hello from the updated score.py: ' + str(result.tolist())\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        return error"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_service.reload()\n",
    "print(\"--------------------------------------------------------------\")\n",
    "\n",
    "# After calling reload(), run() will return the updated message.\n",
    "local_service.run(sample_input)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Service\n",
    "\n",
    "If you want to change your model(s), Conda dependencies or deployment configuration, call `update()` to rebuild the Docker image.\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_service.update(models=[model],\n",
    "                     inference_config=inference_config,\n",
    "                     deployment_config=deployment_config)\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model to AKS cluster based on the LocalWebservice's configuration."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a one time setup for AKS Cluster. You can reuse this cluster for multiple deployments after it has been created. If you delete the cluster or the resource group that contains it, then you would have to recreate it.\n",
    "from azureml.core.compute import AksCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your AKS cluster\n",
    "aks_name = 'my-aks-9' \n",
    "\n",
    "# Verify the cluster does not exist already\n",
    "try:\n",
    "    aks_target = ComputeTarget(workspace=ws, name=aks_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # Use the default configuration (can also provide parameters to customize)\n",
    "    prov_config = AksCompute.provisioning_configuration()\n",
    "\n",
    "    # Create the cluster\n",
    "    aks_target = ComputeTarget.create(workspace = ws, \n",
    "                                      name = aks_name, \n",
    "                                      provisioning_configuration = prov_config)\n",
    "\n",
    "if aks_target.get_status() != \"Succeeded\":\n",
    "    aks_target.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AksWebservice\n",
    "# Set the web service configuration (using default here)\n",
    "aks_config = AksWebservice.deploy_configuration()\n",
    "\n",
    "# # Enable token auth and disable (key) auth on the webservice\n",
    "# aks_config = AksWebservice.deploy_configuration(token_auth_enabled=True, auth_enabled=False)\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "aks_service_name ='aks-service-1'\n",
    "\n",
    "aks_service = local_service.deploy_to_cloud(name=aks_service_name,\n",
    "                                            deployment_config=aks_config,\n",
    "                                            deployment_target=aks_target)\n",
    "\n",
    "aks_service.wait_for_deployment(show_output = True)\n",
    "print(aks_service.state)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test aks service\n",
    "\n",
    "sample_input = json.dumps({\n",
    "    'data': dataset_x[0:2].tolist()\n",
    "})\n",
    "\n",
    "aks_service.run(sample_input)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the service if not needed.\n",
    "aks_service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Service"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Chainer\n",
    "In this tutorial, you will run a Chainer training example on the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset using ChainerMN distributed training across a GPU cluster."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* If you are using an Azure Machine Learning compute instance, you are all set. Otherwise, go through the [Configuration](../../../../configuration.ipynb) notebook to install the Azure Machine Learning Python SDK and create an Azure ML `Workspace`"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "Opt-in diagnostics for better experience, quality, and security of future releases."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Diagnostics"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.telemetry import set_diagnostics_collection\n",
    "\n",
    "set_diagnostics_collection(send_diagnostics=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or attach existing AmlCompute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training your model. In this tutorial, we use Azure ML managed compute ([AmlCompute](https://docs.microsoft.com/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute)) for our remote training compute resource. Specifically, the below code creates an `STANDARD_NC6` GPU cluster that autoscales from `0` to `4` nodes.\n",
    "\n",
    "**Creation of AmlCompute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace, this code will skip the creation process.\n",
    "\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"gpu-cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6',\n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current AmlCompute. \n",
    "print(compute_target.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code creates GPU compute. If you instead want to create CPU compute, provide a different VM size to the `vm_size` parameter, such as `STANDARD_D2_V2`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model on the remote compute\n",
    "Now that we have the AmlCompute ready to go, let's run our distributed training job."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a project directory\n",
    "Create a directory that will contain all the necessary code from your local machine that you will need access to on the remote resource. This includes the training script and any additional files your training script depends on."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "project_folder = './chainer-distr'\n",
    "os.makedirs(project_folder, exist_ok=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training script\n",
    "Now you will need to create your training script. In this tutorial, the script for distributed training of MNIST is already provided for you at `train_mnist.py`. In practice, you should be able to take any custom Chainer training script as is and run it with Azure ML without having to modify your code."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your script is ready, copy the training script `train_mnist.py` into the project directory."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copy('train_mnist.py', project_folder)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiment\n",
    "Create an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment) to track all the runs in your workspace for this distributed Chainer tutorial. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'chainer-distr'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an environment\n",
    "\n",
    "In this tutorial, we will use one of the Azure ML Chainer curated environments for training."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "chainer_env = Environment.get(ws, name='AzureML-Chainer-5.1.0-GPU')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure your training job\n",
    "\n",
    "Create a ScriptRunConfig object to specify the configuration details of your training job, including your training script, environment to use, and the compute target to run on.\n",
    "\n",
    "In order to execute a distributed run using MPI, you must create an `MpiConfiguration` object and specify it to the `distributed_job_config` parameter. The below code will configure a 2-node distributed job. If you would also like to run multiple processes per node (i.e. if your cluster SKU has multiple GPUs), additionally specify the `process_count_per_node` parameter in MpiConfiguration."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.runconfig import MpiConfiguration\n",
    "\n",
    "src = ScriptRunConfig(source_directory=project_folder,\n",
    "                      script='train_mnist.py',\n",
    "                      compute_target=compute_target,\n",
    "                      environment=chainer_env,\n",
    "                      distributed_job_config=MpiConfiguration(node_count=2))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job\n",
    "Run your experiment by submitting your ScriptRunConfig object. Note that this call is asynchronous."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(src)\n",
    "print(run)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor your run\n",
    "You can monitor the progress of the run with a Jupyter widget. Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes. You can see that the widget automatically plots and visualizes the loss metric that we logged to the Azure ML run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/distributed-chainer/distributed-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and hyperparameter tune with Chainer\n",
    "\n",
    "In this tutorial, we demonstrate how to use the Azure ML Python SDK to train a Convolutional Neural Network (CNN) on a single-node GPU with Chainer to perform handwritten digit recognition on the popular MNIST dataset. We will also demonstrate how to perform hyperparameter tuning of the model using Azure ML's HyperDrive service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [Configuration](../../../../configuration.ipynb) notebook to install the Azure Machine Learning Python SDK and create an Azure ML `Workspace`"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension install --py --user azureml.widgets\n",
    "!jupyter nbextension enable --py --user azureml.widgets"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "Opt-in diagnostics for better experience, quality, and security of future releases."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Diagnostics"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.telemetry import set_diagnostics_collection\n",
    "\n",
    "set_diagnostics_collection(send_diagnostics=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Attach existing AmlCompute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training your model. In this tutorial, we use Azure ML managed compute ([AmlCompute](https://docs.microsoft.com/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute)) for our remote training compute resource.\n",
    "\n",
    "**Creation of AmlCompute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace, this code will skip the creation process.\n",
    "\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"gpu-cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6',\n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code creates a GPU cluster. If you instead want to create a CPU cluster, provide a different VM size to the `vm_size` parameter, such as `STANDARD_D2_V2`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model on the remote compute\n",
    "Now that you have your data and training script prepared, you are ready to train on your remote compute cluster. You can take advantage of Azure compute to leverage GPUs to cut down your training time. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a project directory\n",
    "Create a directory that will contain all the necessary code from your local machine that you will need access to on the remote resource. This includes the training script and any additional files your training script depends on."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "project_folder = './chainer-mnist'\n",
    "os.makedirs(project_folder, exist_ok=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training script\n",
    "Now you will need to create your training script. In this tutorial, the training script is already provided for you at `chainer_mnist.py`. In practice, you should be able to take any custom training script as is and run it with Azure ML without having to modify your code.\n",
    "\n",
    "However, if you would like to use Azure ML's [tracking and metrics](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#metrics) capabilities, you will have to add a small amount of Azure ML code inside your training script. \n",
    "\n",
    "In `chainer_mnist.py`, we will log some metrics to our Azure ML run. To do so, we will access the Azure ML `Run` object within the script:\n",
    "```Python\n",
    "from azureml.core.run import Run\n",
    "run = Run.get_context()\n",
    "```\n",
    "Further within `chainer_mnist.py`, we log the batchsize and epochs parameters, and the highest accuracy the model achieves:\n",
    "```Python\n",
    "run.log('Batch size', np.int(args.batchsize))\n",
    "run.log('Epochs', np.int(args.epochs))\n",
    "\n",
    "run.log('Accuracy', np.float(val_accuracy))\n",
    "```\n",
    "These run metrics will become particularly important when we begin hyperparameter tuning our model in the \"Tune model hyperparameters\" section."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your script is ready, copy the training script `chainer_mnist.py` into your project directory."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copy('chainer_mnist.py', project_folder)\n",
    "shutil.copy('chainer_score.py', project_folder)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiment\n",
    "Create an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment) to track all the runs in your workspace for this Chainer tutorial. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'chainer-mnist'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an environment\n",
    "\n",
    "Define a conda environment YAML file with your training script dependencies and create an Azure ML environment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile conda_dependencies.yml\n",
    "\n",
    "channels:\n",
    "- conda-forge\n",
    "dependencies:\n",
    "- python=3.6.2\n",
    "- pip:\n",
    "  - azureml-defaults\n",
    "  - chainer==5.1.0\n",
    "  - cupy-cuda90==5.1.0\n",
    "  - mpi4py==3.0.0\n",
    "  - pytest"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "chainer_env = Environment.from_conda_specification(name = 'chainer-5.1.0-gpu', file_path = './conda_dependencies.yml')\n",
    "\n",
    "# Specify a GPU base image\n",
    "chainer_env.docker.enabled = True\n",
    "chainer_env.docker.base_image = 'mcr.microsoft.com/azureml/intelmpi2018.3-cuda9.0-cudnn7-ubuntu16.04'"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure your training job\n",
    "\n",
    "Create a ScriptRunConfig object to specify the configuration details of your training job, including your training script, environment to use, and the compute target to run on."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "src = ScriptRunConfig(source_directory=project_folder,\n",
    "                      script='chainer_mnist.py',\n",
    "                      arguments=['--epochs', 10, '--batchsize', 128, '--output_dir', './outputs'],\n",
    "                      compute_target=compute_target,\n",
    "                      environment=chainer_env)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job\n",
    "Run your experiment by submitting your ScriptRunConfig object. Note that this call is asynchronous."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(src)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor your run\n",
    "You can monitor the progress of the run with a Jupyter widget. Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get more details of your run\n",
    "print(run.get_details())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune model hyperparameters\n",
    "Now that we've seen how to do a simple Chainer training run using the SDK, let's see if we can further improve the accuracy of our model. We can optimize our model's hyperparameters using Azure Machine Learning's hyperparameter tuning capabilities."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a hyperparameter sweep\n",
    "First, we will define the hyperparameter space to sweep over. Let's tune the batch size and epochs parameters. In this example we will use random sampling to try different configuration sets of hyperparameters to maximize our primary metric, accuracy.\n",
    "\n",
    "Then, we specify the early termination policy to use to early terminate poorly performing runs. Here we use the `BanditPolicy`, which will terminate any run that doesn't fall within the slack factor of our primary evaluation metric. In this tutorial, we will apply this policy every epoch (since we report our `Accuracy` metric every epoch and `evaluation_interval=1`). Notice we will delay the first policy evaluation until after the first `3` epochs (`delay_evaluation=3`).\n",
    "Refer [here](https://docs.microsoft.com/azure/machine-learning/service/how-to-tune-hyperparameters#specify-an-early-termination-policy) for more information on the BanditPolicy and other policies available."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\n",
    "from azureml.train.hyperdrive.policy import BanditPolicy\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive.parameter_expressions import choice\n",
    "    \n",
    "\n",
    "param_sampling = RandomParameterSampling( {\n",
    "    \"--batchsize\": choice(128, 256),\n",
    "    \"--epochs\": choice(5, 10, 20, 40)\n",
    "    }\n",
    ")\n",
    "\n",
    "hyperdrive_config = HyperDriveConfig(run_config=src,\n",
    "                                     hyperparameter_sampling=param_sampling, \n",
    "                                     primary_metric_name='Accuracy',\n",
    "                                     policy=BanditPolicy(evaluation_interval=1, slack_factor=0.1, delay_evaluation=3),\n",
    "                                     primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                                     max_total_runs=8,\n",
    "                                     max_concurrent_runs=4)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lauch the hyperparameter tuning job."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the HyperDrive run\n",
    "hyperdrive_run = experiment.submit(hyperdrive_config)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor HyperDrive runs\n",
    "You can monitor the progress of the runs with the following Jupyter widget. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(hyperdrive_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(hyperdrive_run.get_status() == \"Completed\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm start a Hyperparameter Tuning experiment and resuming child runs\n",
    "Often times, finding the best hyperparameter values for your model can be an iterative process, needing multiple tuning runs that learn from previous hyperparameter tuning runs. Reusing knowledge from these previous runs will accelerate the hyperparameter tuning process, thereby reducing the cost of tuning the model and will potentially improve the primary metric of the resulting model. When warm starting a hyperparameter tuning experiment with Bayesian sampling, trials from the previous run will be used as prior knowledge to intelligently pick new samples, so as to improve the primary metric. Additionally, when using Random or Grid sampling, any early termination decisions will leverage metrics from the previous runs to determine poorly performing training runs. \n",
    "\n",
    "Azure Machine Learning allows you to warm start your hyperparameter tuning run by leveraging knowledge from up to 5 previously completed hyperparameter tuning parent runs. \n",
    "\n",
    "Additionally, there might be occasions when individual training runs of a hyperparameter tuning experiment are cancelled due to budget constraints or fail due to other reasons. It is now possible to resume such individual training runs from the last checkpoint (assuming your training script handles checkpoints). Resuming an individual training run will use the same hyperparameter configuration and mount the storage used for that run. The training script should accept the \"--resume-from\" argument, which contains the checkpoint or model files from which to resume the training run. You can also resume individual runs as part of an experiment that spends additional budget on hyperparameter tuning. Any additional budget, after resuming the specified training runs is used for exploring additional configurations.\n",
    "\n",
    "For more information on warm starting and resuming hyperparameter tuning runs, please refer to the [Hyperparameter Tuning for Azure Machine Learning documentation](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters) \n",
    "\n",
    "### Find and register best model\n",
    "When all jobs finish, we can find out the one that has the highest accuracy."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "print(best_run.get_details()['runDefinition']['arguments'])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's list the model files uploaded during the run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_run.get_file_names())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then register the folder (and all files in it) as a model named `chainer-dnn-mnist` under the workspace for deployment"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_run.register_model(model_name='chainer-dnn-mnist', model_path='outputs/model.npz')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model in ACI\n",
    "Now, we are ready to deploy the model as a web service running in Azure Container Instance, [ACI](https://azure.microsoft.com/en-us/services/container-instances/). Azure Machine Learning accomplishes this by constructing a Docker image with the scoring logic and model baked in.\n",
    "\n",
    "### Create scoring script\n",
    "First, we will create a scoring script that will be invoked by the web service call.\n",
    "+ Now that the scoring script must have two required functions, `init()` and `run(input_data)`.\n",
    "    + In `init()`, you typically load the model into a global object. This function is executed only once when the Docker contianer is started.\n",
    "    + In `run(input_data)`, the model is used to predict a value based on the input data. The input and output to `run` uses NPZ as the serialization and de-serialization format because it is the preferred format for Chainer, but you are not limited to it.\n",
    "    \n",
    "Refer to the scoring script `chainer_score.py` for this tutorial. Our web service will use this file to predict. When writing your own scoring script, don't forget to test it locally first before you go and deploy the web service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy('chainer_score.py', project_folder)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create myenv.yml\n",
    "We also need to create an environment file so that Azure Machine Learning can install the necessary packages in the Docker image which are required by your scoring script. In this case, we need to specify conda package `numpy` and pip install `chainer`. Please note that you must indicate azureml-defaults with verion >= 1.0.45 as a pip dependency, because it contains the functionality needed to host the model as a web service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import CondaDependencies\n",
    "\n",
    "cd = CondaDependencies.create()\n",
    "cd.add_conda_package('numpy')\n",
    "cd.add_pip_package('chainer==5.1.0')\n",
    "cd.add_pip_package(\"azureml-defaults\")\n",
    "cd.save_to_file(base_directory='./', conda_file_path='myenv.yml')\n",
    "\n",
    "print(cd.serialize_to_string())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy to ACI\n",
    "We are almost ready to deploy. Create the inference configuration and deployment configuration and deploy to ACI. This cell will run for about 7-8 minutes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "\n",
    "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\n",
    "inference_config = InferenceConfig(entry_script=\"chainer_score.py\", environment=myenv)\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1,\n",
    "                                               auth_enabled=True, # this flag generates API keys to secure access\n",
    "                                               memory_gb=1,\n",
    "                                               tags={'name': 'mnist', 'framework': 'Chainer'},\n",
    "                                               description='Chainer DNN with MNIST')\n",
    "\n",
    "service = Model.deploy(workspace=ws, \n",
    "                           name='chainer-mnist-1', \n",
    "                           models=[model], \n",
    "                           inference_config=inference_config, \n",
    "                           deployment_config=aciconfig)\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)\n",
    "print(service.scoring_uri)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip: If something goes wrong with the deployment, the first thing to look at is the logs from the service by running the following command:** "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.get_logs())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the scoring web service endpoint:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.scoring_uri)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the deployed model\n",
    "Let's test the deployed model. Pick a random sample from the test set, and send it to the web service hosted in ACI for a prediction. Note, here we are using the an HTTP request to invoke the service.\n",
    "\n",
    "We can retrieve the API keys used for accessing the HTTP endpoint and construct a raw HTTP request to send to the service. Don't forget to add key to the HTTP header."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retreive the API keys. two keys were generated.\n",
    "key1, Key2 = service.get_keys()\n",
    "print(key1)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "import gzip\n",
    "import numpy as np\n",
    "import struct\n",
    "import requests\n",
    "\n",
    "\n",
    "# load compressed MNIST gz files and return numpy arrays\n",
    "def load_data(filename, label=False):\n",
    "    with gzip.open(filename) as gz:\n",
    "        struct.unpack('I', gz.read(4))\n",
    "        n_items = struct.unpack('>I', gz.read(4))\n",
    "        if not label:\n",
    "            n_rows = struct.unpack('>I', gz.read(4))[0]\n",
    "            n_cols = struct.unpack('>I', gz.read(4))[0]\n",
    "            res = np.frombuffer(gz.read(n_items[0] * n_rows * n_cols), dtype=np.uint8)\n",
    "            res = res.reshape(n_items[0], n_rows * n_cols)\n",
    "        else:\n",
    "            res = np.frombuffer(gz.read(n_items[0]), dtype=np.uint8)\n",
    "            res = res.reshape(n_items[0], 1)\n",
    "    return res\n",
    "\n",
    "os.makedirs('./data/mnist', exist_ok=True)\n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz', filename = './data/mnist/test-images.gz')\n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz', filename = './data/mnist/test-labels.gz')\n",
    "\n",
    "X_test = load_data('./data/mnist/test-images.gz', False)\n",
    "y_test = load_data('./data/mnist/test-labels.gz', True).reshape(-1)\n",
    "\n",
    "\n",
    "# send a random row from the test set to score\n",
    "random_index = np.random.randint(0, len(X_test)-1)\n",
    "input_data = \"{\\\"data\\\": [\" + str(random_index) + \"]}\"\n",
    "\n",
    "headers = {'Content-Type':'application/json', 'Authorization': 'Bearer ' + key1}\n",
    "\n",
    "# send sample to service for scoring\n",
    "resp = requests.post(service.scoring_uri, input_data, headers=headers)\n",
    "\n",
    "print(\"label:\", y_test[random_index])\n",
    "print(\"prediction:\", resp.text[1])\n",
    "\n",
    "plt.imshow(X_test[random_index].reshape((28,28)), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the workspace after the web service was deployed. You should see\n",
    "\n",
    " + a registered model named 'chainer-dnn-mnist' and with the id 'chainer-dnn-mnist:1'\n",
    " + a webservice called 'chainer-mnist-svc' with some scoring URL"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ws.models['chainer-dnn-mnist']\n",
    "print(\"Model: {}, ID: {}\".format('chainer-dnn-mnist', model.id))\n",
    "       \n",
    "webservice = ws.webservices['chainer-mnist-1']\n",
    "print(\"Webservice: {}, scoring URI: {}\".format('chainer-mnist-1', webservice.scoring_uri))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can delete the ACI deployment with a simple delete API call."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/chainer/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm start hyperparameter tuning\n",
    "In this tutorial, you will learn how to warm start a hyperparameter tuning run from a previous tuning run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started. First let's import some Python libraries."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "c377ea0c-0cd9-4345-9be2-e20fb29c94c3"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "edaa7f2f-2439-4148-b57a-8c794c0945ec"
    }
   },
   "outputs": [],
   "source": [
    "import azureml\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "Opt-in diagnostics for better experience, quality, and security of future releases."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Diagnostics"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.telemetry import set_diagnostics_collection\n",
    "\n",
    "set_diagnostics_collection(send_diagnostics=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "59f52294-4a25-4c92-bab8-3b07f0f44d15"
    }
   },
   "source": [
    "## Create an Azure ML experiment\n",
    "Let's create an experiment named \"tf-mnist\" and a folder to hold the training scripts. The script runs will be recorded under the experiment in Azure."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "bc70f780-c240-4779-96f3-bc5ef9a37d59"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "script_folder = './tf-mnist'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "\n",
    "exp = Experiment(workspace=ws, name='tf-mnist-2')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "defe921f-8097-44c3-8336-8af6700804a7"
    }
   },
   "source": [
    "## Download MNIST dataset\n",
    "In order to train on the MNIST dataset we will first need to download it from Yan LeCun's web site directly and save them in a `data` folder locally."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "data_folder = 'data'\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "urllib.request.urlretrieve('https://azureopendatastorage.blob.core.windows.net/mnist/train-images-idx3-ubyte.gz',\n",
    "                           filename=os.path.join(data_folder, 'train-images-idx3-ubyte.gz'))\n",
    "urllib.request.urlretrieve('https://azureopendatastorage.blob.core.windows.net/mnist/train-labels-idx1-ubyte.gz',\n",
    "                           filename=os.path.join(data_folder, 'train-labels-idx1-ubyte.gz'))\n",
    "urllib.request.urlretrieve('https://azureopendatastorage.blob.core.windows.net/mnist/t10k-images-idx3-ubyte.gz',\n",
    "                           filename=os.path.join(data_folder, 't10k-images-idx3-ubyte.gz'))\n",
    "urllib.request.urlretrieve('https://azureopendatastorage.blob.core.windows.net/mnist/t10k-labels-idx1-ubyte.gz',\n",
    "                           filename=os.path.join(data_folder, 't10k-labels-idx1-ubyte.gz'))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c3f2f57c-7454-4d3e-b38d-b0946cf066ea"
    }
   },
   "source": [
    "## Show some sample images\n",
    "Let's load the downloaded compressed file into numpy arrays using some utility functions included in the `utils.py` library file from the current folder. Then we use `matplotlib` to plot 30 random images from the dataset along with their labels."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "396d478b-34aa-4afa-9898-cdce8222a516"
    }
   },
   "outputs": [],
   "source": [
    "from utils import load_data\n",
    "\n",
    "# note we also shrink the intensity values (X) from 0-255 to 0-1. This helps the model converge faster.\n",
    "X_train = load_data(os.path.join(data_folder, 'train-images-idx3-ubyte.gz'), False) / 255.0\n",
    "X_test = load_data(os.path.join(data_folder, 't10k-images-idx3-ubyte.gz'), False) / 255.0\n",
    "y_train = load_data(os.path.join(data_folder, 'train-labels-idx1-ubyte.gz'), True).reshape(-1)\n",
    "y_test = load_data(os.path.join(data_folder, 't10k-labels-idx1-ubyte.gz'), True).reshape(-1)\n",
    "\n",
    "# now let's show some randomly chosen images from the training set.\n",
    "count = 0\n",
    "sample_size = 30\n",
    "plt.figure(figsize = (16, 6))\n",
    "for i in np.random.permutation(X_train.shape[0])[:sample_size]:\n",
    "    count = count + 1\n",
    "    plt.subplot(1, sample_size, count)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    plt.text(x=10, y=-10, s=y_train[i], fontsize=18)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap=plt.cm.Greys)\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a FileDataset\n",
    "A FileDataset references single or multiple files in your datastores or public urls. The files can be of any format. FileDataset provides you with the ability to download or mount the files to your compute. By creating a dataset, you create a reference to the data source location. If you applied any subsetting transformations to the dataset, they will be stored in the dataset as well. The data remains in its existing location, so no extra storage cost is incurred. [Learn More](https://aka.ms/azureml/howto/createdatasets)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "web_paths = ['https://azureopendatastorage.blob.core.windows.net/mnist/train-images-idx3-ubyte.gz',\n",
    "             'https://azureopendatastorage.blob.core.windows.net/mnist/train-labels-idx1-ubyte.gz',\n",
    "             'https://azureopendatastorage.blob.core.windows.net/mnist/t10k-images-idx3-ubyte.gz',\n",
    "             'https://azureopendatastorage.blob.core.windows.net/mnist/t10k-labels-idx1-ubyte.gz'\n",
    "            ]\n",
    "dataset = Dataset.File.from_files(path = web_paths)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the register() method to register datasets to your workspace so they can be shared with others, reused across various experiments, and referred to by name in your training script.\n",
    "You can try get the dataset first to see if it's already registered."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_registered = False\n",
    "try:\n",
    "    temp = Dataset.get_by_name(workspace = ws, name = 'mnist-dataset')\n",
    "    dataset_registered = True\n",
    "except:\n",
    "    print(\"The dataset mnist-dataset is not registered in workspace yet.\")\n",
    "\n",
    "if not dataset_registered:\n",
    "    dataset = dataset.register(workspace = ws,\n",
    "                               name = 'mnist-dataset',\n",
    "                               description='training and test dataset',\n",
    "                               create_new_version=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Attach existing AmlCompute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training your model. In this tutorial, you create `AmlCompute` as your training compute resource."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we could not find the cluster with the given name, then we will create a new cluster here. We will create an `AmlCompute` cluster of `STANDARD_NC6` GPU VMs. This process is broken down into 3 steps:\n",
    "1. create the configuration (this step is local and only takes a second)\n",
    "2. create the cluster (this step will take about **20 seconds**)\n",
    "3. provision the VMs to bring the cluster to the initial size (of 1 in this case). This step will take about **3-5 minutes** and is providing only sparse output in the process. Please make sure to wait until the call returns before moving to the next cell"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"gpu-cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6',\n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "# can poll for a minimum number of nodes and for a specific timeout. \n",
    "# if no min node count is provided it uses the scale settings for the cluster\n",
    "compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have created the compute target, let's see what the workspace's `compute_targets` property returns. You should now see one entry named 'gpu-cluster' of type `AmlCompute`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_targets = ws.compute_targets\n",
    "for name, ct in compute_targets.items():\n",
    "    print(name, ct.type, ct.provisioning_state)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the training files into the script folder\n",
    "The TensorFlow training script is already created for you. You can simply copy it into the script folder, together with the utility library used to load compressed data file into numpy array."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# the training logic is in the tf_mnist.py file.\n",
    "shutil.copy('./tf_mnist.py', script_folder)\n",
    "\n",
    "# the utils.py just helps loading data from the downloaded MNIST dataset into numpy arrays.\n",
    "shutil.copy('./utils.py', script_folder)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2039d2d5-aca6-4f25-a12f-df9ae6529cae"
    }
   },
   "source": [
    "## Construct neural network in TensorFlow\n",
    "In the training script `tf_mnist.py`, it creates a very simple DNN (deep neural network), with just 2 hidden layers. The input layer has 28 * 28 = 784 neurons, each representing a pixel in an image. The first hidden layer has 300 neurons, and the second hidden layer has 100 neurons. The output layer has 10 neurons, each representing a targeted label from 0 to 9.\n",
    "\n",
    "![DNN](nn.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure ML concepts  \n",
    "Please note the following three things in the code below:\n",
    "1. The script accepts arguments using the argparse package. In this case there is one argument `--data_folder` which specifies the file system folder in which the script can find the MNIST data\n",
    "```\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_folder')\n",
    "```\n",
    "2. The script is accessing the Azure ML `Run` object by executing `run = Run.get_context()`. Further down the script is using the `run` to report the training accuracy and the validation accuracy as training progresses.\n",
    "```\n",
    "    run.log('training_acc', np.float(acc_train))\n",
    "    run.log('validation_acc', np.float(acc_val))\n",
    "```\n",
    "3. When running the script on Azure ML, you can write files out to a folder `./outputs` that is relative to the root directory. This folder is specially tracked by Azure ML in the sense that any files written to that folder during script execution on the remote target will be picked up by Run History; these files (known as artifacts) will be available as part of the run history record."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will print out the training code for you to inspect it."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(script_folder, './tf_mnist.py'), 'r') as f:\n",
    "    print(f.read())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an environment\n",
    "\n",
    "In this tutorial, we will use one of Azure ML's curated TensorFlow environments for training. [Curated environments](https://docs.microsoft.com/azure/machine-learning/how-to-use-environments#use-a-curated-environment) are available in your workspace by default. Specifically, we will use the TensorFlow 2.0 GPU curated environment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "tf_env = Environment.get(ws, name='AzureML-TensorFlow-2.0-GPU')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the training job\u00c2\u00b6\n",
    "Create a ScriptRunConfig object to specify the configuration details of your training job, including your training script, environment to use, and the compute target to run on."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "args = ['--data-folder', dataset.as_mount(),\n",
    "        '--batch-size', 64,\n",
    "        '--first-layer-neurons', 256,\n",
    "        '--second-layer-neurons', 128,\n",
    "        '--learning-rate', 0.01]\n",
    "\n",
    "src = ScriptRunConfig(source_directory=script_folder,\n",
    "                      script='tf_mnist.py',\n",
    "                      arguments=args,\n",
    "                      compute_target=compute_target,\n",
    "                      environment=tf_env)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit job to run\n",
    "Submit the ScriptRunConfig to an Azure ML experiment to kick off the execution."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(src)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intelligent hyperparameter tuning\n",
    "Now that we have trained the model with one set of hyperparameters, we can tune the model hyperparameters to optimize model performance. First let's define the parameter space using random sampling. Typically, the hyperparameter exploration process is painstakingly manual, given that the search space is vast and evaluation of each configuration can be expensive.\n",
    "\n",
    "Azure Machine Learning allows you to automate hyperparameter exploration in an efficient manner, saving you significant time and resources. You specify the range of hyperparameter values and a maximum number of training runs. The system then automatically launches multiple simultaneous runs with different parameter configurations and finds the configuration that results in the best performance, measured by the metric you choose. Poorly performing training runs are automatically early terminated, reducing wastage of compute resources. These resources are instead used to explore other hyperparameter configurations."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining the hyperparameter space. In this case, we will tune 4 hyperparameters - '--batch-size', '--first-layer-neurons', '--second-layer-neurons' and '--learning-rate'. For each of these hyperparameters, we specify the range of values they can take. In this example, we will use Random Sampling to randomly select hyperparameter values from the defined search space."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import RandomParameterSampling, choice, loguniform\n",
    "\n",
    "ps = RandomParameterSampling(\n",
    "    {\n",
    "        '--batch-size': choice(32, 64, 128),\n",
    "        '--first-layer-neurons': choice(16, 64, 128, 256, 512),\n",
    "        '--second-layer-neurons': choice(16, 64, 256, 512),\n",
    "        '--learning-rate': loguniform(-6, -1)\n",
    "    }\n",
    ")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a new ScriptRunConfig without the above parameters since they will be passed in later. Note we still need to keep the `data-folder` parameter since that's not a hyperparamter we will sweep."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ['--data-folder', dataset.as_mount()]\n",
    "\n",
    "src = ScriptRunConfig(source_directory=script_folder,\n",
    "                      script='tf_mnist.py',\n",
    "                      arguments=args,\n",
    "                      compute_target=compute_target,\n",
    "                      environment=tf_env)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will define an early termnination policy. This will terminate poorly performing runs automatically, reducing wastage of resources and instead efficiently using these resources for exploring other parameter configurations. In this example, we will use the `TruncationSelectionPolicy`, truncating the bottom performing 25% runs. It states to check the job every 2 iterations. If the primary metric (defined later) falls in the bottom 25% range, Azure ML terminate the job. This saves us from continuing to explore hyperparameters that don't show promise of helping reach our target metric."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import TruncationSelectionPolicy\n",
    "policy = TruncationSelectionPolicy(evaluation_interval=2, truncation_percentage=25)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to configure a run configuration object, and specify the primary metric `validation_acc` that's recorded in your training runs. If you go back to visit the training script, you will notice that this value is being logged after every epoch (a full batch set). We also want to tell the service that we are looking to maximizing this value. We also set the number of samples to 15, and maximal concurrent job to 4, which is the same as the number of nodes in our computer cluster."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import HyperDriveConfig, PrimaryMetricGoal\n",
    "htc = HyperDriveConfig(run_config=src, \n",
    "                       hyperparameter_sampling=ps, \n",
    "                       policy=policy, \n",
    "                       primary_metric_name='validation_acc', \n",
    "                       primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "                       max_total_runs=15,\n",
    "                       max_concurrent_runs=4)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's launch the hyperparameter tuning job."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htr = exp.submit(config=htc)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a run history widget to show the progress. Be patient as this might take a while to complete."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(htr).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htr.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(htr.get_status() == \"Completed\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and register best model <a class=\"anchor\" id=\"register-model\"></a>\n",
    "When all the jobs finish, we can find out the one that has the highest accuracy."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = htr.get_best_run_by_primary_metric()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's list the model files uploaded during the run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_run.get_file_names())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then register the folder (and all files in it) as a model named `tf-dnn-mnist` under the workspace for deployment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_run.register_model(model_name='tf-dnn-mnist', model_path='outputs/model')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm start a Hyperparameter Tuning experiment\n",
    "Often times, finding the best hyperparameter values for your model can be an iterative process, needing multiple tuning runs that learn from previous hyperparameter tuning runs. Reusing knowledge from these previous runs will accelerate the hyperparameter tuning process, thereby reducing the cost of tuning the model and will potentially improve the primary metric of the resulting model. When warm starting a hyperparameter tuning experiment with Bayesian sampling, trials from the previous run will be used as prior knowledge to intelligently pick new samples, so as to improve the primary metric. Additionally, when using Random or Grid sampling, any early termination decisions will leverage metrics from the previous runs to determine poorly performing training runs. \n",
    "\n",
    "Azure Machine Learning allows you to warm start your hyperparameter tuning run by leveraging knowledge from up to 5 previously completed hyperparameter tuning parent runs. In this example, we shall warm start from the initial hyperparameter tuning run in this notebook"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_start_parents_to_resume_from=[htr]\n",
    "\n",
    "warm_start_htc = HyperDriveConfig(run_config=src, \n",
    "                       hyperparameter_sampling=ps, \n",
    "                       policy=policy, \n",
    "                       resume_from=warm_start_parents_to_resume_from, \n",
    "                       primary_metric_name='validation_acc', \n",
    "                       primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "                       max_total_runs=20,\n",
    "                       max_concurrent_runs=4)\n",
    "\n",
    "warm_start_htr = exp.submit(config=warm_start_htc)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the run history widget to show the progress of this warm start run. Be patient as this might take a while to complete."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(warm_start_htr).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_start_htr.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find and register best model from the warm start run\n",
    "When all the jobs finish, we can find out the one that has the highest accuracy and register the folder (and all files in it) as a model named tf-dnn-mnist-warm-start under the workspace for deployment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_warm_start_run = warm_start_htr.get_best_run_by_primary_metric()\n",
    "warm_start_model = best_warm_start_run.register_model(model_name='tf-dnn-mnist-warm-start', model_path='outputs/model')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resuming individual training runs in a hyperparameter tuning experiment\n",
    "\n",
    "In the previous section, we saw how you can warm start a hyperparameter tuning run, to learn from a previously completed run. Additionally, there might be occasions when individual training runs of a hyperparameter tuning experiment are cancelled due to budget constraints or fail due to other reasons. It is now possible to resume such individual training runs from the last checkpoint (assuming your training script handles checkpoints). Resuming an individual training run will use the same hyperparameter configuration and mount the storage used for that run. The training script should accept the \"--resume-from\" argument, which contains the checkpoint or model files from which to resume the training run. \n",
    "\n",
    "You can also resume individual runs as part of an experiment that spends additional budget on hyperparameter tuning. Any additional budget, after resuming the specified training runs is used for exploring additional configurations.\n",
    "\n",
    "In this example, we will resume one of the child runs cancelled in the previous hyperparameter tuning run in this notebook"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancelled_child_runs = []\n",
    "for child_run in htr.get_children():\n",
    "    if child_run.status == \"Canceled\":\n",
    "        cancelled_child_runs.append(child_run)\n",
    "        \n",
    "if len(cancelled_child_runs) != 0:\n",
    "    child_runs_to_resume=[cancelled_child_runs[0]]\n",
    "else:\n",
    "    child_runs_to_resume=[]"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will configure the hyperparameter tuning experiment to warm start from the previous experiment and resume individual training runs and submit this warm start hyperparameter tuning run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_child_runs_htc = HyperDriveConfig(run_config=src, \n",
    "                       hyperparameter_sampling=ps, \n",
    "                       policy=policy, \n",
    "                       resume_child_runs=child_runs_to_resume, \n",
    "                       primary_metric_name='validation_acc', \n",
    "                       primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "                       max_total_runs=10,\n",
    "                       max_concurrent_runs=4)\n",
    "\n",
    "resume_child_runs_htr = exp.submit(config=resume_child_runs_htc)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can use the run history widget to show the progress of this resumed run. Be patient as this might take a while to complete."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(resume_child_runs_htr).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_child_runs_htr.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find and register best model from the resumed run\n",
    "When all the jobs finish, we can find out the one that has the highest accuracy and register the folder (and all files in it) as a model named tf-dnn-mnist-resumed under the workspace for deployment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_resume_child_run = resume_child_runs_htr.get_best_run_by_primary_metric()\n",
    "resume_child_run_model = best_resume_child_run.register_model(model_name='tf-dnn-mnist-resumed', model_path='outputs/model')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow/hyperparameter-tune-and-warm-start-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed TensorFlow with parameter server\n",
    "In this tutorial, you will train a TensorFlow model on the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset using native [distributed TensorFlow](https://www.tensorflow.org/deploy/distributed)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* Understand the [architecture and terms](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning (AML)\n",
    "* If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration notebook](../../../../configuration.ipynb) to:\n",
    "    * install the AML SDK\n",
    "    * create a workspace and its configuration file (`config.json`)\n",
    "* Review the [tutorial](../train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb) on single-node TensorFlow training using the SDK"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "Opt-in diagnostics for better experience, quality, and security of future releases."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Diagnostics"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.telemetry import set_diagnostics_collection\n",
    "\n",
    "set_diagnostics_collection(send_diagnostics=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Attach existing AmlCompute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training your model. In this tutorial, you create `AmlCompute` as your training compute resource.\n",
    "\n",
    "**Creation of AmlCompute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace this code will skip the creation process.\n",
    "\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"gpu-cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', \n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model on the remote compute\n",
    "Now that we have the cluster ready to go, let's run our distributed training job."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a project directory\n",
    "Create a directory that will contain all the necessary code from your local machine that you will need access to on the remote resource. This includes the training script, and any additional files your training script depends on."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "project_folder = './tf-distr-ps'\n",
    "os.makedirs(project_folder, exist_ok=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the training script `tf_mnist_replica.py` into this project directory."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copy('tf_mnist_replica.py', project_folder)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiment\n",
    "Create an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment) to track all the runs in your workspace for this distributed TensorFlow tutorial. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'tf-distr-ps'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an environment\n",
    "\n",
    "In this tutorial, we will use one of Azure ML's curated TensorFlow environments for training. [Curated environments](https://docs.microsoft.com/azure/machine-learning/how-to-use-environments#use-a-curated-environment) are available in your workspace by default. Specifically, we will use the TensorFlow 1.13 GPU curated environment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "tf_env = Environment.get(ws, name='AzureML-TensorFlow-1.13-GPU')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the training job\n",
    "\n",
    "Create a ScriptRunConfig object to specify the configuration details of your training job, including your training script, environment to use, and the compute target to run on.\n",
    "\n",
    "In order to execute a distributed TensorFlow run with the parameter server strategy, you must create a `TensorflowConfiguration` object and pass it to the `distributed_job_config` parameter of the ScriptRunConfig constructor. The below code configures a distributed TensorFlow run with `2` workers and `1` parameter server."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.runconfig import TensorflowConfiguration\n",
    "\n",
    "src = ScriptRunConfig(source_directory=project_folder,\n",
    "                      script='tf_mnist_replica.py',\n",
    "                      arguments=['--num_gpus', 1, '--train_steps', 500],\n",
    "                      compute_target=compute_target,\n",
    "                      environment=tf_env,\n",
    "                      distributed_job_config=TensorflowConfiguration(worker_count=2, parameter_server_count=1))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job\n",
    "Run your experiment by submitting your ScriptRunConfig object. Note that this call is asynchronous."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(src)\n",
    "print(run)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor your run\n",
    "You can monitor the progress of the run with a Jupyter widget. Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can block until the script has completed training before running more code."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True) # this provides a verbose log"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-parameter-server/distributed-tensorflow-with-parameter-server.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resuming Tensorflow training from previous run\n",
    "In this tutorial, you will resume a mnist model in TensorFlow from a previously submitted run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* Understand the [architecture and terms](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning (AML)\n",
    "* Go through the [configuration notebook](../../../../configuration.ipynb) to:\n",
    "    * install the AML SDK\n",
    "    * create a workspace and its configuration file (`config.json`)\n",
    "* Review the [tutorial](../train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb) on single-node TensorFlow training using the SDK"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "Opt-in diagnostics for better experience, quality, and security of future releases."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Diagnostics"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.telemetry import set_diagnostics_collection\n",
    "\n",
    "set_diagnostics_collection(send_diagnostics=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Attach existing AmlCompute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training your model. In this tutorial, you create `AmlCompute` as your training compute resource.\n",
    "\n",
    "**Creation of AmlCompute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace this code will skip the creation process.\n",
    "\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"gpu-cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', \n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code creates a GPU cluster. If you instead want to create a CPU cluster, provide a different VM size to the `vm_size` parameter, such as `STANDARD_D2_V2`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dataset for Files\n",
    "A Dataset can reference single or multiple files in your datastores or public urls. The files can be of any format. Dataset provides you with the ability to download or mount the files to your compute. By creating a dataset, you create a reference to the data source location. If you applied any subsetting transformations to the dataset, they will be stored in the dataset as well. The data remains in its existing location, so no extra storage cost is incurred. [Learn More](https://aka.ms/azureml/howto/createdatasets)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize file dataset \n",
    "from azureml.core.dataset import Dataset\n",
    "web_paths = ['http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
    "             'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
    "             'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
    "             'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'\n",
    "            ]\n",
    "dataset = Dataset.File.from_files(path = web_paths)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you may want to register datasets using the register() method to your workspace so they can be shared with others, reused across various experiments, and referred to by name in your training script.\n",
    "You can try get the dataset first to see if it's already registered."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_registered = False\n",
    "try:\n",
    "    temp = Dataset.get_by_name(workspace = ws, name = 'mnist-dataset')\n",
    "    dataset_registered = True\n",
    "except:\n",
    "    print(\"The dataset mnist-dataset is not registered in workspace yet.\")\n",
    "\n",
    "if not dataset_registered:\n",
    "    #register dataset to workspace\n",
    "    dataset = dataset.register(workspace = ws,\n",
    "                               name = 'mnist-dataset',\n",
    "                               description='training and test dataset',\n",
    "                               create_new_version=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the files referenced by dataset\n",
    "dataset.to_path()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model on the remote compute"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a project directory\n",
    "Create a directory that will contain all the necessary code from your local machine that you will need access to on the remote resource. This includes the training script, and any additional files your training script depends on."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = './tf-resume-training'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the training script `tf_mnist_with_checkpoint.py` into this project directory."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# the training logic is in the tf_mnist_with_checkpoint.py file.\n",
    "shutil.copy('./tf_mnist_with_checkpoint.py', script_folder)\n",
    "\n",
    "# the utils.py just helps loading data from the downloaded MNIST dataset into numpy arrays.\n",
    "shutil.copy('./utils.py', script_folder)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiment\n",
    "Create an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment) to track all the runs in your workspace for this distributed TensorFlow tutorial. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'tf-resume-training'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an environment\n",
    "\n",
    "In this tutorial, we will use one of Azure ML's curated TensorFlow environments for training. [Curated environments](https://docs.microsoft.com/azure/machine-learning/how-to-use-environments#use-a-curated-environment) are available in your workspace by default. Specifically, we will use the TensorFlow 1.13 GPU curated environment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "tf_env = Environment.get(ws, name='AzureML-TensorFlow-1.13-GPU')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the training job\n",
    "\n",
    "Create a ScriptRunConfig object to specify the configuration details of your training job, including your training script, environment to use, and the compute target to run on."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "args = ['--data-folder', dataset.as_mount()]\n",
    "\n",
    "src = ScriptRunConfig(source_directory=script_folder,\n",
    "                      script='tf_mnist_with_checkpoint.py',\n",
    "                      arguments=args,\n",
    "                      compute_target=compute_target,\n",
    "                      environment=tf_env)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job\n",
    "Run your experiment by submitting your ScriptRunConfig object. Note that this call is asynchronous."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(src)\n",
    "print(run)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor your run\n",
    "You can monitor the progress of the run with a Jupyter widget. Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can block until the script has completed training before running more code."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's resume training from the above run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will get the DataPath to the outputs directory of the above run which\n",
    "contains the checkpoint files. We will create a DataReference from this DataPath and specify the compute binding as mount mode; this will tell Azure ML to mount the checkpoint files on the compute target for the run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data.datapath import DataPathComputeBinding\n",
    "\n",
    "checkpoint_path = run._get_outputs_datapath()\n",
    "checkpoint_data_ref = checkpoint_path.create_data_reference(datapath_compute_binding=DataPathComputeBinding(mode=\"mount\"))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create a new ScriptRunConfig and append the additional `'--resume-from'` argument with the corresponding checkpoint location to the `arguments` parameter."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "args = ['--data-folder', dataset.as_mount(),\n",
    "        '--resume-from', str(checkpoint_data_ref)]\n",
    "\n",
    "src = ScriptRunConfig(source_directory=script_folder,\n",
    "                      script='tf_mnist_with_checkpoint.py',\n",
    "                      arguments=args,\n",
    "                      compute_target=compute_target,\n",
    "                      environment=tf_env)\n",
    "\n",
    "src.run_config.data_references = {checkpoint_data_ref.data_reference_name : checkpoint_data_ref.to_config()}"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can submit the experiment and it should resume from previous run's checkpoint files."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumed_run = experiment.submit(src)\n",
    "print(resumed_run)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumed_run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-tensorflow-resume-training/train-tensorflow-resume-training.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bf74d2e9-2708-49b1-934b-e0ede342f475"
    }
   },
   "source": [
    "# Training, hyperparameter tune, and deploy with TensorFlow\n",
    "\n",
    "## Introduction\n",
    "This tutorial shows how to train a simple deep neural network using the MNIST dataset and TensorFlow on Azure Machine Learning. MNIST is a popular dataset consisting of 70,000 grayscale images. Each image is a handwritten digit of `28x28` pixels, representing number from 0 to 9. The goal is to create a multi-class classifier to identify the digit each image represents, and deploy it as a web service in Azure.\n",
    "\n",
    "For more information about the MNIST dataset, please visit [Yan LeCun's website](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "## Prerequisite:\n",
    "* Understand the [architecture and terms](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning\n",
    "* If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration notebook](../../../../configuration.ipynb) to:\n",
    "    * install the AML SDK\n",
    "    * create a workspace and its configuration file (`config.json`)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started. First let's import some Python libraries."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "c377ea0c-0cd9-4345-9be2-e20fb29c94c3"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "edaa7f2f-2439-4148-b57a-8c794c0945ec"
    }
   },
   "outputs": [],
   "source": [
    "import azureml\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "Opt-in diagnostics for better experience, quality, and security of future releases."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Diagnostics"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.telemetry import set_diagnostics_collection\n",
    "\n",
    "set_diagnostics_collection(send_diagnostics=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "59f52294-4a25-4c92-bab8-3b07f0f44d15"
    }
   },
   "source": [
    "## Create an Azure ML experiment\n",
    "Let's create an experiment named \"tf-mnist\" and a folder to hold the training scripts. The script runs will be recorded under the experiment in Azure."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "bc70f780-c240-4779-96f3-bc5ef9a37d59"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "script_folder = './tf-mnist'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "\n",
    "exp = Experiment(workspace=ws, name='tf-mnist')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "defe921f-8097-44c3-8336-8af6700804a7"
    }
   },
   "source": [
    "## Download MNIST dataset\n",
    "In order to train on the MNIST dataset we will first need to download it from azuremlopendatasets blob directly and save them in a `data` folder locally. If you want you can directly download the same data from Yan LeCun's web site."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "data_folder = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "urllib.request.urlretrieve('https://azureopendatastorage.blob.core.windows.net/mnist/train-images-idx3-ubyte.gz',\n",
    "                           filename=os.path.join(data_folder, 'train-images-idx3-ubyte.gz'))\n",
    "urllib.request.urlretrieve('https://azureopendatastorage.blob.core.windows.net/mnist/train-labels-idx1-ubyte.gz',\n",
    "                           filename=os.path.join(data_folder, 'train-labels-idx1-ubyte.gz'))\n",
    "urllib.request.urlretrieve('https://azureopendatastorage.blob.core.windows.net/mnist/t10k-images-idx3-ubyte.gz',\n",
    "                           filename=os.path.join(data_folder, 't10k-images-idx3-ubyte.gz'))\n",
    "urllib.request.urlretrieve('https://azureopendatastorage.blob.core.windows.net/mnist/t10k-labels-idx1-ubyte.gz',\n",
    "                           filename=os.path.join(data_folder, 't10k-labels-idx1-ubyte.gz'))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c3f2f57c-7454-4d3e-b38d-b0946cf066ea"
    }
   },
   "source": [
    "## Show some sample images\n",
    "Let's load the downloaded compressed file into numpy arrays using some utility functions included in the `utils.py` library file from the current folder. Then we use `matplotlib` to plot 30 random images from the dataset along with their labels."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "396d478b-34aa-4afa-9898-cdce8222a516"
    }
   },
   "outputs": [],
   "source": [
    "from utils import load_data\n",
    "\n",
    "# note we also shrink the intensity values (X) from 0-255 to 0-1. This helps the neural network converge faster.\n",
    "X_train = load_data(os.path.join(data_folder, 'train-images-idx3-ubyte.gz'), False) / np.float32(255.0)\n",
    "X_test = load_data(os.path.join(data_folder, 't10k-images-idx3-ubyte.gz'), False) / np.float32(255.0)\n",
    "y_train = load_data(os.path.join(data_folder, 'train-labels-idx1-ubyte.gz'), True).reshape(-1)\n",
    "y_test = load_data(os.path.join(data_folder, 't10k-labels-idx1-ubyte.gz'), True).reshape(-1)\n",
    "\n",
    "count = 0\n",
    "sample_size = 30\n",
    "plt.figure(figsize = (16, 6))\n",
    "for i in np.random.permutation(X_train.shape[0])[:sample_size]:\n",
    "    count = count + 1\n",
    "    plt.subplot(1, sample_size, count)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    plt.text(x = 10, y = -10, s = y_train[i], fontsize = 18)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap = plt.cm.Greys)\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dataset for Files\n",
    "A Dataset can reference single or multiple files in your datastores or public urls. The files can be of any format. Dataset provides you with the ability to download or mount the files to your compute. By creating a dataset, you create a reference to the data source location. If you applied any subsetting transformations to the dataset, they will be stored in the dataset as well. The data remains in its existing location, so no extra storage cost is incurred. [Learn More](https://aka.ms/azureml/howto/createdatasets)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "web_paths = ['https://azureopendatastorage.blob.core.windows.net/mnist/train-images-idx3-ubyte.gz',\n",
    "             'https://azureopendatastorage.blob.core.windows.net/mnist/train-labels-idx1-ubyte.gz',\n",
    "             'https://azureopendatastorage.blob.core.windows.net/mnist/t10k-images-idx3-ubyte.gz',\n",
    "             'https://azureopendatastorage.blob.core.windows.net/mnist/t10k-labels-idx1-ubyte.gz'\n",
    "            ]\n",
    "dataset = Dataset.File.from_files(path = web_paths)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to regiester datasets using the register() method to your workspace so they can be shared with others, reused across various experiments, and referred to by name in your training script.\n",
    "You can try get the dataset first to see if it's already registered."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_registered = False\n",
    "try:\n",
    "    temp = Dataset.get_by_name(workspace = ws, name = 'mnist-dataset')\n",
    "    dataset_registered = True\n",
    "except:\n",
    "    print(\"The dataset mnist-dataset is not registered in workspace yet.\")\n",
    "\n",
    "if not dataset_registered:\n",
    "    dataset = dataset.register(workspace = ws,\n",
    "                               name = 'mnist-dataset',\n",
    "                               description='training and test dataset',\n",
    "                               create_new_version=True)\n",
    "# list the files referenced by dataset\n",
    "dataset.to_path()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Attach existing AmlCompute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training your model. In this tutorial, you create `AmlCompute` as your training compute resource."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we could not find the cluster with the given name, then we will create a new cluster here. We will create an `AmlCompute` cluster of `STANDARD_NC6` GPU VMs. This process is broken down into 3 steps:\n",
    "1. create the configuration (this step is local and only takes a second)\n",
    "2. create the cluster (this step will take about **20 seconds**)\n",
    "3. provision the VMs to bring the cluster to the initial size (of 1 in this case). This step will take about **3-5 minutes** and is providing only sparse output in the process. Please make sure to wait until the call returns before moving to the next cell"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"gpu-cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', \n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "# can poll for a minimum number of nodes and for a specific timeout. \n",
    "# if no min node count is provided it uses the scale settings for the cluster\n",
    "compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have created the compute target, let's see what the workspace's `compute_targets` property returns. You should now see one entry named 'gpu-cluster' of type `AmlCompute`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_targets = ws.compute_targets\n",
    "for name, ct in compute_targets.items():\n",
    "    print(name, ct.type, ct.provisioning_state)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the training files into the script folder\n",
    "The TensorFlow training script is already created for you. You can simply copy it into the script folder, together with the utility library used to load compressed data file into numpy array."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# the training logic is in the tf_mnist.py file.\n",
    "shutil.copy('./tf_mnist.py', script_folder)\n",
    "\n",
    "# the utils.py just helps loading data from the downloaded MNIST dataset into numpy arrays.\n",
    "shutil.copy('./utils.py', script_folder)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2039d2d5-aca6-4f25-a12f-df9ae6529cae"
    }
   },
   "source": [
    "## Construct neural network in TensorFlow\n",
    "In the training script `tf_mnist.py`, it creates a very simple DNN (deep neural network), with just 2 hidden layers. The input layer has 28 * 28 = 784 neurons, each representing a pixel in an image. The first hidden layer has 300 neurons, and the second hidden layer has 100 neurons. The output layer has 10 neurons, each representing a targeted label from 0 to 9.\n",
    "\n",
    "![DNN](nn.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure ML concepts  \n",
    "Please note the following three things in the code below:\n",
    "1. The script accepts arguments using the argparse package. In this case there is one argument `--data_folder` which specifies the file system folder in which the script can find the MNIST data\n",
    "```\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_folder')\n",
    "```\n",
    "2. The script is accessing the Azure ML `Run` object by executing `run = Run.get_context()`. Further down the script is using the `run` to report the training accuracy and the validation accuracy as training progresses.\n",
    "```\n",
    "    run.log('training_acc', np.float(acc_train))\n",
    "    run.log('validation_acc', np.float(acc_val))\n",
    "```\n",
    "3. When running the script on Azure ML, you can write files out to a folder `./outputs` that is relative to the root directory. This folder is specially tracked by Azure ML in the sense that any files written to that folder during script execution on the remote target will be picked up by Run History; these files (known as artifacts) will be available as part of the run history record."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will print out the training code for you to inspect it."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(script_folder, './tf_mnist.py'), 'r') as f:\n",
    "    print(f.read())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an environment\n",
    "\n",
    "In this tutorial, we will use one of Azure ML's curated TensorFlow environments for training. [Curated environments](https://docs.microsoft.com/azure/machine-learning/how-to-use-environments#use-a-curated-environment) are available in your workspace by default. Specifically, we will use the TensorFlow 2.0 GPU curated environment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "tf_env = Environment.get(ws, name='AzureML-TensorFlow-2.0-GPU')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the training job\n",
    "\n",
    "Create a ScriptRunConfig object to specify the configuration details of your training job, including your training script, environment to use, and the compute target to run on."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "args = ['--data-folder', dataset.as_named_input('mnist').as_mount(),\n",
    "        '--batch-size', 64,\n",
    "        '--first-layer-neurons', 256,\n",
    "        '--second-layer-neurons', 128,\n",
    "        '--learning-rate', 0.01]\n",
    "\n",
    "src = ScriptRunConfig(source_directory=script_folder,\n",
    "                      script='tf_mnist.py',\n",
    "                      arguments=args,\n",
    "                      compute_target=compute_target,\n",
    "                      environment=tf_env)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit job to run\n",
    "Submit the ScriptRunConfig to an Azure ML experiment to kick off the execution."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(src)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor the Run <a class=\"anchor\" id=\"monitor-run\"></a>\n",
    "As the Run is executed, it will go through the following stages:\n",
    "1. Preparing: A docker image is created based on the specifications of the Azure ML environment and it will be uploaded to the workspace's Azure Container Registry. This step will only happen once for each Python environment -- the container will then be cached for subsequent runs. Creating and uploading the image takes about **5 minutes**. While the job is preparing, logs are streamed to the run history and can be viewed to monitor the progress of the image creation.\n",
    "\n",
    "2. Scaling: If the compute needs to be scaled up (i.e. the Batch AI cluster requires more nodes to execute the run than currently available), the cluster will attempt to scale up in order to make the required amount of nodes available. Scaling typically takes about **5 minutes**.\n",
    "\n",
    "3. Running: All scripts in the script folder are uploaded to the compute target, data stores are mounted/copied and the `entry_script` is executed. While the job is running, stdout and the `./logs` folder are streamed to the run history and can be viewed to monitor the progress of the run.\n",
    "\n",
    "4. Post-Processing: The `./outputs` folder of the run is copied over to the run history\n",
    "\n",
    "There are multiple ways to check the progress of a running job. We can use a Jupyter notebook widget. \n",
    "\n",
    "**Note: The widget will automatically update ever 10-15 seconds, always showing you the most up-to-date information about the run**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also periodically check the status of the run object, and navigate to Azure portal to monitor the run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Run object <a class=\"anchor\" id=\"run-object\"></a>\n",
    "The Run object provides the interface to the run history -- both to the job and to the control plane (this notebook), and both while the job is running and after it has completed. It provides a number of interesting features for instance:\n",
    "* `run.get_details()`: Provides a rich set of properties of the run\n",
    "* `run.get_metrics()`: Provides a dictionary with all the metrics that were reported for the Run\n",
    "* `run.get_file_names()`: List all the files that were uploaded to the run history for this Run. This will include the `outputs` and `logs` folder, azureml-logs and other logs, as well as files that were explicitly uploaded to the run using `run.upload_file()`\n",
    "\n",
    "Below are some examples -- please run through them and inspect their output. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_details()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_metrics()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_file_names()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot accuracy over epochs\n",
    "Since we can retrieve the metrics from the run, we can easily make plots using `matplotlib` in the notebook. Then we can add the plotted image to the run using `run.log_image()`, so all information about the run is kept together."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.makedirs('./imgs', exist_ok=True)\n",
    "metrics = run.get_metrics()\n",
    "\n",
    "plt.figure(figsize = (13,5))\n",
    "plt.plot(metrics['validation_acc'], 'r-', lw=4, alpha=.6)\n",
    "plt.plot(metrics['training_acc'], 'b--', alpha=0.5)\n",
    "plt.legend(['Full evaluation set', 'Training set mini-batch'])\n",
    "plt.xlabel('epochs', fontsize=14)\n",
    "plt.ylabel('accuracy', fontsize=14)\n",
    "plt.title('Accuracy over Epochs', fontsize=16)\n",
    "run.log_image(name='acc_over_epochs.png', plot=plt)\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the saved model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training script, a TensorFlow `saver` object is used to persist the model in a local folder (local to the compute target). The model was saved to the `./outputs` folder on the disk of the Batch AI cluster node where the job is run. Azure ML automatically uploaded anything written in the `./outputs` folder into run history file store. Subsequently, we can use the `Run` object to download the model files the `saver` object saved. They are under the the `outputs/model` folder in the run history file store, and are downloaded into a local folder named `model`. Note the TensorFlow model consists of four files in binary format and they are not human-readable."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.download_files(prefix='outputs/model', output_directory='./model', append_prefix=False)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on the test set (Optional)\n",
    "Now load the saved TensorFlow graph, and list all operations under the `network` scope. This way we can discover the input tensor `network/X:0` and the output tensor `network/output/MatMul:0`, and use them in the scoring script in the next step.\n",
    "\n",
    "Note: if your local TensorFlow version is different than the version running in the cluster where the model is trained, you might see a \"compiletime version mismatch\" warning. You can ignore it."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    import tensorflow as tf\n",
    "    imported_model = tf.saved_model.load('./model')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pred = imported_model(X_test)\n",
    "    y_hat = np.argmax(pred, axis=1)\n",
    "\n",
    "    # print the first 30 labels and predictions\n",
    "    print('labels:  \\t', y_test[:30])\n",
    "    print('predictions:\\t', y_hat[:30])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    print(\"Accuracy on the test set:\", np.average(y_hat == y_test))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    print(\"Accuracy on the test set:\", np.average(y_hat == y_test))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intelligent hyperparameter tuning\n",
    "We have trained the model with one set of hyperparameters, now let's how we can do hyperparameter tuning by launching multiple runs on the cluster. First let's define the parameter space using random sampling."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import RandomParameterSampling, BanditPolicy, HyperDriveConfig, PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive import choice, loguniform\n",
    "\n",
    "ps = RandomParameterSampling(\n",
    "    {\n",
    "        '--batch-size': choice(32, 64, 128),\n",
    "        '--first-layer-neurons': choice(16, 64, 128, 256, 512),\n",
    "        '--second-layer-neurons': choice(16, 64, 256, 512),\n",
    "        '--learning-rate': loguniform(-6, -1)\n",
    "    }\n",
    ")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a new ScriptRunConfig without the above parameters since they will be passed in later. Note we still need to keep the `data-folder` parameter since that's not a hyperparamter we will sweep."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ['--data-folder', dataset.as_named_input('mnist').as_mount()]\n",
    "\n",
    "src = ScriptRunConfig(source_directory=script_folder,\n",
    "                      script='tf_mnist.py',\n",
    "                      arguments=args,\n",
    "                      compute_target=compute_target,\n",
    "                      environment=tf_env)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define an early termnination policy. The `BanditPolicy` basically states to check the job every 2 iterations. If the primary metric (defined later) falls outside of the top 10% range, Azure ML terminate the job. This saves us from continuing to explore hyperparameters that don't show promise of helping reach our target metric."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to configure a run configuration object, and specify the primary metric `validation_acc` that's recorded in your training runs. If you go back to visit the training script, you will notice that this value is being logged after every epoch (a full batch set). We also want to tell the service that we are looking to maximizing this value. We also set the number of samples to 20, and maximal concurrent job to 4, which is the same as the number of nodes in our computer cluster."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htc = HyperDriveConfig(run_config=src, \n",
    "                       hyperparameter_sampling=ps, \n",
    "                       policy=policy, \n",
    "                       primary_metric_name='validation_acc', \n",
    "                       primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "                       max_total_runs=8,\n",
    "                       max_concurrent_runs=4)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's launch the hyperparameter tuning job."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htr = exp.submit(config=htc)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a run history widget to show the progress. Be patient as this might take a while to complete."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(htr).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htr.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(htr.get_status() == \"Completed\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm start a Hyperparameter Tuning experiment and resuming child runs\n",
    "Often times, finding the best hyperparameter values for your model can be an iterative process, needing multiple tuning runs that learn from previous hyperparameter tuning runs. Reusing knowledge from these previous runs will accelerate the hyperparameter tuning process, thereby reducing the cost of tuning the model and will potentially improve the primary metric of the resulting model. When warm starting a hyperparameter tuning experiment with Bayesian sampling, trials from the previous run will be used as prior knowledge to intelligently pick new samples, so as to improve the primary metric. Additionally, when using Random or Grid sampling, any early termination decisions will leverage metrics from the previous runs to determine poorly performing training runs. \n",
    "\n",
    "Azure Machine Learning allows you to warm start your hyperparameter tuning run by leveraging knowledge from up to 5 previously completed hyperparameter tuning parent runs. \n",
    "\n",
    "Additionally, there might be occasions when individual training runs of a hyperparameter tuning experiment are cancelled due to budget constraints or fail due to other reasons. It is now possible to resume such individual training runs from the last checkpoint (assuming your training script handles checkpoints). Resuming an individual training run will use the same hyperparameter configuration and mount the storage used for that run. The training script should accept the \"--resume-from\" argument, which contains the checkpoint or model files from which to resume the training run. You can also resume individual runs as part of an experiment that spends additional budget on hyperparameter tuning. Any additional budget, after resuming the specified training runs is used for exploring additional configurations.\n",
    "\n",
    "For more information on warm starting and resuming hyperparameter tuning runs, please refer to the [Hyperparameter Tuning for Azure Machine Learning documentation](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters) \n",
    "\n",
    "## Find and register best model <a class=\"anchor\" id=\"register-model\"></a>\n",
    "When all the jobs finish, we can find out the one that has the highest accuracy."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = htr.get_best_run_by_primary_metric()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's list the model files uploaded during the run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_run.get_file_names())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then register the folder (and all files in it) as a model named `tf-dnn-mnist` under the workspace for deployment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_run.register_model(model_name='tf-dnn-mnist', model_path='outputs/model')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model in ACI\n",
    "Now we are ready to deploy the model as a web service running in Azure Container Instance [ACI](https://azure.microsoft.com/en-us/services/container-instances/). Azure Machine Learning accomplishes this by constructing a Docker image with the scoring logic and model baked in.\n",
    "### Create score.py\n",
    "First, we will create a scoring script that will be invoked by the web service call. \n",
    "\n",
    "* Note that the scoring script must have two required functions, `init()` and `run(input_data)`. \n",
    "  * In `init()` function, you typically load the model into a global object. This function is executed only once when the Docker container is started. \n",
    "  * In `run(input_data)` function, the model is used to predict a value based on the input data. The input and output to `run` typically use JSON as serialization and de-serialization format but you are not limited to that."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from azureml.core.model import Model\n",
    "\n",
    "def init():\n",
    "    global tf_model\n",
    "    model_root = os.getenv('AZUREML_MODEL_DIR')\n",
    "    # the name of the folder in which to look for tensorflow model files\n",
    "    tf_model_folder = 'model'\n",
    "    \n",
    "    tf_model = tf.saved_model.load(os.path.join(model_root, tf_model_folder))\n",
    "\n",
    "def run(raw_data):\n",
    "    data = np.array(json.loads(raw_data)['data'], dtype=np.float32)\n",
    "    \n",
    "    # make prediction\n",
    "    out = tf_model(data)\n",
    "    y_hat = np.argmax(out, axis=1)\n",
    "\n",
    "    return y_hat.tolist()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create myenv.yml\n",
    "We also need to create an environment file so that Azure Machine Learning can install the necessary packages in the Docker image which are required by your scoring script. In this case, we need to specify packages `numpy`, `tensorflow`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import CondaDependencies\n",
    "\n",
    "cd = CondaDependencies.create()\n",
    "cd.add_conda_package('numpy')\n",
    "cd.add_pip_package('tensorflow==2.0.0')\n",
    "cd.add_pip_package(\"azureml-defaults\")\n",
    "cd.save_to_file(base_directory='./', conda_file_path='myenv.yml')\n",
    "\n",
    "print(cd.serialize_to_string())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy to ACI\n",
    "We are almost ready to deploy. Create the inference configuration and deployment configuration and deploy to ACI. This cell will run for about 7-8 minutes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "\n",
    "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags={'name':'mnist', 'framework': 'TensorFlow DNN'},\n",
    "                                               description='Tensorflow DNN on MNIST')\n",
    "\n",
    "service = Model.deploy(workspace=ws, \n",
    "                           name='tf-mnist-svc', \n",
    "                           models=[model], \n",
    "                           inference_config=inference_config, \n",
    "                           deployment_config=aciconfig)\n",
    "\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip: If something goes wrong with the deployment, the first thing to look at is the logs from the service by running the following command:**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.get_logs())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the scoring web service endpoint:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.scoring_uri)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the deployed model\n",
    "Let's test the deployed model. Pick 30 random samples from the test set, and send it to the web service hosted in ACI. Note here we are using the `run` API in the SDK to invoke the service. You can also make raw HTTP calls using any HTTP tool such as curl.\n",
    "\n",
    "After the invocation, we print the returned predictions and plot them along with the input images. Use red font color and inversed image (white on black) to highlight the misclassified samples. Note since the model accuracy is pretty high, you might have to run the below cell a few times before you can see a misclassified sample."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# find 30 random samples from test set\n",
    "n = 30\n",
    "sample_indices = np.random.permutation(X_test.shape[0])[0:n]\n",
    "\n",
    "test_samples = json.dumps({\"data\": X_test[sample_indices].tolist()})\n",
    "test_samples = bytes(test_samples, encoding='utf8')\n",
    "\n",
    "# predict using the deployed model\n",
    "result = service.run(input_data=test_samples)\n",
    "\n",
    "# compare actual value vs. the predicted values:\n",
    "i = 0\n",
    "plt.figure(figsize = (20, 1))\n",
    "\n",
    "for s in sample_indices:\n",
    "    plt.subplot(1, n, i + 1)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    \n",
    "    # use different color for misclassified sample\n",
    "    font_color = 'red' if y_test[s] != result[i] else 'black'\n",
    "    clr_map = plt.cm.gray if y_test[s] != result[i] else plt.cm.Greys\n",
    "    \n",
    "    plt.text(x=10, y=-10, s=result[i], fontsize=18, color=font_color)\n",
    "    plt.imshow(X_test[s].reshape(28, 28), cmap=clr_map)\n",
    "    \n",
    "    i = i + 1\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also send raw HTTP request to the service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# send a random row from the test set to score\n",
    "random_index = np.random.randint(0, len(X_test)-1)\n",
    "input_data = \"{\\\"data\\\": [\" + str(list(X_test[random_index])) + \"]}\"\n",
    "\n",
    "headers = {'Content-Type':'application/json'}\n",
    "\n",
    "resp = requests.post(service.scoring_uri, input_data, headers=headers)\n",
    "\n",
    "print(\"POST to url\", service.scoring_uri)\n",
    "#print(\"input data:\", input_data)\n",
    "print(\"label:\", y_test[random_index])\n",
    "print(\"prediction:\", resp.text)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the workspace after the web service was deployed. You should see \n",
    "* a registered model named 'model' and with the id 'model:1'\n",
    "* a webservice called 'tf-mnist' with some scoring URL"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ws.models['tf-dnn-mnist']\n",
    "print(\"Model: {}, ID: {}\".format('tf-dnn-mnist', model.id))\n",
    "    \n",
    "webservice = ws.webservices['tf-mnist-svc']\n",
    "print(\"Webservice: {}, scoring URI: {}\".format('tf-mnist-svc', webservice.scoring_uri))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "You can delete the ACI deployment with a simple delete API call."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed TensorFlow with Horovod\n",
    "In this tutorial, you will train a word2vec model in TensorFlow using distributed training via [Horovod](https://github.com/uber/horovod)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* Understand the [architecture and terms](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning (AML)\n",
    "* If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration notebook](../../../../configuration.ipynb) to:\n",
    "    * install the AML SDK\n",
    "    * create a workspace and its configuration file (`config.json`)\n",
    "* Review the [tutorial](../train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb) on single-node TensorFlow training using the SDK"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "Opt-in diagnostics for better experience, quality, and security of future releases."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Diagnostics"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.telemetry import set_diagnostics_collection\n",
    "\n",
    "set_diagnostics_collection(send_diagnostics=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Attach existing AmlCompute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training your model. In this tutorial, you create `AmlCompute` as your training compute resource.\n",
    "\n",
    "**Creation of AmlCompute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace this code will skip the creation process.\n",
    "\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"gpu-cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', \n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code creates a GPU cluster. If you instead want to create a CPU cluster, provide a different VM size to the `vm_size` parameter, such as `STANDARD_D2_V2`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dataset for Files\n",
    "A Dataset can reference single or multiple files in your datastores or public urls. The files can be of any format. FileDataset provides you with the ability to download or mount the files to your compute. By creating a dataset, you create a reference to the data source location. The data remains in its existing location, so no extra storage cost is incurred. [Learn More](https://aka.ms/azureml/howto/createdatasets)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "web_paths = ['https://azureopendatastorage.blob.core.windows.net/testpublic/text8.zip']\n",
    "dataset = Dataset.File.from_files(path=web_paths)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to register datasets using the register() method to your workspace so that the dataset can be shared with others, reused across various experiments, and referred to by name in your training script."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.register(workspace=ws,\n",
    "                           name='wikipedia-text',\n",
    "                           description='Wikipedia text training and test dataset',\n",
    "                           create_new_version=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the files referenced by the dataset\n",
    "dataset.to_path()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model on the remote compute"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a project directory\n",
    "Create a directory that will contain all the necessary code from your local machine that you will need access to on the remote resource. This includes the training script, and any additional files your training script depends on."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "project_folder = './tf-distr-hvd'\n",
    "os.makedirs(project_folder, exist_ok=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the training script `tf_horovod_word2vec.py` into this project directory."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copy('tf_horovod_word2vec.py', project_folder)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiment\n",
    "Create an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment) to track all the runs in your workspace for this distributed TensorFlow tutorial. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'tf-distr-hvd'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an environment\n",
    "\n",
    "In this tutorial, we will use one of Azure ML's curated TensorFlow environments for training. [Curated environments](https://docs.microsoft.com/azure/machine-learning/how-to-use-environments#use-a-curated-environment) are available in your workspace by default. Specifically, we will use the TensorFlow 1.13 GPU curated environment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "tf_env = Environment.get(ws, name='AzureML-TensorFlow-1.13-GPU')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the training job\n",
    "\n",
    "Create a ScriptRunConfig object to specify the configuration details of your training job, including your training script, environment to use, and the compute target to run on.\n",
    "\n",
    "In order to execute a distributed run using MPI/Horovod, you must create an `MpiConfiguration` object and pass it to the `distributed_job_config` parameter of the ScriptRunConfig constructor. The below code will configure a 2-node distributed job running one process per node. If you would also like to run multiple processes per node (i.e. if your cluster SKU has multiple GPUs), additionally specify the `process_count_per_node` parameter in `MpiConfiguration` (the default is `1`)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.runconfig import MpiConfiguration\n",
    "\n",
    "src = ScriptRunConfig(source_directory=project_folder,\n",
    "                      script='tf_horovod_word2vec.py',\n",
    "                      arguments=['--input_data', dataset.as_mount()],\n",
    "                      compute_target=compute_target,\n",
    "                      environment=tf_env,\n",
    "                      distributed_job_config=MpiConfiguration(node_count=2))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job\n",
    "Run your experiment by submitting your ScriptRunConfig object. Note that this call is asynchronous."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(src)\n",
    "print(run)\n",
    "run.get_details()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor your run\n",
    "You can monitor the progress of the run with a Jupyter widget. Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can block until the script has completed training before running more code."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/tensorflow/distributed-tensorflow-with-horovod/distributed-tensorflow-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/ml-frameworks/fastai/train-with-custom-docker/fastai-with-custom-docker.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/fastai/fastai-with-custom-docker/fastai-with-custom-docker.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/fastai/fastai-with-custom-docker/fastai-with-custom-docker.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model using a custom Docker image"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/fastai/fastai-with-custom-docker/fastai-with-custom-docker.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, learn how to use a custom Docker image when training models with Azure Machine Learning.\n",
    "\n",
    "The example scripts in this article are used to classify pet images by creating a convolutional neural network. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/fastai/fastai-with-custom-docker/fastai-with-custom-docker.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the experiment\n",
    "This section sets up the training experiment by initializing a workspace, creating an experiment, and uploading the training data and training scripts."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/fastai/fastai-with-custom-docker/fastai-with-custom-docker.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a workspace\n",
    "The Azure Machine Learning workspace is the top-level resource for the service. It provides you with a centralized place to work with all the artifacts you create. In the Python SDK, you can access the workspace artifacts by creating a `workspace` object.\n",
    "\n",
    "Create a workspace object from the config.json file."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/fastai/fastai-with-custom-docker/fastai-with-custom-docker.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/fastai/fastai-with-custom-docker/fastai-with-custom-docker.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare scripts\n",
    "Create a directory titled `fastai-example`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/fastai/fastai-with-custom-docker/fastai-with-custom-docker.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('fastai-example', exist_ok=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/fastai/fastai-with-custom-docker/fastai-with-custom-docker.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the cell below to create the training script train.py in the directory."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/fastai/fastai-with-custom-docker/fastai-with-custom-docker.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%writefile fastai-example/train.py\n",
    "\n",
    "from fastai.vision.all import *\n",
    "\n",
    "path = untar_data(URLs.PETS)\n",
    "path.ls()\n",
    "\n",
    "files = get_image_files(path/\"images\")\n",
    "len(files)\n",
    "\n",
    "#(Path('/home/ashwin/.fastai/data/oxford-iiit-pet/images/yorkshire_terrier_102.jpg'),Path('/home/ashwin/.fastai/data/oxford-iiit-pet/images/great_pyrenees_102.jpg'))\n",
    "\n",
    "def label_func(f): return f[0].isupper()\n",
    "\n",
    "#To get our data ready for a model, we need to put it in a DataLoaders object. Here we have a function that labels using the file names, so we will use ImageDataLoaders.from_name_func. There are other factory methods of ImageDataLoaders that could be more suitable for your problem, so make sure to check them all in vision.data.\n",
    "\n",
    "dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(224))\n",
    "\n",
    "#We have passed to this function the directory we're working in, the files we grabbed, our label_func and one last piece as item_tfms: this is a Transform applied on all items of our dataset that will resize each imge to 224 by 224, by using a random crop on the largest dimension to make it a square, then resizing to 224 by 224. If we didn't pass this, we would get an error later as it would be impossible to batch the items together.\n",
    "\n",
    "dls.show_batch()\n",
    "\n",
    "learn = cnn_learner(dls, resnet34, metrics=error_rate)\n",
    "learn.fine_tune(1)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/fastai/fastai-with-custom-docker/fastai-with-custom-docker.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your environment\n",
    "Create an environment object and enable Docker."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/fastai/fastai-with-custom-docker/fastai-with-custom-docker.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "fastai_env = Environment(\"fastai\")\n",
    "fastai_env.docker.enabled = True"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/fastai/fastai-with-custom-docker/fastai-with-custom-docker.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This specified base image supports the fast.ai library which allows for distributed deep learning capabilities. For more information, see the [fast.ai DockerHub](https://hub.docker.com/u/fastdotai). \n",
    "\n",
    "When you are using your custom Docker image, you might already have your Python environment properly set up. In that case, set the `user_managed_dependencies` flag to True in order to leverage your custom image's built-in python environment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/fastai/fastai-with-custom-docker/fastai-with-custom-docker.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai_env.docker.base_image = \"fastdotai/fastai:latest\"\n",
    "fastai_env.python.user_managed_dependencies = True"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/fastai/fastai-with-custom-docker/fastai-with-custom-docker.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use an image from a private container registry that is not in your workspace, you must use `docker.base_image_registry` to specify the address of the repository as well as a username and password."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/fastai/fastai-with-custom-docker/fastai-with-custom-docker.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "fastai_env.docker.base_image_registry.address = \"myregistry.azurecr.io\"\n",
    "fastai_env.docker.base_image_registry.username = \"username\"\n",
    "fastai_env.docker.base_image_registry.password = \"password\"\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/fastai/fastai-with-custom-docker/fastai-with-custom-docker.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to use a custom Dockerfile. Use this approach if you need to install non-Python packages as dependencies and remember to set the base image to None. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/fastai/fastai-with-custom-docker/fastai-with-custom-docker.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify docker steps as a string:\n",
    "```python \n",
    "dockerfile = r\"\"\" \\\n",
    "FROM mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04\n",
    "RUN echo \"Hello from custom container!\" \\\n",
    "\"\"\"\n",
    "```\n",
    "Set base image to None, because the image is defined by dockerfile:\n",
    "```python\n",
    "fastai_env.docker.base_image = None \\\n",
    "fastai_env.docker.base_dockerfile = dockerfile\n",
    "```\n",
    "Alternatively, load the string from a file:\n",
    "```python\n",
    "fastai_env.docker.base_image = None \\\n",
    "fastai_env.docker.base_dockerfile = \"./Dockerfile\"\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/fastai/fastai-with-custom-docker/fastai-with-custom-docker.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or attach existing AmlCompute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training your model. In this tutorial, you create `AmlCompute` as your training compute resource.\n",
    "\n",
    "**Creation of AmlCompute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace this code will skip the creation process.\n",
    "\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/fastai/fastai-with-custom-docker/fastai-with-custom-docker.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"gpu-cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6',\n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current AmlCompute\n",
    "print(compute_target.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/fastai/fastai-with-custom-docker/fastai-with-custom-docker.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a ScriptRunConfig\n",
    "This ScriptRunConfig will configure your job for execution on the desired compute target."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/fastai/fastai-with-custom-docker/fastai-with-custom-docker.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "fastai_config = ScriptRunConfig(source_directory='fastai-example',\n",
    "                                script='train.py',\n",
    "                                compute_target=compute_target,\n",
    "                                environment=fastai_env)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/fastai/fastai-with-custom-docker/fastai-with-custom-docker.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit your run\n",
    "When a training run is submitted using a ScriptRunConfig object, the submit method returns an object of type ScriptRun. The returned ScriptRun object gives you programmatic access to information about the training run. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/fastai/fastai-with-custom-docker/fastai-with-custom-docker.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "run = Experiment(ws,'fastai-custom-image').submit(fastai_config)\n",
    "run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/fastai/fastai-with-custom-docker/fastai-with-custom-docker.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and hyperparameter tune on Iris Dataset with Scikit-learn\n",
    "In this tutorial, we demonstrate how to use the Azure ML Python SDK to train a support vector machine (SVM) on a single-node CPU with Scikit-learn to perform classification on the popular [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris). We will also demonstrate how to perform hyperparameter tuning of the model using Azure ML's HyperDrive service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Go through the [Configuration](../../../../configuration.ipynb) notebook to install the Azure Machine Learning Python SDK and create an Azure ML Workspace"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opt-in diagnostics for better experience, quality, and security of future releases."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.telemetry import set_diagnostics_collection\n",
    "\n",
    "set_diagnostics_collection(send_diagnostics=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AmlCompute"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training your model. In this tutorial, we use Azure ML managed compute ([AmlCompute](https://docs.microsoft.com/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute)) for our remote training compute resource.\n",
    "\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we could not find the cluster with the given name, then we will create a new cluster here. We will create an `AmlCompute` cluster of `STANDARD_D2_V2` CPU VMs. This process is broken down into 3 steps:\n",
    "1. create the configuration (this step is local and only takes a second)\n",
    "2. create the cluster (this step will take about **20 seconds**)\n",
    "3. provision the VMs to bring the cluster to the initial size (of 1 in this case). This step will take about **3-5 minutes** and is providing only sparse output in the process. Please make sure to wait until the call returns before moving to the next cell"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"cpu-cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2', \n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "# can poll for a minimum number of nodes and for a specific timeout. \n",
    "# if no min node count is provided it uses the scale settings for the cluster\n",
    "compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code retrieves a CPU compute target. Scikit-learn does not support GPU computing."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model on the remote compute"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have your data and training script prepared, you are ready to train on your remote compute. You can take advantage of Azure compute to leverage a CPU cluster."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a project directory"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory that will contain all the necessary code from your local machine that you will need access to on the remote resource. This includes the training script and any additional files your training script depends on."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "project_folder = './sklearn-iris'\n",
    "os.makedirs(project_folder, exist_ok=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training script"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will need to create your training script. In this tutorial, the training script is already provided for you at `train_iris`.py. In practice, you should be able to take any custom training script as is and run it with Azure ML without having to modify your code.\n",
    "\n",
    "However, if you would like to use Azure ML's [tracking and metrics](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#metrics) capabilities, you will have to add a small amount of Azure ML code inside your training script.\n",
    "\n",
    "In `train_iris.py`, we will log some metrics to our Azure ML run. To do so, we will access the Azure ML Run object within the script:\n",
    "\n",
    "```python\n",
    "from azureml.core.run import Run\n",
    "run = Run.get_context()\n",
    "```\n",
    "\n",
    "Further within `train_iris.py`, we log the kernel and penalty parameters, and the highest accuracy the model achieves:\n",
    "\n",
    "```python\n",
    "run.log('Kernel type', np.string(args.kernel))\n",
    "run.log('Penalty', np.float(args.penalty))\n",
    "\n",
    "run.log('Accuracy', np.float(accuracy))\n",
    "```\n",
    "\n",
    "These run metrics will become particularly important when we begin hyperparameter tuning our model in the \"Tune model hyperparameters\" section.\n",
    "\n",
    "Once your script is ready, copy the training script `train_iris.py` into your project directory."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copy('train_iris.py', project_folder)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiment"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment) to track all the runs in your workspace for this Scikit-learn tutorial."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'train_iris'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an environment\n",
    "\n",
    "Define a conda environment YAML file with your training script dependencies and create an Azure ML environment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile conda_dependencies.yml\n",
    "\n",
    "dependencies:\n",
    "- python=3.6.2\n",
    "- scikit-learn\n",
    "- pip:\n",
    "  - azureml-defaults"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "sklearn_env = Environment.from_conda_specification(name = 'sklearn-env', file_path = './conda_dependencies.yml')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the training job\n",
    "\n",
    "Create a ScriptRunConfig object to specify the configuration details of your training job, including your training script, environment to use, and the compute target to run on."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "src = ScriptRunConfig(source_directory=project_folder,\n",
    "                      script='train_iris.py',\n",
    "                      arguments=['--kernel', 'linear', '--penalty', 1.0],\n",
    "                      compute_target=compute_target,\n",
    "                      environment=sklearn_env)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your experiment by submitting your ScriptRunConfig object. Note that this call is asynchronous."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(src)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor your run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can monitor the progress of the run with a Jupyter widget. Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune model hyperparameters"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen how to do a simple Scikit-learn training run using the SDK, let's see if we can further improve the accuracy of our model. We can optimize our model's hyperparameters using Azure Machine Learning's hyperparameter tuning capabilities."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a hyperparameter sweep"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will define the hyperparameter space to sweep over. Let's tune the `kernel` and `penalty` parameters. In this example we will use random sampling to try different configuration sets of hyperparameters to maximize our primary metric, `Accuracy`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive.parameter_expressions import choice\n",
    "    \n",
    "\n",
    "param_sampling = RandomParameterSampling( {\n",
    "    \"--kernel\": choice('linear', 'rbf', 'poly', 'sigmoid'),\n",
    "    \"--penalty\": choice(0.5, 1, 1.5)\n",
    "    }\n",
    ")\n",
    "\n",
    "hyperdrive_config = HyperDriveConfig(run_config=src,\n",
    "                                     hyperparameter_sampling=param_sampling, \n",
    "                                     primary_metric_name='Accuracy',\n",
    "                                     primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                                     max_total_runs=12,\n",
    "                                     max_concurrent_runs=4)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lauch the hyperparameter tuning job."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the HyperDrive run\n",
    "hyperdrive_run = experiment.submit(hyperdrive_config)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor HyperDrive runs"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can monitor the progress of the runs with the following Jupyter widget."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(hyperdrive_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(hyperdrive_run.get_status() == \"Completed\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm start a Hyperparameter Tuning experiment and resuming child runs\n",
    "Often times, finding the best hyperparameter values for your model can be an iterative process, needing multiple tuning runs that learn from previous hyperparameter tuning runs. Reusing knowledge from these previous runs will accelerate the hyperparameter tuning process, thereby reducing the cost of tuning the model and will potentially improve the primary metric of the resulting model. When warm starting a hyperparameter tuning experiment with Bayesian sampling, trials from the previous run will be used as prior knowledge to intelligently pick new samples, so as to improve the primary metric. Additionally, when using Random or Grid sampling, any early termination decisions will leverage metrics from the previous runs to determine poorly performing training runs. \n",
    "\n",
    "Azure Machine Learning allows you to warm start your hyperparameter tuning run by leveraging knowledge from up to 5 previously completed hyperparameter tuning parent runs. \n",
    "\n",
    "Additionally, there might be occasions when individual training runs of a hyperparameter tuning experiment are cancelled due to budget constraints or fail due to other reasons. It is now possible to resume such individual training runs from the last checkpoint (assuming your training script handles checkpoints). Resuming an individual training run will use the same hyperparameter configuration and mount the storage used for that run. The training script should accept the \"--resume-from\" argument, which contains the checkpoint or model files from which to resume the training run. You can also resume individual runs as part of an experiment that spends additional budget on hyperparameter tuning. Any additional budget, after resuming the specified training runs is used for exploring additional configurations.\n",
    "\n",
    "For more information on warm starting and resuming hyperparameter tuning runs, please refer to the [Hyperparameter Tuning for Azure Machine Learning documentation](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters) \n",
    "\n",
    "### Find and register best model\n",
    "When all jobs finish, we can find out the one that has the highest accuracy."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "print(best_run.get_details()['runDefinition']['arguments'])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's list the model files uploaded during the run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_run.get_file_names())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then register the folder (and all files in it) as a model named `sklearn-iris` under the workspace for deployment"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_run.register_model(model_name='sklearn-iris', model_path='outputs/model.joblib')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/scikit-learn/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed PyTorch with Horovod\n",
    "In this tutorial, you will train a PyTorch model on the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset using distributed training via [Horovod](https://github.com/uber/horovod) across a GPU cluster."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [Configuration](../../../../configuration.ipynb) notebook to install the Azure Machine Learning Python SDK and create an Azure ML `Workspace`\n",
    "* Review the [tutorial](../train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb) on single-node PyTorch training using Azure Machine Learning"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "Opt-in diagnostics for better experience, quality, and security of future releases."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Diagnostics"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.telemetry import set_diagnostics_collection\n",
    "\n",
    "set_diagnostics_collection(send_diagnostics=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or attach existing AmlCompute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training your model. In this tutorial, we use Azure ML managed compute ([AmlCompute](https://docs.microsoft.com/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute)) for our remote training compute resource. Specifically, the below code creates an `STANDARD_NC6` GPU cluster that autoscales from `0` to `4` nodes.\n",
    "\n",
    "**Creation of AmlCompute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace, this code will skip the creation process.\n",
    "\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"gpu-cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6',\n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current AmlCompute. \n",
    "print(compute_target.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code creates GPU compute. If you instead want to create CPU compute, provide a different VM size to the `vm_size` parameter, such as `STANDARD_D2_V2`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model on the remote compute\n",
    "Now that we have the AmlCompute ready to go, let's run our distributed training job."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a project directory\n",
    "Create a directory that will contain all the necessary code from your local machine that you will need access to on the remote resource. This includes the training script and any additional files your training script depends on."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "project_folder = './pytorch-distr-hvd'\n",
    "os.makedirs(project_folder, exist_ok=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training script\n",
    "Now you will need to create your training script. In this tutorial, the script for distributed training of MNIST is already provided for you at `pytorch_horovod_mnist.py`. In practice, you should be able to take any custom PyTorch training script as is and run it with Azure ML without having to modify your code.\n",
    "\n",
    "However, if you would like to use Azure ML's [metric logging](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#logging) capabilities, you will have to add a small amount of Azure ML logic inside your training script. In this example, at each logging interval, we will log the loss for that minibatch to our Azure ML run.\n",
    "\n",
    "To do so, in `pytorch_horovod_mnist.py`, we will first access the Azure ML `Run` object within the script:\n",
    "```Python\n",
    "from azureml.core.run import Run\n",
    "run = Run.get_context()\n",
    "```\n",
    "Later within the script, we log the loss metric to our run:\n",
    "```Python\n",
    "run.log('loss', loss.item())\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your script is ready, copy the training script `pytorch_horovod_mnist.py` into the project directory."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copy('pytorch_horovod_mnist.py', project_folder)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiment\n",
    "Create an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment) to track all the runs in your workspace for this distributed PyTorch tutorial. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'pytorch-distr-hvd'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an environment\n",
    "\n",
    "In this tutorial, we will use one of Azure ML's curated PyTorch environments for training. [Curated environments](https://docs.microsoft.com/azure/machine-learning/how-to-use-environments#use-a-curated-environment) are available in your workspace by default. Specifically, we will use the PyTorch 1.6 GPU curated environment. The curated environment includes the `torch`, `torchvision` and `horovod` packages required by the training script."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "pytorch_env = Environment.get(ws, name='AzureML-PyTorch-1.6-GPU')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the training job\n",
    "\n",
    "Create a ScriptRunConfig object to specify the configuration details of your training job, including your training script, environment to use, and the compute target to run on.\n",
    "\n",
    "In order to execute a distributed run using MPI/Horovod, you must create an `MpiConfiguration` object and pass it to the `distributed_job_config` parameter of the ScriptRunConfig constructor. The below code will configure a 2-node distributed job running one process per node. If you would also like to run multiple processes per node (i.e. if your cluster SKU has multiple GPUs), additionally specify the `process_count_per_node` parameter in `MpiConfiguration` (the default is `1`)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.runconfig import MpiConfiguration\n",
    "\n",
    "src = ScriptRunConfig(source_directory=project_folder,\n",
    "                      script='pytorch_horovod_mnist.py',\n",
    "                      compute_target=compute_target,\n",
    "                      environment=pytorch_env,\n",
    "                      distributed_job_config=MpiConfiguration(node_count=2))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job\n",
    "Run your experiment by submitting your ScriptRunConfig object. Note that this call is asynchronous."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(src)\n",
    "print(run)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor your run\n",
    "You can monitor the progress of the run with a Jupyter widget. Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes. You can see that the widget automatically plots and visualizes the loss metric that we logged to the Azure ML run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can block until the script has completed training before running more code."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True) # this provides a verbose log"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, hyperparameter tune, and deploy with PyTorch\n",
    "\n",
    "In this tutorial, you will train, hyperparameter tune, and deploy a PyTorch model using the Azure Machine Learning (Azure ML) Python SDK.\n",
    "\n",
    "This tutorial will train an image classification model using transfer learning, based on PyTorch's [Transfer Learning tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html). The model is trained to classify chickens and turkeys by first using a pretrained ResNet18 model that has been trained on the [ImageNet](http://image-net.org/index) dataset."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [Configuration](../../../../configuration.ipynb) notebook to install the Azure Machine Learning Python SDK and create an Azure ML `Workspace`"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "Opt-in diagnostics for better experience, quality, and security of future releases."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Diagnostics"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.telemetry import set_diagnostics_collection\n",
    "\n",
    "set_diagnostics_collection(send_diagnostics=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Attach existing AmlCompute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training your model. In this tutorial, we use Azure ML managed compute ([AmlCompute](https://docs.microsoft.com/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute)) for our remote training compute resource.\n",
    "\n",
    "**Creation of AmlCompute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace, this code will skip the creation process.\n",
    "\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"gpu-cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', \n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code creates a GPU cluster. If you instead want to create a CPU cluster, provide a different VM size to the `vm_size` parameter, such as `STANDARD_D2_V2`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model on the remote compute\n",
    "Now that you have your data and training script prepared, you are ready to train on your remote compute cluster. You can take advantage of Azure compute to leverage GPUs to cut down your training time. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a project directory\n",
    "Create a directory that will contain all the necessary code from your local machine that you will need access to on the remote resource. This includes the training script and any additional files your training script depends on."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "project_folder = './pytorch-birds'\n",
    "os.makedirs(project_folder, exist_ok=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download training data\n",
    "The dataset we will use (located on a public blob [here](https://azureopendatastorage.blob.core.windows.net/testpublic/temp/fowl_data.zip) as a zip file) consists of about 120 training images each for turkeys and chickens, with 100 validation images for each class. The images are a subset of the [Open Images v5 Dataset](https://storage.googleapis.com/openimages/web/index.html). We will download and extract the dataset as part of our training script `pytorch_train.py`"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training script\n",
    "Now you will need to create your training script. In this tutorial, the training script is already provided for you at `pytorch_train.py`. In practice, you should be able to take any custom training script as is and run it with Azure ML without having to modify your code.\n",
    "\n",
    "However, if you would like to use Azure ML's [tracking and metrics](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#metrics) capabilities, you will have to add a small amount of Azure ML code inside your training script. \n",
    "\n",
    "In `pytorch_train.py`, we will log some metrics to our Azure ML run. To do so, we will access the Azure ML `Run` object within the script:\n",
    "```Python\n",
    "from azureml.core.run import Run\n",
    "run = Run.get_context()\n",
    "```\n",
    "Further within `pytorch_train.py`, we log the learning rate and momentum parameters, and the best validation accuracy the model achieves:\n",
    "```Python\n",
    "run.log('lr', np.float(learning_rate))\n",
    "run.log('momentum', np.float(momentum))\n",
    "\n",
    "run.log('best_val_acc', np.float(best_acc))\n",
    "```\n",
    "These run metrics will become particularly important when we begin hyperparameter tuning our model in the \"Tune model hyperparameters\" section."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your script is ready, copy the training script `pytorch_train.py` into your project directory."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copy('pytorch_train.py', project_folder)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiment\n",
    "Create an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment) to track all the runs in your workspace for this transfer learning PyTorch tutorial. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'pytorch-birds'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an environment\n",
    "\n",
    "Define a conda environment YAML file with your training script dependencies and create an Azure ML environment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile conda_dependencies.yml\n",
    "\n",
    "channels:\n",
    "- conda-forge\n",
    "dependencies:\n",
    "- python=3.6.2\n",
    "- pip:\n",
    "  - azureml-defaults\n",
    "  - torch==1.6.0\n",
    "  - torchvision==0.7.0\n",
    "  - future==0.17.1\n",
    "  - pillow"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "pytorch_env = Environment.from_conda_specification(name = 'pytorch-1.6-gpu', file_path = './conda_dependencies.yml')\n",
    "\n",
    "# Specify a GPU base image\n",
    "pytorch_env.docker.enabled = True\n",
    "pytorch_env.docker.base_image = 'mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04'"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the training job\n",
    "\n",
    "Create a ScriptRunConfig object to specify the configuration details of your training job, including your training script, environment to use, and the compute target to run on. The following code will configure a single-node PyTorch job."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "src = ScriptRunConfig(source_directory=project_folder,\n",
    "                      script='pytorch_train.py',\n",
    "                      arguments=['--num_epochs', 30, '--output_dir', './outputs'],\n",
    "                      compute_target=compute_target,\n",
    "                      environment=pytorch_env)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job\n",
    "Run your experiment by submitting your ScriptRunConfig object. Note that this call is asynchronous."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(src)\n",
    "print(run)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get more details of your run\n",
    "print(run.get_details())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor your run\n",
    "You can monitor the progress of the run with a Jupyter widget. Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can block until the script has completed training before running more code."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune model hyperparameters\n",
    "Now that we've seen how to do a simple PyTorch training run using the SDK, let's see if we can further improve the accuracy of our model. We can optimize our model's hyperparameters using Azure Machine Learning's hyperparameter tuning capabilities."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a hyperparameter sweep\n",
    "First, we will define the hyperparameter space to sweep over. Since our training script uses a learning rate schedule to decay the learning rate every several epochs, let's tune the initial learning rate and the momentum parameters. In this example we will use random sampling to try different configuration sets of hyperparameters to maximize our primary metric, the best validation accuracy (`best_val_acc`).\n",
    "\n",
    "Then, we specify the early termination policy to use to early terminate poorly performing runs. Here we use the `BanditPolicy`, which will terminate any run that doesn't fall within the slack factor of our primary evaluation metric. In this tutorial, we will apply this policy every epoch (since we report our `best_val_acc` metric every epoch and `evaluation_interval=1`). Notice we will delay the first policy evaluation until after the first `10` epochs (`delay_evaluation=10`).\n",
    "Refer [here](https://docs.microsoft.com/azure/machine-learning/service/how-to-tune-hyperparameters#specify-an-early-termination-policy) for more information on the BanditPolicy and other policies available."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import RandomParameterSampling, BanditPolicy, HyperDriveConfig, uniform, PrimaryMetricGoal\n",
    "\n",
    "param_sampling = RandomParameterSampling( {\n",
    "        'learning_rate': uniform(0.0005, 0.005),\n",
    "        'momentum': uniform(0.9, 0.99)\n",
    "    }\n",
    ")\n",
    "\n",
    "early_termination_policy = BanditPolicy(slack_factor=0.15, evaluation_interval=1, delay_evaluation=10)\n",
    "\n",
    "hyperdrive_config = HyperDriveConfig(run_config=src,\n",
    "                                     hyperparameter_sampling=param_sampling, \n",
    "                                     policy=early_termination_policy,\n",
    "                                     primary_metric_name='best_val_acc',\n",
    "                                     primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                                     max_total_runs=8,\n",
    "                                     max_concurrent_runs=4)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lauch the hyperparameter tuning job."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the HyperDrive run\n",
    "hyperdrive_run = experiment.submit(hyperdrive_config)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor HyperDrive runs\n",
    "You can monitor the progress of the runs with the following Jupyter widget. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(hyperdrive_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or block until the HyperDrive sweep has completed:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(hyperdrive_run.get_status() == \"Completed\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm start a Hyperparameter Tuning experiment and resuming child runs\n",
    "Often times, finding the best hyperparameter values for your model can be an iterative process, needing multiple tuning runs that learn from previous hyperparameter tuning runs. Reusing knowledge from these previous runs will accelerate the hyperparameter tuning process, thereby reducing the cost of tuning the model and will potentially improve the primary metric of the resulting model. When warm starting a hyperparameter tuning experiment with Bayesian sampling, trials from the previous run will be used as prior knowledge to intelligently pick new samples, so as to improve the primary metric. Additionally, when using Random or Grid sampling, any early termination decisions will leverage metrics from the previous runs to determine poorly performing training runs. \n",
    "\n",
    "Azure Machine Learning allows you to warm start your hyperparameter tuning run by leveraging knowledge from up to 5 previously completed hyperparameter tuning parent runs. \n",
    "\n",
    "Additionally, there might be occasions when individual training runs of a hyperparameter tuning experiment are cancelled due to budget constraints or fail due to other reasons. It is now possible to resume such individual training runs from the last checkpoint (assuming your training script handles checkpoints). Resuming an individual training run will use the same hyperparameter configuration and mount the storage used for that run. The training script should accept the \"--resume-from\" argument, which contains the checkpoint or model files from which to resume the training run. You can also resume individual runs as part of an experiment that spends additional budget on hyperparameter tuning. Any additional budget, after resuming the specified training runs is used for exploring additional configurations.\n",
    "\n",
    "For more information on warm starting and resuming hyperparameter tuning runs, please refer to the [Hyperparameter Tuning for Azure Machine Learning documentation](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters) \n",
    "\n",
    "### Find and register the best model\n",
    "Once all the runs complete, we can find the run that produced the model with the highest accuracy."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "print(best_run)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Run is:\\n  Validation accuracy: {0:.5f} \\n  Learning rate: {1:.5f} \\n  Momentum: {2:.5f}'.format(\n",
    "        best_run_metrics['best_val_acc'][-1],\n",
    "        best_run_metrics['lr'],\n",
    "        best_run_metrics['momentum'])\n",
    "     )"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, register the model from your best-performing run to your workspace. The `model_path` parameter takes in the relative path on the remote VM to the model file in your `outputs` directory. In the next section, we will deploy this registered model as a web service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_run.register_model(model_name = 'pytorch-birds', model_path = 'outputs/model.pt')\n",
    "print(model.name, model.id, model.version, sep = '\\t')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model as web service\n",
    "Once you have your trained model, you can deploy the model on Azure. In this tutorial, we will deploy the model as a web service in [Azure Container Instances](https://docs.microsoft.com/en-us/azure/container-instances/) (ACI). For more information on deploying models using Azure ML, refer [here](https://docs.microsoft.com/azure/machine-learning/service/how-to-deploy-and-where)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create scoring script\n",
    "\n",
    "First, we will create a scoring script that will be invoked by the web service call. Note that the scoring script must have two required functions:\n",
    "* `init()`: In this function, you typically load the model into a `global` object. This function is executed only once when the Docker container is started. \n",
    "* `run(input_data)`: In this function, the model is used to predict a value based on the input data. The input and output typically use JSON as serialization and deserialization format, but you are not limited to that.\n",
    "\n",
    "Refer to the scoring script `pytorch_score.py` for this tutorial. Our web service will use this file to predict whether an image is a chicken or a turkey. When writing your own scoring script, don't forget to test it locally first before you go and deploy the web service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the environment\n",
    "\n",
    "Then, we will need to create an Azure ML environment that specifies all of the scoring script's package dependencies. In this tutorial, we will reuse the same environment, `pytorch_env`, that we created for training."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy to ACI container\n",
    "We are ready to deploy. Create an inference configuration which gives specifies the inferencing environment and scripts. Create a deployment configuration file to specify the number of CPUs and gigabytes of RAM needed for your ACI container. While it depends on your model, the default of `1` core and `1` gigabyte of RAM is usually sufficient for many models. This cell will run for about 7-8 minutes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import Model\n",
    "\n",
    "inference_config = InferenceConfig(entry_script=\"pytorch_score.py\", environment=pytorch_env)\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags={'data': 'birds',  'method':'transfer learning', 'framework':'pytorch'},\n",
    "                                               description='Classify turkey/chickens using transfer learning with PyTorch')\n",
    "\n",
    "service = Model.deploy(workspace=ws, \n",
    "                           name='aci-birds', \n",
    "                           models=[model], \n",
    "                           inference_config=inference_config, \n",
    "                           deployment_config=aciconfig)\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your deployment fails for any reason and you need to redeploy, make sure to delete the service before you do so: `service.delete()`"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip: If something goes wrong with the deployment, the first thing to look at is the logs from the service by running the following command:**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.get_logs()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the web service's HTTP endpoint, which accepts REST client calls. This endpoint can be shared with anyone who wants to test the web service or integrate it into an application."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.scoring_uri)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the web service\n",
    "Finally, let's test our deployed web service. We will send the data as a JSON string to the web service hosted in ACI and use the SDK's `run` API to invoke the service. Here we will take an image from our validation data to predict on."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.imshow(Image.open('test_img.jpg'))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "    \n",
    "def preprocess(image_file):\n",
    "    \"\"\"Preprocess the input image.\"\"\"\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    image = Image.open(image_file)\n",
    "    image = data_transforms(image).float()\n",
    "    image = torch.tensor(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    return image.numpy()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = preprocess('test_img.jpg')\n",
    "result = service.run(input_data=json.dumps({'data': input_data.tolist()}))\n",
    "print(result)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "Once you no longer need the web service, you can delete it with a simple API call."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed PyTorch with DistributedDataParallel\n",
    "In this tutorial, you will train a PyTorch model on the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset using distributed training with PyTorch's `DistributedDataParallel` module across a GPU cluster. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [Configuration](../../../../configuration.ipynb) notebook to install the Azure Machine Learning Python SDK and create an Azure ML `Workspace`"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "Opt-in diagnostics for better experience, quality, and security of future releases."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Diagnostics"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.telemetry import set_diagnostics_collection\n",
    "\n",
    "set_diagnostics_collection(send_diagnostics=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or attach existing AmlCompute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training your model. In this tutorial, we use Azure ML managed compute ([AmlCompute](https://docs.microsoft.com/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute)) for our remote training compute resource. Specifically, the below code creates an `STANDARD_NC6` GPU cluster that autoscales from `0` to `4` nodes.\n",
    "\n",
    "**Creation of AmlCompute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace, this code will skip the creation process.\n",
    "\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"gpu-cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6',\n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current AmlCompute. \n",
    "print(compute_target.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code creates GPU compute. If you instead want to create CPU compute, provide a different VM size to the `vm_size` parameter, such as `STANDARD_D2_V2`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model on the remote compute\n",
    "Now that we have the AmlCompute ready to go, let's run our distributed training job."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a project directory\n",
    "Create a directory that will contain all the necessary code from your local machine that you will need access to on the remote resource. This includes the training script and any additional files your training script depends on."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "project_folder = './pytorch-distr'\n",
    "os.makedirs(project_folder, exist_ok=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training script\n",
    "Now you will need to create your training script. In this tutorial, the script for distributed training of MNIST is already provided for you at `pytorch_mnist.py`. In practice, you should be able to take any custom PyTorch training script as is and run it with Azure ML without having to modify your code.\n",
    "\n",
    "However, if you would like to use Azure ML's [metric logging](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#logging) capabilities, you will have to add a small amount of Azure ML logic inside your training script. In this example, at each logging interval, we will log the loss for that minibatch to our Azure ML run.\n",
    "\n",
    "To do so, in `pytorch_mnist.py`, we will first access the Azure ML `Run` object within the script:\n",
    "```Python\n",
    "from azureml.core.run import Run\n",
    "run = Run.get_context()\n",
    "```\n",
    "Later within the script, we log the loss metric to our run:\n",
    "```Python\n",
    "run.log('loss', losses.avg)\n",
    "```"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your script is ready, copy the training script `pytorch_mnist.py` into the project directory."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copy('pytorch_mnist.py', project_folder)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiment\n",
    "Create an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment) to track all the runs in your workspace for this distributed PyTorch tutorial. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'pytorch-distr'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an environment\n",
    "\n",
    "Define a conda environment YAML file with your training script dependencies and create an Azure ML environment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile conda_dependencies.yml\n",
    "\n",
    "channels:\n",
    "- conda-forge\n",
    "dependencies:\n",
    "- python=3.6.2\n",
    "- pip:\n",
    "  - azureml-defaults\n",
    "  - torch==1.6.0\n",
    "  - torchvision==0.7.0\n",
    "  - future==0.17.1"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "pytorch_env = Environment.from_conda_specification(name = 'pytorch-1.6-gpu', file_path = './conda_dependencies.yml')\n",
    "\n",
    "# Specify a GPU base image\n",
    "pytorch_env.docker.enabled = True\n",
    "pytorch_env.docker.base_image = 'mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04'"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the training job: torch.distributed with NCCL backend\n",
    "\n",
    "Create a ScriptRunConfig object to specify the configuration details of your training job, including your training script, environment to use, and the compute target to run on.\n",
    "\n",
    "In order to run a distributed PyTorch job with **torch.distributed** using the NCCL backend, create a `PyTorchConfiguration` and pass it to the `distributed_job_config` parameter of the ScriptRunConfig constructor. Specify `communication_backend='Nccl'` in the PyTorchConfiguration. The below code will configure a 2-node distributed job. The NCCL backend is the recommended backend for PyTorch distributed GPU training.\n",
    "\n",
    "The script arguments refers to the Azure ML-set environment variables `AZ_BATCHAI_PYTORCH_INIT_METHOD` for shared file-system initialization and `AZ_BATCHAI_TASK_INDEX` for the global rank of each worker process."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.runconfig import PyTorchConfiguration\n",
    "\n",
    "args = ['--dist-backend', 'nccl',\n",
    "        '--dist-url', '$AZ_BATCHAI_PYTORCH_INIT_METHOD',\n",
    "        '--rank', '$AZ_BATCHAI_TASK_INDEX',\n",
    "        '--world-size', 2]\n",
    "\n",
    "src = ScriptRunConfig(source_directory=project_folder,\n",
    "                      script='pytorch_mnist.py',\n",
    "                      arguments=args,\n",
    "                      compute_target=compute_target,\n",
    "                      environment=pytorch_env,\n",
    "                      distributed_job_config=PyTorchConfiguration(communication_backend='Nccl', node_count=2))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job\n",
    "Run your experiment by submitting your ScriptRunConfig object. Note that this call is asynchronous."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(src)\n",
    "print(run)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor your run\n",
    "You can monitor the progress of the run with a Jupyter widget. Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes. You can see that the widget automatically plots and visualizes the loss metric that we logged to the Azure ML run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can block until the script has completed training before running more code."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True) # this provides a verbose log"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure training job: torch.distributed with Gloo backend\n",
    "\n",
    "If you would instead like to use the Gloo backend for distributed training, you can do so via the following code. The Gloo backend is recommended for distributed CPU training."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.runconfig import PyTorchConfiguration\n",
    "\n",
    "args = ['--dist-backend', 'gloo',\n",
    "        '--dist-url', '$AZ_BATCHAI_PYTORCH_INIT_METHOD',\n",
    "        '--rank', '$AZ_BATCHAI_TASK_INDEX',\n",
    "        '--world-size', 2]\n",
    "\n",
    "src = ScriptRunConfig(source_directory=project_folder,\n",
    "                      script='pytorch_mnist.py',\n",
    "                      arguments=args,\n",
    "                      compute_target=compute_target,\n",
    "                      environment=pytorch_env,\n",
    "                      distributed_job_config=PyTorchConfiguration(communication_backend='Gloo', node_count=2))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you create the ScriptRunConfig, you can follow the submit steps as shown in the previous steps to submit a PyTorch distributed run using the Gloo backend."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-nccl-gloo/distributed-pytorch-with-nccl-gloo.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bf74d2e9-2708-49b1-934b-e0ede342f475"
    }
   },
   "source": [
    "# Training, hyperparameter tune, and deploy with Keras\n",
    "\n",
    "## Introduction\n",
    "This tutorial shows how to train a simple deep neural network using the MNIST dataset and Keras on Azure Machine Learning. MNIST is a popular dataset consisting of 70,000 grayscale images. Each image is a handwritten digit of `28x28` pixels, representing number from 0 to 9. The goal is to create a multi-class classifier to identify the digit each image represents, and deploy it as a web service in Azure.\n",
    "\n",
    "For more information about the MNIST dataset, please visit [Yan LeCun's website](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "## Prerequisite:\n",
    "* Understand the [architecture and terms](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning\n",
    "* If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration notebook](../../../../configuration.ipynb) to:\n",
    "    * install the AML SDK\n",
    "    * create a workspace and its configuration file (`config.json`)\n",
    "* For local scoring test, you will also need to have `tensorflow` and `keras` installed in the current Jupyter kernel."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started. First let's import some Python libraries."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "c377ea0c-0cd9-4345-9be2-e20fb29c94c3"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "edaa7f2f-2439-4148-b57a-8c794c0945ec"
    }
   },
   "outputs": [],
   "source": [
    "import azureml\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "59f52294-4a25-4c92-bab8-3b07f0f44d15"
    }
   },
   "source": [
    "## Create an Azure ML experiment\n",
    "Let's create an experiment named \"keras-mnist\" and a folder to hold the training scripts. The script runs will be recorded under the experiment in Azure."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "bc70f780-c240-4779-96f3-bc5ef9a37d59"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "script_folder = './keras-mnist'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "\n",
    "exp = Experiment(workspace=ws, name='keras-mnist')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data\n",
    "\n",
    "Before you train a model, you need to understand the data that you are using to train it. In this section you learn how to:\n",
    "\n",
    "* Download the MNIST dataset\n",
    "* Display some sample images\n",
    "\n",
    "### Download the MNIST dataset\n",
    "\n",
    "Download the MNIST dataset and save the files into a `data` directory locally.  Images and labels for both training and testing are downloaded."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "data_folder = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "urllib.request.urlretrieve('https://azureopendatastorage.blob.core.windows.net/mnist/train-images-idx3-ubyte.gz',\n",
    "                           filename=os.path.join(data_folder, 'train-images-idx3-ubyte.gz'))\n",
    "urllib.request.urlretrieve('https://azureopendatastorage.blob.core.windows.net/mnist/train-labels-idx1-ubyte.gz',\n",
    "                           filename=os.path.join(data_folder, 'train-labels-idx1-ubyte.gz'))\n",
    "urllib.request.urlretrieve('https://azureopendatastorage.blob.core.windows.net/mnist/t10k-images-idx3-ubyte.gz',\n",
    "                           filename=os.path.join(data_folder, 't10k-images-idx3-ubyte.gz'))\n",
    "urllib.request.urlretrieve('https://azureopendatastorage.blob.core.windows.net/mnist/t10k-labels-idx1-ubyte.gz',\n",
    "                           filename=os.path.join(data_folder, 't10k-labels-idx1-ubyte.gz'))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display some sample images\n",
    "\n",
    "Load the compressed files into `numpy` arrays. Then use `matplotlib` to plot 30 random images from the dataset with their labels above them. Note this step requires a `load_data` function that's included in an `utils.py` file. This file is included in the sample folder. Please make sure it is placed in the same folder as this notebook. The `load_data` function simply parses the compressed files into numpy arrays."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure utils.py is in the same directory as this code\n",
    "from utils import load_data, one_hot_encode\n",
    "\n",
    "# note we also shrink the intensity values (X) from 0-255 to 0-1. This helps the model converge faster.\n",
    "X_train = load_data(os.path.join(data_folder, 'train-images-idx3-ubyte.gz'), False) / 255.0\n",
    "X_test = load_data(os.path.join(data_folder, 't10k-images-idx3-ubyte.gz'), False) / 255.0\n",
    "y_train = load_data(os.path.join(data_folder, 'train-labels-idx1-ubyte.gz'), True).reshape(-1)\n",
    "y_test = load_data(os.path.join(data_folder, 't10k-labels-idx1-ubyte.gz'), True).reshape(-1)\n",
    "\n",
    "# now let's show some randomly chosen images from the training set.\n",
    "count = 0\n",
    "sample_size = 30\n",
    "plt.figure(figsize = (16, 6))\n",
    "for i in np.random.permutation(X_train.shape[0])[:sample_size]:\n",
    "    count = count + 1\n",
    "    plt.subplot(1, sample_size, count)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    plt.text(x=10, y=-10, s=y_train[i], fontsize=18)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap=plt.cm.Greys)\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have an idea of what these images look like and the expected prediction outcome."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "defe921f-8097-44c3-8336-8af6700804a7"
    }
   },
   "source": [
    "## Create a FileDataset\n",
    "A FileDataset references one or multiple files in your datastores or public urls. The files can be of any format. FileDataset provides you with the ability to download or mount the files to your compute. By creating a dataset, you create a reference to the data source location. If you applied any subsetting transformations to the dataset, they will be stored in the dataset as well. The data remains in its existing location, so no extra storage cost is incurred. [Learn More](https://aka.ms/azureml/howto/createdatasets)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "web_paths = ['https://azureopendatastorage.blob.core.windows.net/mnist/train-images-idx3-ubyte.gz',\n",
    "             'https://azureopendatastorage.blob.core.windows.net/mnist/train-labels-idx1-ubyte.gz',\n",
    "             'https://azureopendatastorage.blob.core.windows.net/mnist/t10k-images-idx3-ubyte.gz',\n",
    "             'https://azureopendatastorage.blob.core.windows.net/mnist/t10k-labels-idx1-ubyte.gz'\n",
    "            ]\n",
    "dataset = Dataset.File.from_files(path = web_paths)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `register()` method to register datasets to your workspace so they can be shared with others, reused across various experiments, and referred to by name in your training script.\n",
    "You can try get the dataset first to see if it's already registered."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_registered = False\n",
    "try:\n",
    "    temp = Dataset.get_by_name(workspace = ws, name = 'mnist-dataset')\n",
    "    dataset_registered = True\n",
    "except:\n",
    "    print(\"The dataset mnist-dataset is not registered in workspace yet.\")\n",
    "\n",
    "if not dataset_registered:\n",
    "    dataset = dataset.register(workspace = ws,\n",
    "                               name = 'mnist-dataset',\n",
    "                               description='training and test dataset',\n",
    "                               create_new_version=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Attach existing AmlCompute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training your model. In this tutorial, you create `AmlCompute` as your training compute resource."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we could not find the cluster with the given name, then we will create a new cluster here. We will create an `AmlCompute` cluster of `STANDARD_NC6` GPU VMs. This process is broken down into 3 steps:\n",
    "1. create the configuration (this step is local and only takes a second)\n",
    "2. create the cluster (this step will take about **20 seconds**)\n",
    "3. provision the VMs to bring the cluster to the initial size (of 1 in this case). This step will take about **3-5 minutes** and is providing only sparse output in the process. Please make sure to wait until the call returns before moving to the next cell"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"gpu-cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', \n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "# can poll for a minimum number of nodes and for a specific timeout. \n",
    "# if no min node count is provided it uses the scale settings for the cluster\n",
    "compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have created the compute target, let's see what the workspace's `compute_targets` property returns. You should now see one entry named \"gpu-cluster\" of type `AmlCompute`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_targets = ws.compute_targets\n",
    "for name, ct in compute_targets.items():\n",
    "    print(name, ct.type, ct.provisioning_state)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the training files into the script folder\n",
    "The Keras training script is already created for you. You can simply copy it into the script folder, together with the utility library used to load compressed data file into numpy array."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# the training logic is in the keras_mnist.py file.\n",
    "shutil.copy('./keras_mnist.py', script_folder)\n",
    "\n",
    "# the utils.py just helps loading data from the downloaded MNIST dataset into numpy arrays.\n",
    "shutil.copy('./utils.py', script_folder)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2039d2d5-aca6-4f25-a12f-df9ae6529cae"
    }
   },
   "source": [
    "## Construct neural network in Keras\n",
    "In the training script `keras_mnist.py`, it creates a very simple DNN (deep neural network), with just 2 hidden layers. The input layer has 28 * 28 = 784 neurons, each representing a pixel in an image. The first hidden layer has 300 neurons, and the second hidden layer has 100 neurons. The output layer has 10 neurons, each representing a targeted label from 0 to 9.\n",
    "\n",
    "![DNN](nn.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure ML concepts  \n",
    "Please note the following three things in the code below:\n",
    "1. The script accepts arguments using the argparse package. In this case there is one argument `--data_folder` which specifies the FileDataset in which the script can find the MNIST data\n",
    "```\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_folder')\n",
    "```\n",
    "2. The script is accessing the Azure ML `Run` object by executing `run = Run.get_context()`. Further down the script is using the `run` to report the loss and accuracy at the end of each epoch via callback.\n",
    "```\n",
    "    run.log('Loss', log['val_loss'])\n",
    "    run.log('Accuracy', log['val_accuracy'])\n",
    "```\n",
    "3. When running the script on Azure ML, you can write files out to a folder `./outputs` that is relative to the root directory. This folder is specially tracked by Azure ML in the sense that any files written to that folder during script execution on the remote target will be picked up by Run History; these files (known as artifacts) will be available as part of the run history record."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will print out the training code for you to inspect."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(script_folder, './keras_mnist.py'), 'r') as f:\n",
    "    print(f.read())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an environment\n",
    "\n",
    "Define a conda environment YAML file with your training script dependencies, which include TensorFlow, Keras and matplotlib, and create an Azure ML environment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile conda_dependencies.yml\n",
    "\n",
    "channels:\n",
    "- conda-forge\n",
    "dependencies:\n",
    "- python=3.6.2\n",
    "- pip:\n",
    "  - h5py<=2.10.0\n",
    "  - azureml-defaults\n",
    "  - tensorflow-gpu==2.0.0\n",
    "  - keras<=2.3.1\n",
    "  - matplotlib"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "keras_env = Environment.from_conda_specification(name = 'keras-2.3.1', file_path = './conda_dependencies.yml')\n",
    "\n",
    "# Specify a GPU base image\n",
    "keras_env.docker.enabled = True\n",
    "keras_env.docker.base_image = 'mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.0-cudnn7-ubuntu18.04'"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the training job\n",
    "\n",
    "Create a ScriptRunConfig object to specify the configuration details of your training job, including your training script, environment to use, and the compute target to run on.\n",
    "\n",
    "Note that we are specifying a DatasetConsumptionConfig for our FileDataset as an argument to the training script. Azure ML will resolve this DatasetConsumptionConfig to the mount-point of the backing datastore, which we access from the training script."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.get_by_name(ws, 'mnist-dataset')\n",
    "\n",
    "# list the files referenced by mnist dataset\n",
    "dataset.to_path()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "args = ['--data-folder', dataset.as_named_input('mnist').as_mount(),\n",
    "        '--batch-size', 50,\n",
    "        '--first-layer-neurons', 300,\n",
    "        '--second-layer-neurons', 100,\n",
    "        '--learning-rate', 0.001]\n",
    "\n",
    "src = ScriptRunConfig(source_directory=script_folder,\n",
    "                      script='keras_mnist.py',\n",
    "                      arguments=args,\n",
    "                      compute_target=compute_target,\n",
    "                      environment=keras_env)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit job to run\n",
    "Submit the ScriptRunConfig to the Azure ML experiment to kick off the execution."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(src)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor the Run\n",
    "As the Run is executed, it will go through the following stages:\n",
    "1. Preparing: A docker image is created matching the Python environment specified by the Azure ML environment, and it will be uploaded to the workspace's Azure Container Registry. This step will only happen once for each Python environment -- the container will then be cached for subsequent runs. Creating and uploading the image takes about **5 minutes**. While the job is preparing, logs are streamed to the run history and can be viewed to monitor the progress of the image creation.\n",
    "\n",
    "2. Scaling: If the compute needs to be scaled up (i.e. the AmlCompute cluster requires more nodes to execute the run than currently available), the cluster will attempt to scale up in order to make the required amount of nodes available. Scaling typically takes about **5 minutes**.\n",
    "\n",
    "3. Running: All scripts in the script folder are uploaded to the compute target, data stores are mounted/copied and the `entry_script` is executed. While the job is running, stdout and the `./logs` folder are streamed to the run history and can be viewed to monitor the progress of the run.\n",
    "\n",
    "4. Post-Processing: The `./outputs` folder of the run is copied over to the run history\n",
    "\n",
    "There are multiple ways to check the progress of a running job. We can use a Jupyter notebook widget. \n",
    "\n",
    "**Note: The widget will automatically update ever 10-15 seconds, always showing you the most up-to-date information about the run**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also periodically check the status of the run object, and navigate to Azure portal to monitor the run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the outputs of the training script, it prints out the Keras version number. Please make a note of it."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Run object\n",
    "The Run object provides the interface to the run history -- both to the job and to the control plane (this notebook), and both while the job is running and after it has completed. It provides a number of interesting features for instance:\n",
    "* `run.get_details()`: Provides a rich set of properties of the run\n",
    "* `run.get_metrics()`: Provides a dictionary with all the metrics that were reported for the Run\n",
    "* `run.get_file_names()`: List all the files that were uploaded to the run history for this Run. This will include the `outputs` and `logs` folder, azureml-logs and other logs, as well as files that were explicitly uploaded to the run using `run.upload_file()`\n",
    "\n",
    "Below are some examples -- please run through them and inspect their output. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_details()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_metrics()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_file_names()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the saved model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training script, the Keras model is saved into two files, `model.json` and `model.h5`, in the `outputs/models` folder on the gpu-cluster AmlCompute node. Azure ML automatically uploaded anything written in the `./outputs` folder into run history file store. Subsequently, we can use the `run` object to download the model files. They are under the the `outputs/model` folder in the run history file store, and are downloaded into a local folder named `model`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model folder in the current directory\n",
    "os.makedirs('./model', exist_ok=True)\n",
    "\n",
    "for f in run.get_file_names():\n",
    "    if f.startswith('outputs/model'):\n",
    "        output_file_path = os.path.join('./model', f.split('/')[-1])\n",
    "        print('Downloading from {} to {} ...'.format(f, output_file_path))\n",
    "        run.download_file(name=f, output_file_path=output_file_path)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on the test set (Optional)\n",
    "Let's check the version of the local Keras. Make sure it matches with the version number printed out in the training script. Otherwise you might not be able to load the model properly."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    import keras\n",
    "    import tensorflow as tf\n",
    "\n",
    "    print(\"Keras version:\", keras.__version__)\n",
    "    print(\"Tensorflow version:\", tf.__version__)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the downloaded model."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    from keras.models import model_from_json\n",
    "\n",
    "    # load json and create model\n",
    "    json_file = open('model/model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"model/model.h5\")\n",
    "    print(\"Model loaded from disk.\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed test dataset to the persisted model to get predictions."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # evaluate loaded model on test data\n",
    "    loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    y_test_ohe = one_hot_encode(y_test, 10)\n",
    "    y_hat = np.argmax(loaded_model.predict(X_test), axis=1)\n",
    "\n",
    "    # print the first 30 labels and predictions\n",
    "    print('labels:  \\t', y_test[:30])\n",
    "    print('predictions:\\t', y_hat[:30])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the overall accuracy by comparing the predicted value against the test set."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    print(\"Accuracy on the test set:\", np.average(y_hat == y_test))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intelligent hyperparameter tuning\n",
    "We have trained the model with one set of hyperparameters, now let's how we can do hyperparameter tuning by launching multiple runs on the cluster. First let's define the parameter space using random sampling."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import RandomParameterSampling, BanditPolicy, HyperDriveConfig, PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive import choice, loguniform\n",
    "\n",
    "ps = RandomParameterSampling(\n",
    "    {\n",
    "        '--batch-size': choice(25, 50, 100),\n",
    "        '--first-layer-neurons': choice(10, 50, 200, 300, 500),\n",
    "        '--second-layer-neurons': choice(10, 50, 200, 500),\n",
    "        '--learning-rate': loguniform(-6, -1)\n",
    "    }\n",
    ")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a new ScriptRunConfig without the above arguments since they will be passed in later by our Hyperdrive configuration. Note we still need to keep the `data-folder` parameter since that's not a hyperparameter we will sweep."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ['--data-folder', dataset.as_named_input('mnist').as_mount()]\n",
    "\n",
    "src = ScriptRunConfig(source_directory=script_folder,\n",
    "                      script='keras_mnist.py',\n",
    "                      arguments=args,\n",
    "                      compute_target=compute_target,\n",
    "                      environment=keras_env)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define an early termnination policy. The `BanditPolicy` basically states to check the job every 2 iterations. If the primary metric (defined later) falls outside of the top 10% range, Azure ML terminate the job. This saves us from continuing to explore hyperparameters that don't show promise of helping reach our target metric."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to configure a run configuration object, and specify the primary metric `Accuracy` that's recorded in your training runs. If you go back to visit the training script, you will notice that this value is being logged after every epoch (a full batch set). We also want to tell the service that we are looking to maximizing this value. We also set the number of samples to 20, and maximal concurrent job to 4, which is the same as the number of nodes in our computer cluster."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_config = HyperDriveConfig(run_config=src,\n",
    "                                     hyperparameter_sampling=ps,\n",
    "                                     policy=policy,\n",
    "                                     primary_metric_name='Accuracy',\n",
    "                                     primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                                     max_total_runs=20,\n",
    "                                     max_concurrent_runs=4)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's launch the hyperparameter tuning job."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run = exp.submit(config=hyperdrive_config)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a run history widget to show the progress. Be patient as this might take a while to complete."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(hyperdrive_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(hyperdrive_run.get_status() == \"Completed\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm start a Hyperparameter Tuning experiment and resuming child runs\n",
    "Often times, finding the best hyperparameter values for your model can be an iterative process, needing multiple tuning runs that learn from previous hyperparameter tuning runs. Reusing knowledge from these previous runs will accelerate the hyperparameter tuning process, thereby reducing the cost of tuning the model and will potentially improve the primary metric of the resulting model. When warm starting a hyperparameter tuning experiment with Bayesian sampling, trials from the previous run will be used as prior knowledge to intelligently pick new samples, so as to improve the primary metric. Additionally, when using Random or Grid sampling, any early termination decisions will leverage metrics from the previous runs to determine poorly performing training runs. \n",
    "\n",
    "Azure Machine Learning allows you to warm start your hyperparameter tuning run by leveraging knowledge from up to 5 previously completed hyperparameter tuning parent runs. \n",
    "\n",
    "Additionally, there might be occasions when individual training runs of a hyperparameter tuning experiment are cancelled due to budget constraints or fail due to other reasons. It is now possible to resume such individual training runs from the last checkpoint (assuming your training script handles checkpoints). Resuming an individual training run will use the same hyperparameter configuration and mount the storage used for that run. The training script should accept the \"--resume-from\" argument, which contains the checkpoint or model files from which to resume the training run. You can also resume individual runs as part of an experiment that spends additional budget on hyperparameter tuning. Any additional budget, after resuming the specified training runs is used for exploring additional configurations.\n",
    "\n",
    "For more information on warm starting and resuming hyperparameter tuning runs, please refer to the [Hyperparameter Tuning for Azure Machine Learning documentation](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters) \n",
    "\n",
    "## Find and register best model\n",
    "When all the jobs finish, we can find out the one that has the highest accuracy."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "print(best_run.get_details()['runDefinition']['arguments'])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's list the model files uploaded during the run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_run.get_file_names())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then register the folder (and all files in it) as a model named `keras-dnn-mnist` under the workspace for deployment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_run.register_model(model_name='keras-mlp-mnist', model_path='outputs/model')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model in ACI\n",
    "Now we are ready to deploy the model as a web service running in Azure Container Instance [ACI](https://azure.microsoft.com/en-us/services/container-instances/). Azure Machine Learning accomplishes this by constructing a Docker image with the scoring logic and model baked in.\n",
    "### Create score.py\n",
    "First, we will create a scoring script that will be invoked by the web service call. \n",
    "\n",
    "* Note that the scoring script must have two required functions, `init()` and `run(input_data)`. \n",
    "  * In `init()` function, you typically load the model into a global object. This function is executed only once when the Docker container is started. \n",
    "  * In `run(input_data)` function, the model is used to predict a value based on the input data. The input and output to `run` typically use JSON as serialization and de-serialization format but you are not limited to that."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from azureml.core.model import Model\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    \n",
    "    model_root = Model.get_model_path('keras-mlp-mnist')\n",
    "    # load json and create model\n",
    "    json_file = open(os.path.join(model_root, 'model.json'), 'r')\n",
    "    model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(model_json)\n",
    "    # load weights into new model\n",
    "    model.load_weights(os.path.join(model_root, \"model.h5\"))   \n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    \n",
    "def run(raw_data):\n",
    "    data = np.array(json.loads(raw_data)['data'])\n",
    "    # make prediction\n",
    "    y_hat = np.argmax(model.predict(data), axis=1)\n",
    "    return y_hat.tolist()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create myenv.yml\n",
    "We also need to create an environment file so that Azure Machine Learning can install the necessary packages in the Docker image which are required by your scoring script. In this case, we need to specify conda packages `tensorflow` and `keras`."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "cd = CondaDependencies.create()\n",
    "cd.add_tensorflow_conda_package()\n",
    "cd.add_conda_package('h5py<=2.10.0')\n",
    "cd.add_conda_package('keras<=2.3.1')\n",
    "cd.add_pip_package(\"azureml-defaults\")\n",
    "cd.save_to_file(base_directory='./', conda_file_path='myenv.yml')\n",
    "\n",
    "print(cd.serialize_to_string())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy to ACI\n",
    "We are almost ready to deploy. Create the inference configuration and deployment configuration and deploy to ACI. This cell will run for about 7-8 minutes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "\n",
    "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1,\n",
    "                                               auth_enabled=True, # this flag generates API keys to secure access\n",
    "                                               memory_gb=1,\n",
    "                                               tags={'name': 'mnist', 'framework': 'Keras'},\n",
    "                                               description='Keras MLP on MNIST')\n",
    "\n",
    "service = Model.deploy(workspace=ws, \n",
    "                           name='keras-mnist-svc', \n",
    "                           models=[model], \n",
    "                           inference_config=inference_config, \n",
    "                           deployment_config=aciconfig)\n",
    "\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip: If something goes wrong with the deployment, the first thing to look at is the logs from the service by running the following command:**"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.get_logs())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the scoring web service endpoint:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.scoring_uri)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the deployed model\n",
    "Let's test the deployed model. Pick 30 random samples from the test set, and send it to the web service hosted in ACI. Note here we are using the `run` API in the SDK to invoke the service. You can also make raw HTTP calls using any HTTP tool such as curl.\n",
    "\n",
    "After the invocation, we print the returned predictions and plot them along with the input images. Use red font color and inversed image (white on black) to highlight the misclassified samples. Note since the model accuracy is pretty high, you might have to run the below cell a few times before you can see a misclassified sample."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# find 30 random samples from test set\n",
    "n = 30\n",
    "sample_indices = np.random.permutation(X_test.shape[0])[0:n]\n",
    "\n",
    "test_samples = json.dumps({\"data\": X_test[sample_indices].tolist()})\n",
    "test_samples = bytes(test_samples, encoding='utf8')\n",
    "\n",
    "# predict using the deployed model\n",
    "result = service.run(input_data=test_samples)\n",
    "\n",
    "# compare actual value vs. the predicted values:\n",
    "i = 0\n",
    "plt.figure(figsize = (20, 1))\n",
    "\n",
    "for s in sample_indices:\n",
    "    plt.subplot(1, n, i + 1)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    \n",
    "    # use different color for misclassified sample\n",
    "    font_color = 'red' if y_test[s] != result[i] else 'black'\n",
    "    clr_map = plt.cm.gray if y_test[s] != result[i] else plt.cm.Greys\n",
    "    \n",
    "    plt.text(x=10, y=-10, s=result[i], fontsize=18, color=font_color)\n",
    "    plt.imshow(X_test[s].reshape(28, 28), cmap=clr_map)\n",
    "    \n",
    "    i = i + 1\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the API keys used for accessing the HTTP endpoint."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the API keys. Two keys were generated.\n",
    "key1, Key2 = service.get_keys()\n",
    "print(key1)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now send construct raw HTTP request and send to the service. Don't forget to add key to the HTTP header."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# send a random row from the test set to score\n",
    "random_index = np.random.randint(0, len(X_test)-1)\n",
    "input_data = \"{\\\"data\\\": [\" + str(list(X_test[random_index])) + \"]}\"\n",
    "\n",
    "headers = {'Content-Type':'application/json', 'Authorization': 'Bearer ' + key1}\n",
    "\n",
    "resp = requests.post(service.scoring_uri, input_data, headers=headers)\n",
    "\n",
    "print(\"POST to url\", service.scoring_uri)\n",
    "#print(\"input data:\", input_data)\n",
    "print(\"label:\", y_test[random_index])\n",
    "print(\"prediction:\", resp.text)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the workspace after the web service was deployed. You should see \n",
    "* a registered model named 'keras-mlp-mnist' and with the id 'model:1'  \n",
    "* a webservice called 'keras-mnist-svc' with some scoring URL"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ws.models['keras-mlp-mnist']\n",
    "print(\"Model: {}, ID: {}\".format('keras-mlp-mnist', model.id))\n",
    "    \n",
    "webservice = ws.webservices['keras-mnist-svc']\n",
    "print(\"Webservice: {}, scoring URI: {}\".format('keras-mnist-svc', webservice.scoring_uri))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "You can delete the ACI deployment with a simple delete API call."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/keras/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use MLflow with Azure Machine Learning to Train and Deploy PyTorch Image Classifier\n",
    "\n",
    "This example shows you how to use MLflow together with Azure Machine Learning services for tracking the metrics and artifacts while training a PyTorch model to classify MNIST digit images and deploy the model  as a web service. You'll learn how to:\n",
    "\n",
    " 1. Set up MLflow tracking URI so as to use Azure ML\n",
    " 2. Create experiment\n",
    " 3. Instrument your model with MLflow tracking\n",
    " 4. Train a PyTorch model locally\n",
    " 5. Train a model on GPU compute on Azure\n",
    " 6. View your experiment within your Azure ML Workspace in Azure Portal\n",
    " 7. Deploy the model as a web service on Azure Container Instance\n",
    " 8. Call the model to make predictions\n",
    " \n",
    "## Pre-requisites\n",
    " \n",
    "If you are using a Notebook VM, you are all set. Otherwise, go through the [Configuration](../../../../configuration.ipnyb) notebook to set up your Azure Machine Learning workspace and ensure other common prerequisites are met.\n",
    "\n",
    "Install PyTorch, this notebook has been tested with torch==1.4\n",
    "\n",
    "Also, install azureml-mlflow package using ```pip install azureml-mlflow```. Note that azureml-mlflow installs mlflow package itself as a dependency if you haven't done so previously.\n",
    "\n",
    "## Set-up\n",
    "\n",
    "Import packages and check versions of Azure ML SDK and MLflow installed on your computer. Then connect to your Workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import mlflow\n",
    "import mlflow.azureml\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n",
    "print(\"MLflow version:\", mlflow.version.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "ws.get_details()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set tracking URI\n",
    "\n",
    "Set the MLflow tracking URI to point to your Azure ML Workspace. The subsequent logging calls from MLflow APIs will go to Azure ML services and will be tracked under your Workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experiment\n",
    "\n",
    "In both MLflow and Azure ML, training runs are grouped into experiments. Let's create one for our experimentation."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"pytorch-with-mlflow\"\n",
    "mlflow.set_experiment(experiment_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model locally while logging metrics and artifacts\n",
    "\n",
    "The ```scripts/train.py``` program contains the code to load the image dataset, train and test the model. Within this program, the train.driver function wraps the end-to-end workflow.\n",
    "\n",
    "Within the driver, the ```mlflow.start_run``` starts MLflow tracking. Then, ```mlflow.log_metric``` functions are used to track the convergence of the neural network training iterations. Finally ```mlflow.pytorch.save_model``` is used to save the trained model in framework-aware manner.\n",
    "\n",
    "Let's add the program to search path, import it as a module and invoke the driver function. Note that the training can take few minutes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_path = os.path.abspath(\"scripts\")\n",
    "sys.path.append(lib_path)\n",
    "\n",
    "import train"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = train.driver()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model on GPU compute on Azure\n",
    "\n",
    "Next, let's run the same script on GPU-enabled compute for faster training. If you've completed the the [Configuration](../../../configuration.ipnyb) notebook, you should have a GPU cluster named \"gpu-cluster\" available in your workspace. Otherwise, follow the instructions in the notebook to create one. For simplicity, this example uses single process on single VM to train the model.\n",
    "\n",
    "Clone an environment object from the PyTorch 1.4 Azure ML curated environment. Azure ML curated environments are pre-configured environments to simplify ML setup, reference [this doc](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-environments#use-a-curated-environment) for more information. To enable MLflow tracking, add ```azureml-mlflow``` as pip package."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "env = Environment.get(workspace=ws, name=\"AzureML-PyTorch-1.4-GPU\").clone(\"mlflow-env\")\n",
    "\n",
    "env.python.conda_dependencies.add_pip_package(\"azureml-mlflow\")\n",
    "env.python.conda_dependencies.add_pip_package(\"Pillow==6.0.0\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a ScriptRunConfig to specify the training configuration: script, compute as well as environment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "src = ScriptRunConfig(source_directory=\"./scripts\", script=\"train.py\")\n",
    "src.run_config.environment = env\n",
    "src.run_config.target = \"gpu-cluster\""
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a reference to the experiment you created previously, but this time, as an Azure Machine Learning experiment object.\n",
    "\n",
    "Then, use the ```Experiment.submit``` method to start the remote training run. Note that the first training run often takes longer as Azure Machine Learning service builds the Docker image for executing the script. Subsequent runs will be faster as the cached image is used."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "exp = Experiment(ws, experiment_name)\n",
    "run = exp.submit(src)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can monitor the run and its metrics on Azure Portal."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, you can wait for run to complete."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model as web service\n",
    "\n",
    "The ```mlflow.azureml.deploy``` function registers the logged PyTorch model and deploys the model in a framework-aware manner. It automatically creates the PyTorch-specific inferencing wrapper code and specifies package dependencies for you. See [this doc](https://mlflow.org/docs/latest/models.html#id34) for more information on deploying models on Azure ML using MLflow.\n",
    "\n",
    "In this example, we deploy the Docker image to Azure Container Instance: a serverless compute capable of running a single container. You can tag and add descriptions to help keep track of your web service. \n",
    "\n",
    "[Other inferencing compute choices](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where) include Azure Kubernetes Service which provides scalable endpoint suitable for production use.\n",
    "\n",
    "Note that the service deployment can take several minutes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "\n",
    "model_path = \"model\"\n",
    "\n",
    "aci_config = AciWebservice.deploy_configuration(cpu_cores=2, \n",
    "                                                memory_gb=5, \n",
    "                                                tags={\"data\": \"MNIST\",  \"method\" : \"pytorch\"}, \n",
    "                                                description=\"Predict using webservice\")\n",
    "\n",
    "webservice, azure_model = mlflow.azureml.deploy(model_uri='runs:/{}/{}'.format(run.id, model_path),\n",
    "                                                      workspace=ws,\n",
    "                                                      deployment_config=aci_config,\n",
    "                                                      service_name=\"pytorch-mnist-1\",\n",
    "                                                      model_name=\"pytorch_mnist\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the deployment has completed you can check the scoring URI of the web service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scoring URI is: {}\".format(webservice.scoring_uri))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of a service creation issue, you can use ```webservice.get_logs()``` to get logs to debug."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions using a web service\n",
    "\n",
    "To make the web service, create a test data set as normalized PyTorch tensors. \n",
    "\n",
    "Then, let's define a utility function that takes a random image and converts it into a format and shape suitable for input to the PyTorch inferencing end-point. The conversion is done by: \n",
    "\n",
    " 1. Select a random (image, label) tuple\n",
    " 2. Take the image and converting the tensor to NumPy array \n",
    " 3. Reshape array into 1 x 1 x N array\n",
    "    * 1 image in batch, 1 color channel, N = 784 pixels for MNIST images\n",
    "    * Note also ```x = x.view(-1, 1, 28, 28)``` in net definition in ```train.py``` program to shape incoming scoring requests.\n",
    " 4. Convert the NumPy array to list to make it into a built-in type.\n",
    " 5. Create a dictionary {\"data\", &lt;list&gt;} that can be converted to JSON string for web service requests."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "test_data = datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "\n",
    "\n",
    "def get_random_image():\n",
    "    image_idx = random.randint(0,len(test_data))\n",
    "    image_as_tensor = test_data[image_idx][0]\n",
    "    return {\"data\": elem for elem in image_as_tensor.numpy().reshape(1,1,-1).tolist()}"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, invoke the web service using a random test image. Convert the dictionary containing the image to JSON string before passing it to web service.\n",
    "\n",
    "The response contains the raw scores for each label, with greater value indicating higher probability. Sort the labels and select the one with greatest score to get the prediction. Let's also plot the image sent to web service for comparison purposes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_image = get_random_image()\n",
    "\n",
    "response = webservice.run(json.dumps(test_image))\n",
    "\n",
    "response = sorted(response[0].items(), key = lambda x: x[1], reverse = True)\n",
    "\n",
    "\n",
    "print(\"Predicted label:\", response[0][0])\n",
    "plt.imshow(np.array(test_image[\"data\"]).reshape(28,28), cmap = \"gray\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also call the web service using a raw POST method against the web service"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(url=webservice.scoring_uri, data=json.dumps(test_image),headers={\"Content-type\": \"application/json\"})\n",
    "print(response.text)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "You can delete the ACI deployment with a delete API call."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webservice.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-pytorch/train-and-deploy-pytorch.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use MLflow with Azure Machine Learning to Train and Deploy Keras Image Classifier\n",
    "\n",
    "This example shows you how to use MLflow together with Azure Machine Learning services for tracking the metrics and artifacts while training a Keras model to classify MNIST digit images and deploy the model as a web service. You'll learn how to:\n",
    "\n",
    " 1. Set up MLflow tracking URI so as to use Azure ML\n",
    " 2. Create experiment\n",
    " 3. Instrument your model with MLflow tracking\n",
    " 4. Train a Keras model locally with MLflow auto logging\n",
    " 5. Train a model on GPU compute on Azure with MLflow auto logging\n",
    " 6. View your experiment within your Azure ML Workspace in Azure Portal\n",
    " 7. Deploy the model as a web service on Azure Container Instance\n",
    " 8. Call the model to make predictions\n",
    " \n",
    "### Pre-requisites\n",
    " \n",
    "If you are using a Notebook VM, you are all set. Otherwise, go through the [Configuration](../../../../configuration.ipnyb) notebook to set up your Azure Machine Learning workspace and ensure other common prerequisites are met.\n",
    "\n",
    "Install TensorFlow and Keras, this notebook has been tested with TensorFlow version 2.1.0 and Keras version 2.3.1.\n",
    "\n",
    "Also, install azureml-mlflow package using ```pip install azureml-mlflow```. Note that azureml-mlflow installs mlflow package itself as a dependency if you haven't done so previously.\n",
    "\n",
    "### Set-up\n",
    "\n",
    "Import packages and check versions of Azure ML SDK and MLflow installed on your computer. Then connect to your Workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import mlflow\n",
    "import mlflow.azureml\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n",
    "print(\"MLflow version:\", mlflow.version.VERSION)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "ws.get_details()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set tracking URI\n",
    "\n",
    "Set the MLflow tracking URI to point to your Azure ML Workspace. The subsequent logging calls from MLflow APIs will go to Azure ML services and will be tracked under your Workspace."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Experiment\n",
    "\n",
    "In both MLflow and Azure ML, training runs are grouped into experiments. Let's create one for our experimentation."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"keras-with-mlflow\"\n",
    "mlflow.set_experiment(experiment_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model locally while logging metrics and artifacts\n",
    "\n",
    "The ```scripts/train.py``` program contains the code to load the image dataset, train and test the model. Within this program, the train.driver function wraps the end-to-end workflow.\n",
    "\n",
    "Within the driver, the ```mlflow.start_run``` starts MLflow tracking. Then, MLflow's automatic logging is used to log metrics, parameters and model for the Keras run.\n",
    "\n",
    "Let's add the program to search path, import it as a module and invoke the driver function. Note that the training can take few minutes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_path = os.path.abspath(\"scripts\")\n",
    "sys.path.append(lib_path)\n",
    "\n",
    "import train"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = train.driver()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model on GPU compute on Azure\n",
    "\n",
    "Next, let's run the same script on GPU-enabled compute for faster training. If you've completed the the [Configuration](../../../configuration.ipnyb) notebook, you should have a GPU cluster named \"gpu-cluster\" available in your workspace. Otherwise, follow the instructions in the notebook to create one. For simplicity, this example uses single process on single VM to train the model.\n",
    "\n",
    "Clone an environment object from the Tensorflow 2.1 Azure ML curated environment. Azure ML curated environments are pre-configured environments to simplify ML setup, reference [this doc](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-environments#use-a-curated-environment) for more information. To enable MLflow tracking, add ```azureml-mlflow``` as pip package."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "env = Environment.get(workspace=ws, name=\"AzureML-TensorFlow-2.1-GPU\").clone(\"mlflow-env\")\n",
    "\n",
    "env.python.conda_dependencies.add_pip_package(\"azureml-mlflow\")\n",
    "env.python.conda_dependencies.add_pip_package(\"keras==2.3.1\")\n",
    "env.python.conda_dependencies.add_pip_package(\"numpy\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a ScriptRunConfig to specify the training configuration: script, compute as well as environment."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "src = ScriptRunConfig(source_directory=\"./scripts\", script=\"train.py\")\n",
    "src.run_config.environment = env\n",
    "src.run_config.target = \"gpu-cluster\""
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a reference to the experiment you created previously, but this time, as an Azure Machine Learning experiment object.\n",
    "\n",
    "Then, use the ```Experiment.submit``` method to start the remote training run. Note that the first training run often takes longer as Azure Machine Learning service builds the Docker image for executing the script. Subsequent runs will be faster as the cached image is used."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "exp = Experiment(ws, experiment_name)\n",
    "run = exp.submit(src)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can monitor the run and its metrics on Azure Portal."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, you can wait for run to complete."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model as web service\n",
    "\n",
    "The ```mlflow.azureml.deploy``` function registers the logged Keras+Tensorflow model and deploys the model in a framework-aware manner. It automatically creates the Tensorflow-specific inferencing wrapper code and specifies package dependencies for you. See [this doc](https://mlflow.org/docs/latest/models.html#id34) for more information on deploying models on Azure ML using MLflow.\n",
    "\n",
    "In this example, we deploy the Docker image to Azure Container Instance: a serverless compute capable of running a single container. You can tag and add descriptions to help keep track of your web service. \n",
    "\n",
    "[Other inferencing compute choices](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where) include Azure Kubernetes Service which provides scalable endpoint suitable for production use.\n",
    "\n",
    "Note that the service deployment can take several minutes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "\n",
    "model_path = \"model\"\n",
    "\n",
    "aci_config = AciWebservice.deploy_configuration(cpu_cores=2, \n",
    "                                                memory_gb=5, \n",
    "                                                tags={\"data\": \"MNIST\",  \"method\" : \"keras\"}, \n",
    "                                                description=\"Predict using webservice\")\n",
    "\n",
    "webservice, azure_model = mlflow.azureml.deploy(model_uri='runs:/{}/{}'.format(run.id, model_path),\n",
    "                                                      workspace=ws,\n",
    "                                                      deployment_config=aci_config,\n",
    "                                                      service_name=\"keras-mnist-1\",\n",
    "                                                      model_name=\"keras_mnist\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the deployment has completed you can check the scoring URI of the web service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scoring URI is: {}\".format(webservice.scoring_uri))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of a service creation issue, you can use ```webservice.get_logs()``` to get logs to debug."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions using a web service\n",
    "\n",
    "To make the web service, create a test data set as normalized NumPy array. \n",
    "\n",
    "Then, let's define a utility function that takes a random image and converts it into a format and shape suitable for input to the Keras inferencing end-point. The conversion is done by: \n",
    "\n",
    " 1. Select a random (image, label) tuple\n",
    " 2. Take the image and converting to to NumPy array \n",
    " 3. Reshape array into 1 x 1 x N array\n",
    "    * 1 image in batch, 1 color channel, N = 784 pixels for MNIST images\n",
    "    * Note also ```x = x.view(-1, 1, 28, 28)``` in net definition in ```train.py``` program to shape incoming scoring requests.\n",
    " 4. Convert the NumPy array to list to make it into a built-in type.\n",
    " 5. Create a dictionary {\"data\", &lt;list&gt;} that can be converted to JSON string for web service requests."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "x_test = x_test.reshape(len(x_test), -1)\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# send a random row from the test set to score\n",
    "random_index = np.random.randint(0, len(x_test)-1)\n",
    "input_data = \"{\\\"data\\\": [\" + str(list(x_test[random_index])) + \"]}\"\n",
    "\n",
    "response = webservice.run(input_data)\n",
    "\n",
    "response = sorted(response[0].items(), key = lambda x: x[1], reverse = True)\n",
    "\n",
    "print(\"Predicted label:\", response[0][0])\n",
    "plt.imshow(x_test[random_index].reshape(28,28), cmap = \"gray\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also call the web service using a raw POST method against the web service"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(url=webservice.scoring_uri, data=input_data,headers={\"Content-type\": \"application/json\"})\n",
    "print(response.text)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "You can delete the ACI deployment with a delete API call."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webservice.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/ml-frameworks/using-mlflow/train-and-deploy-keras-auto-logging/train-and-deploy-keras-auto-logging.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning\n",
    "\n",
    "#### Forecasting away from training data\n",
    "\n",
    "\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup](#Setup)\n",
    "3. [Data](#Data)\n",
    "4. [Prepare remote compute and data.](#prepare_remote)\n",
    "4. [Create the configuration and train a forecaster](#train)\n",
    "5. [Forecasting from the trained model](#forecasting)\n",
    "6. [Forecasting away from training data](#forecasting_away)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook demonstrates the full interface of the `forecast()` function. \n",
    "\n",
    "The best known and most frequent usage of `forecast` enables forecasting on test sets that immediately follows training data. \n",
    "\n",
    "However, in many use cases it is necessary to continue using the model for some time before retraining it. This happens especially in **high frequency forecasting** when forecasts need to be made more frequently than the model can be retrained. Examples are in Internet of Things and predictive cloud resource scaling.\n",
    "\n",
    "Here we show how to use the `forecast()` function when a time gap exists between training data and prediction period.\n",
    "\n",
    "Terminology:\n",
    "* forecast origin: the last period when the target value is known\n",
    "* forecast periods(s): the period(s) for which the value of the target is desired.\n",
    "* lookback: how many past periods (before forecast origin) the model function depends on. The larger of number of lags and length of rolling window.\n",
    "* prediction context: `lookback` periods immediately preceding the forecast origin\n",
    "\n",
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/automated-machine-learning/automl-forecasting-function.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure you have followed the `configuration.ipynb` notebook so that your ML workspace information is saved in the config file."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.dataset import Dataset\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Squash warning messages for cleaner output in the notebook\n",
    "warnings.showwarning = lambda *args, **kwargs: None\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True, linewidth=120)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample notebook may use features that are not available in previous versions of the Azure ML SDK."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This notebook was created using version 1.22.0 of the Azure ML SDK\")\n",
    "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# choose a name for the run history container in the workspace\n",
    "experiment_name = 'automl-forecast-function-demo'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "\n",
    "output = {}\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace'] = ws.name\n",
    "output['SKU'] = ws.sku\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Run History Name'] = experiment_name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "outputDf = pd.DataFrame(data = output, index = [''])\n",
    "outputDf.T"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "For the demonstration purposes we will generate the data artificially and use them for the forecasting."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_COLUMN_NAME = 'date'\n",
    "TIME_SERIES_ID_COLUMN_NAME = 'time_series_id'\n",
    "TARGET_COLUMN_NAME = 'y'\n",
    "\n",
    "def get_timeseries(train_len: int,\n",
    "                   test_len: int,\n",
    "                   time_column_name: str,\n",
    "                   target_column_name: str,\n",
    "                   time_series_id_column_name: str,\n",
    "                   time_series_number: int = 1,\n",
    "                   freq: str = 'H'):\n",
    "    \"\"\"\n",
    "    Return the time series of designed length.\n",
    "\n",
    "    :param train_len: The length of training data (one series).\n",
    "    :type train_len: int\n",
    "    :param test_len: The length of testing data (one series).\n",
    "    :type test_len: int\n",
    "    :param time_column_name: The desired name of a time column.\n",
    "    :type time_column_name: str\n",
    "    :param time_series_number: The number of time series in the data set.\n",
    "    :type time_series_number: int\n",
    "    :param freq: The frequency string representing pandas offset.\n",
    "                 see https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n",
    "    :type freq: str\n",
    "    :returns: the tuple of train and test data sets.\n",
    "    :rtype: tuple\n",
    "\n",
    "    \"\"\"\n",
    "    data_train = []  # type: List[pd.DataFrame]\n",
    "    data_test = []  # type: List[pd.DataFrame]\n",
    "    data_length = train_len + test_len\n",
    "    for i in range(time_series_number):\n",
    "        X = pd.DataFrame({\n",
    "            time_column_name: pd.date_range(start='2000-01-01',\n",
    "                                            periods=data_length,\n",
    "                                            freq=freq),\n",
    "            target_column_name: np.arange(data_length).astype(float) + np.random.rand(data_length) + i*5,\n",
    "            'ext_predictor': np.asarray(range(42, 42 + data_length)),\n",
    "            time_series_id_column_name: np.repeat('ts{}'.format(i), data_length)\n",
    "        })\n",
    "        data_train.append(X[:train_len])\n",
    "        data_test.append(X[train_len:])\n",
    "    X_train = pd.concat(data_train)\n",
    "    y_train = X_train.pop(target_column_name).values\n",
    "    X_test = pd.concat(data_test)\n",
    "    y_test = X_test.pop(target_column_name).values\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "n_test_periods = 6\n",
    "n_train_periods = 30\n",
    "X_train, y_train, X_test, y_test = get_timeseries(train_len=n_train_periods,\n",
    "                                                  test_len=n_test_periods,\n",
    "                                                  time_column_name=TIME_COLUMN_NAME,\n",
    "                                                  target_column_name=TARGET_COLUMN_NAME,\n",
    "                                                  time_series_id_column_name=TIME_SERIES_ID_COLUMN_NAME,\n",
    "                                                  time_series_number=2)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the training data looks like."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.tail()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the example time series\n",
    "import matplotlib.pyplot as plt\n",
    "whole_data = X_train.copy()\n",
    "target_label = 'y'\n",
    "whole_data[target_label] = y_train\n",
    "for g in whole_data.groupby('time_series_id'):    \n",
    "    plt.plot(g[1]['date'].values, g[1]['y'].values, label=g[0])\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare remote compute and data. <a id=\"prepare_remote\"></a>\n",
    "The [Machine Learning service workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-workspace), is paired with the storage account, which contains the default data store. We will use it to upload the artificial data and create [tabular dataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabulardataset?view=azure-ml-py) for training. A tabular dataset defines a series of lazily-evaluated, immutable operations to load data from the data source into tabular representation."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to save thw artificial data and then upload them to default workspace datastore.\n",
    "DATA_PATH = \"fc_fn_data\"\n",
    "DATA_PATH_X = \"{}/data_train.csv\".format(DATA_PATH)\n",
    "if not os.path.isdir('data'):\n",
    "    os.mkdir('data')\n",
    "pd.DataFrame(whole_data).to_csv(\"data/data_train.csv\", index=False)\n",
    "# Upload saved data to the default data store.\n",
    "ds = ws.get_default_datastore()\n",
    "ds.upload(src_dir='./data', target_path=DATA_PATH, overwrite=True, show_progress=True)\n",
    "train_data = Dataset.Tabular.from_delimited_files(path=ds.path(DATA_PATH_X))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to create a [compute target](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute) for your AutoML run. In this tutorial, you create AmlCompute as your training compute resource."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "amlcompute_cluster_name = \"fcfn-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=6)\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the configuration and train a forecaster <a id=\"train\"></a>\n",
    "First generate the configuration, in which we:\n",
    "* Set metadata columns: target, time column and time-series id column names.\n",
    "* Validate our data using cross validation with rolling window method.\n",
    "* Set normalized root mean squared error as a metric to select the best model.\n",
    "* Set early termination to True, so the iterations through the models will stop when no improvements in accuracy score will be made.\n",
    "* Set limitations on the length of experiment run to 15 minutes.\n",
    "* Finally, we set the task to be forecasting.\n",
    "* We apply the lag lead operator to the target value i.e. we use the previous values as a predictor for the future ones.\n",
    "* [Optional] Forecast frequency parameter (freq) represents the period with which the forecast is desired, for example, daily, weekly, yearly, etc. Use this parameter for the correction of time series containing irregular data points or for padding of short time series. The frequency needs to be a pandas offset alias. Please refer to [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects) for more information."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.automl.core.forecasting_parameters import ForecastingParameters\n",
    "lags = [1,2,3]\n",
    "forecast_horizon = n_test_periods\n",
    "forecasting_parameters = ForecastingParameters(\n",
    "    time_column_name=TIME_COLUMN_NAME,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    time_series_id_column_names=[ TIME_SERIES_ID_COLUMN_NAME ],\n",
    "    target_lags=lags\n",
    ")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model selection and training process.  Validation errors and current status will be shown when setting `show_output=True` and the execution will be synchronous."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "\n",
    "automl_config = AutoMLConfig(task='forecasting',\n",
    "                             debug_log='automl_forecasting_function.log',\n",
    "                             primary_metric='normalized_root_mean_squared_error',\n",
    "                             experiment_timeout_hours=0.25,\n",
    "                             enable_early_stopping=True,\n",
    "                             training_data=train_data,\n",
    "                             compute_target=compute_target,\n",
    "                             n_cross_validations=3,\n",
    "                             verbosity = logging.INFO,\n",
    "                             max_concurrent_iterations=4,\n",
    "                             max_cores_per_iteration=-1,\n",
    "                             label_column_name=target_label,\n",
    "                             forecasting_parameters=forecasting_parameters)\n",
    "\n",
    "remote_run = experiment.submit(automl_config, show_output=False)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the best model to use it further.\n",
    "_, fitted_model = remote_run.get_output()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting from the trained model <a id=\"forecasting\"></a>"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will review the `forecast` interface for two main scenarios: forecasting right after the training data, and the more complex interface for forecasting when there is a gap (in the time sense) between training and testing data."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X_train is directly followed by the X_test\n",
    "\n",
    "Let's first consider the case when the prediction period immediately follows the training data. This is typical in scenarios where we have the time to retrain the model every time we wish to forecast. Forecasts that are made on daily and slower cadence typically fall into this category. Retraining the model every time benefits the accuracy because the most recent data is often the most informative.\n",
    "\n",
    "![Forecasting after training](forecast_function_at_train.png)\n",
    "\n",
    "We use `X_test` as a **forecast request** to generate the predictions."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Typical path: X_test is known, forecast all upcoming periods"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data set contains hourly data, the training set ends at 01/02/2000 at 05:00\n",
    "\n",
    "# These are predictions we are asking the model to make (does not contain thet target column y),\n",
    "# for 6 periods beginning with 2000-01-02 06:00, which immediately follows the training data\n",
    "X_test"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_no_gap, xy_nogap =  fitted_model.forecast(X_test)\n",
    "\n",
    "# xy_nogap contains the predictions in the _automl_target_col column.\n",
    "# Those same numbers are output in y_pred_no_gap\n",
    "xy_nogap"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confidence intervals"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecasting model may be used for the prediction of forecasting intervals by running ```forecast_quantiles()```. \n",
    "This method accepts the same parameters as forecast()."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles =  fitted_model.forecast_quantiles(X_test)\n",
    "quantiles"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution forecasts\n",
    "\n",
    "Often the figure of interest is not just the point prediction, but the prediction at some quantile of the distribution. \n",
    "This arises when the forecast is used to control some kind of inventory, for example of grocery items or virtual machines for a cloud service. In such case, the control point is usually something like \"we want the item to be in stock and not run out 99% of the time\". This is called a \"service level\". Here is how you get quantile forecasts."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify which quantiles you would like \n",
    "fitted_model.quantiles = [0.01, 0.5, 0.95]\n",
    "# use forecast_quantiles function, not the forecast() one\n",
    "y_pred_quantiles =  fitted_model.forecast_quantiles(X_test)\n",
    "\n",
    "# quantile forecasts returned in a Dataframe along with the time and time series id columns \n",
    "y_pred_quantiles"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Destination-date forecast: \"just do something\"\n",
    "\n",
    "In some scenarios, the X_test is not known. The forecast is likely to be weak, because it is missing contemporaneous predictors, which we will need to impute. If you still wish to predict forward under the assumption that the last known values will be carried forward, you can forecast out to \"destination date\". The destination date still needs to fit within the forecast horizon from training."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will take the destination date as a last date in the test set.\n",
    "dest = max(X_test[TIME_COLUMN_NAME])\n",
    "y_pred_dest, xy_dest = fitted_model.forecast(forecast_destination=dest)\n",
    "\n",
    "# This form also shows how we imputed the predictors which were not given. (Not so well! Use with caution!)\n",
    "xy_dest"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting away from training data <a id=\"forecasting_away\"></a>\n",
    "\n",
    "Suppose we trained a model, some time passed, and now we want to apply the model without re-training. If the model \"looks back\" -- uses previous values of the target -- then we somehow need to provide those values to the model.\n",
    "\n",
    "![Forecasting after training](forecast_function_away_from_train.png)\n",
    "\n",
    "The notion of forecast origin comes into play: the forecast origin is **the last period for which we have seen the target value**. This applies per time-series, so each time-series can have a different forecast origin. \n",
    "\n",
    "The part of data before the forecast origin is the **prediction context**. To provide the context values the model needs when it looks back, we pass definite values in `y_test` (aligned with corresponding times in `X_test`)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the same kind of test data we trained on, \n",
    "# but now make the train set much longer, so that the test set will be in the future\n",
    "X_context, y_context, X_away, y_away = get_timeseries(train_len=42, # train data was 30 steps long\n",
    "                                      test_len=4,\n",
    "                                      time_column_name=TIME_COLUMN_NAME,\n",
    "                                      target_column_name=TARGET_COLUMN_NAME,\n",
    "                                      time_series_id_column_name=TIME_SERIES_ID_COLUMN_NAME,\n",
    "                                      time_series_number=2)\n",
    "\n",
    "# end of the data we trained on\n",
    "print(X_train.groupby(TIME_SERIES_ID_COLUMN_NAME)[TIME_COLUMN_NAME].max())\n",
    "# start of the data we want to predict on\n",
    "print(X_away.groupby(TIME_SERIES_ID_COLUMN_NAME)[TIME_COLUMN_NAME].min())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a gap of 12 hours between end of training and beginning of `X_away`. (It looks like 13 because all timestamps point to the start of the one hour periods.) Using only `X_away` will fail without adding context data for the model to consume."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    y_pred_away, xy_away = fitted_model.forecast(X_away)\n",
    "    xy_away\n",
    "except Exception as e:\n",
    "    print(e)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should we read that eror message? The forecast origin is at the last time the model saw an actual value of `y` (the target). That was at the end of the training data! The model is attempting to forecast from the end of training data. But the requested forecast periods are past the forecast horizon. We need to provide a define `y` value to establish the forecast origin.\n",
    "\n",
    "We will use this helper function to take the required amount of context from the data preceding the testing data. It's definition is intentionally simplified to keep the idea in the clear."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_forecasting_query(fulldata, time_column_name, target_column_name, forecast_origin, horizon, lookback):\n",
    "\n",
    "    \"\"\"\n",
    "    This function will take the full dataset, and create the query\n",
    "    to predict all values of the time series from the `forecast_origin`\n",
    "    forward for the next `horizon` horizons. Context from previous\n",
    "    `lookback` periods will be included.\n",
    "\n",
    "    \n",
    "\n",
    "    fulldata: pandas.DataFrame           a time series dataset. Needs to contain X and y.\n",
    "    time_column_name: string             which column (must be in fulldata) is the time axis\n",
    "    target_column_name: string           which column (must be in fulldata) is to be forecast\n",
    "    forecast_origin: datetime type       the last time we (pretend to) have target values \n",
    "    horizon: timedelta                   how far forward, in time units (not periods)\n",
    "    lookback: timedelta                  how far back does the model look?\n",
    "\n",
    "    Example:\n",
    "\n",
    "\n",
    "    ```\n",
    "\n",
    "    forecast_origin = pd.to_datetime('2012-09-01') + pd.DateOffset(days=5) # forecast 5 days after end of training\n",
    "    print(forecast_origin)\n",
    "\n",
    "    X_query, y_query = make_forecasting_query(data, \n",
    "                       forecast_origin = forecast_origin,\n",
    "                       horizon = pd.DateOffset(days=7), # 7 days into the future\n",
    "                       lookback = pd.DateOffset(days=1), # model has lag 1 period (day)\n",
    "                      )\n",
    "\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    X_past = fulldata[ (fulldata[ time_column_name ] > forecast_origin - lookback) &\n",
    "                       (fulldata[ time_column_name ] <= forecast_origin)\n",
    "                     ]\n",
    "\n",
    "    X_future = fulldata[ (fulldata[ time_column_name ] > forecast_origin) &\n",
    "                         (fulldata[ time_column_name ] <= forecast_origin + horizon)\n",
    "                       ]\n",
    "\n",
    "    y_past = X_past.pop(target_column_name).values.astype(np.float)\n",
    "    y_future = X_future.pop(target_column_name).values.astype(np.float)\n",
    "\n",
    "    # Now take y_future and turn it into question marks\n",
    "    y_query = y_future.copy().astype(np.float)  # because sometimes life hands you an int\n",
    "    y_query.fill(np.NaN)\n",
    "\n",
    "\n",
    "    print(\"X_past is \" + str(X_past.shape) + \" - shaped\")\n",
    "    print(\"X_future is \" + str(X_future.shape) + \" - shaped\")\n",
    "    print(\"y_past is \" + str(y_past.shape) + \" - shaped\")\n",
    "    print(\"y_query is \" + str(y_query.shape) + \" - shaped\")\n",
    "\n",
    "\n",
    "    X_pred = pd.concat([X_past, X_future])\n",
    "    y_pred = np.concatenate([y_past, y_query])\n",
    "    return X_pred, y_pred"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see where the context data ends - it ends, by construction, just before the testing data starts."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_context.groupby(TIME_SERIES_ID_COLUMN_NAME)[TIME_COLUMN_NAME].agg(['min','max','count']))\n",
    "print(X_away.groupby(TIME_SERIES_ID_COLUMN_NAME)[TIME_COLUMN_NAME].agg(['min','max','count']))\n",
    "X_context.tail(5)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the length of the lookback is 3, \n",
    "# we need to add 3 periods from the context to the request\n",
    "# so that the model has the data it needs\n",
    "\n",
    "# Put the X and y back together for a while. \n",
    "# They like each other and it makes them happy.\n",
    "X_context[TARGET_COLUMN_NAME] = y_context\n",
    "X_away[TARGET_COLUMN_NAME] = y_away\n",
    "fulldata = pd.concat([X_context, X_away])\n",
    "\n",
    "# forecast origin is the last point of data, which is one 1-hr period before test\n",
    "forecast_origin = X_away[TIME_COLUMN_NAME].min() - pd.DateOffset(hours=1)\n",
    "# it is indeed the last point of the context\n",
    "assert forecast_origin == X_context[TIME_COLUMN_NAME].max()\n",
    "print(\"Forecast origin: \" + str(forecast_origin))\n",
    "      \n",
    "# the model uses lags and rolling windows to look back in time\n",
    "n_lookback_periods = max(lags)\n",
    "lookback = pd.DateOffset(hours=n_lookback_periods)\n",
    "\n",
    "horizon = pd.DateOffset(hours=forecast_horizon)\n",
    "\n",
    "# now make the forecast query from context (refer to figure)\n",
    "X_pred, y_pred = make_forecasting_query(fulldata, TIME_COLUMN_NAME, TARGET_COLUMN_NAME,\n",
    "                                        forecast_origin, horizon, lookback)\n",
    "\n",
    "# show the forecast request aligned\n",
    "X_show = X_pred.copy()\n",
    "X_show[TARGET_COLUMN_NAME] = y_pred\n",
    "X_show"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the forecast origin is at 17:00 for both time-series, and periods from 18:00 are to be forecast."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now everything works\n",
    "y_pred_away, xy_away = fitted_model.forecast(X_pred, y_pred)\n",
    "\n",
    "# show the forecast aligned\n",
    "X_show = xy_away.reset_index()\n",
    "# without the generated features\n",
    "X_show[['date', 'time_series_id', 'ext_predictor', '_automl_target_col']]\n",
    "# prediction is in _automl_target_col"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting farther than the forecast horizon <a id=\"recursive forecasting\"></a>\n",
    "When the forecast destination, or the latest date in the prediction data frame, is farther into the future than the specified forecast horizon, the `forecast()` function will still make point predictions out to the later date using a recursive operation mode. Internally, the method recursively applies the regular forecaster to generate context so that we can forecast further into the future. \n",
    "\n",
    "To illustrate the use-case and operation of recursive forecasting, we'll consider an example with a single time-series where the forecasting period directly follows the training period and is twice as long as the forecasting horizon given at training time.\n",
    "\n",
    "![Recursive_forecast_overview](recursive_forecast_overview_small.png)\n",
    "\n",
    "Internally, we apply the forecaster in an iterative manner and finish the forecast task in two interations. In the first iteration, we apply the forecaster and get the prediction for the first forecast-horizon periods (y_pred1). In the second iteraction, y_pred1 is used as the context to produce the prediction for the next forecast-horizon periods (y_pred2). The combination of (y_pred1 and y_pred2) gives the results for the total forecast periods. \n",
    "\n",
    "A caveat: forecast accuracy will likely be worse the farther we predict into the future since errors are compounded with recursive application of the forecaster.\n",
    "\n",
    "![Recursive_forecast_iter1](recursive_forecast_iter1.png)\n",
    "![Recursive_forecast_iter2](recursive_forecast_iter2.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the same kind of test data we trained on, but with a single time-series and test period twice as long\n",
    "# as the forecast_horizon.\n",
    "_, _, X_test_long, y_test_long = get_timeseries(train_len=n_train_periods,\n",
    "                                                  test_len=forecast_horizon*2,\n",
    "                                                  time_column_name=TIME_COLUMN_NAME,\n",
    "                                                  target_column_name=TARGET_COLUMN_NAME,\n",
    "                                                  time_series_id_column_name=TIME_SERIES_ID_COLUMN_NAME,\n",
    "                                                  time_series_number=1)\n",
    "\n",
    "print(X_test_long.groupby(TIME_SERIES_ID_COLUMN_NAME)[TIME_COLUMN_NAME].min())\n",
    "print(X_test_long.groupby(TIME_SERIES_ID_COLUMN_NAME)[TIME_COLUMN_NAME].max())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast() function will invoke the recursive forecast method internally.\n",
    "y_pred_long, X_trans_long = fitted_model.forecast(X_test_long)\n",
    "y_pred_long"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What forecast() function does in this case is equivalent to iterating it twice over the test set as the following. \n",
    "y_pred1, _ = fitted_model.forecast(X_test_long[:forecast_horizon])\n",
    "y_pred_all, _ = fitted_model.forecast(X_test_long, np.concatenate((y_pred1, np.full(forecast_horizon, np.nan))))\n",
    "np.array_equal(y_pred_all, y_pred_long)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confidence interval and distributional forecasts\n",
    "AutoML cannot currently estimate forecast errors beyond the forecast horizon set during training, so the `forecast_quantiles()` function will return missing values for quantiles not equal to 0.5 beyond the forecast horizon. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model.forecast_quantiles(X_test_long)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly with the simple senarios illustrated above, forecasting farther than the forecast horizon in other senarios like 'multiple time-series', 'Destination-date forecast', and 'forecast away from the training data' are also automatically handled by the `forecast()` function. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning\n",
    "_**Orange Juice Sales Forecasting**_\n",
    "\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Compute](#Compute)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Predict](#Predict)\n",
    "1. [Operationalize](#Operationalize)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this example, we use AutoML to train, select, and operationalize a time-series forecasting model for multiple time-series.\n",
    "\n",
    "Make sure you have executed the [configuration notebook](../../../configuration.ipynb) before running this notebook.\n",
    "\n",
    "The examples in the follow code samples use the University of Chicago's Dominick's Finer Foods dataset to forecast orange juice sales. Dominick's was a grocery chain in the Chicago metropolitan area."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.automl.core.featurization import FeaturizationConfig"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample notebook may use features that are not available in previous versions of the Azure ML SDK."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This notebook was created using version 1.22.0 of the Azure ML SDK\")\n",
    "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the setup you have already created a <b>Workspace</b>. To run AutoML, you also need to create an <b>Experiment</b>. An Experiment corresponds to a prediction problem you are trying to solve, while a Run corresponds to a specific approach to the problem. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# choose a name for the run history container in the workspace\n",
    "experiment_name = 'automl-ojforecasting'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "\n",
    "output = {}\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace'] = ws.name\n",
    "output['SKU'] = ws.sku\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Run History Name'] = experiment_name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "outputDf = pd.DataFrame(data = output, index = [''])\n",
    "outputDf.T"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute) for your AutoML run. In this tutorial, you create AmlCompute as your training compute resource.\n",
    "#### Creation of AmlCompute takes approximately 5 minutes. \n",
    "If the AmlCompute with that name is already in your workspace this code will skip the creation process.\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "amlcompute_cluster_name = \"oj-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=6)\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "You are now ready to load the historical orange juice sales data. We will load the CSV file into a plain pandas DataFrame; the time column in the CSV is called _WeekStarting_, so it will be specially parsed into the datetime type."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_column_name = 'WeekStarting'\n",
    "data = pd.read_csv(\"dominicks_OJ.csv\", parse_dates=[time_column_name])\n",
    "\n",
    "# Drop the columns 'logQuantity' as it is a leaky feature.\n",
    "data.drop('logQuantity', axis=1, inplace=True)\n",
    "\n",
    "data.head()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the DataFrame holds a quantity of weekly sales for an OJ brand at a single store. The data also includes the sales price, a flag indicating if the OJ brand was advertised in the store that week, and some customer demographic information based on the store location. For historical reasons, the data also include the logarithm of the sales quantity. The Dominick's grocery data is commonly used to illustrate econometric modeling techniques where logarithms of quantities are generally preferred.    \n",
    "\n",
    "The task is now to build a time-series model for the _Quantity_ column. It is important to note that this dataset is comprised of many individual time-series - one for each unique combination of _Store_ and _Brand_. To distinguish the individual time-series, we define the **time_series_id_column_names** - the columns whose values determine the boundaries between time-series: "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_id_column_names = ['Store', 'Brand']\n",
    "nseries = data.groupby(time_series_id_column_names).ngroups\n",
    "print('Data contains {0} individual time-series.'.format(nseries))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, we extract sales time-series for just a few of the stores:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_stores = [2, 5, 8]\n",
    "data_subset = data[data.Store.isin(use_stores)]\n",
    "nseries = data_subset.groupby(time_series_id_column_names).ngroups\n",
    "print('Data subset contains {0} individual time-series.'.format(nseries))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting\n",
    "We now split the data into a training and a testing set for later forecast evaluation. The test set will contain the final 20 weeks of observed sales for each time-series. The splits should be stratified by series, so we use a group-by statement on the time series identifier columns."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_periods = 20\n",
    "\n",
    "def split_last_n_by_series_id(df, n):\n",
    "    \"\"\"Group df by series identifiers and split on last n rows for each group.\"\"\"\n",
    "    df_grouped = (df.sort_values(time_column_name) # Sort by ascending time\n",
    "                  .groupby(time_series_id_column_names, group_keys=False))\n",
    "    df_head = df_grouped.apply(lambda dfg: dfg.iloc[:-n])\n",
    "    df_tail = df_grouped.apply(lambda dfg: dfg.iloc[-n:])\n",
    "    return df_head, df_tail\n",
    "\n",
    "train, test = split_last_n_by_series_id(data_subset, n_test_periods)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to datastore\n",
    "The [Machine Learning service workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-workspace), is paired with the storage account, which contains the default data store. We will use it to upload the train and test data and create [tabular datasets](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabulardataset?view=azure-ml-py) for training and testing. A tabular dataset defines a series of lazily-evaluated, immutable operations to load data from the data source into tabular representation."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv (r'./dominicks_OJ_train.csv', index = None, header=True)\n",
    "test.to_csv (r'./dominicks_OJ_test.csv', index = None, header=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = ws.get_default_datastore()\n",
    "datastore.upload_files(files = ['./dominicks_OJ_train.csv', './dominicks_OJ_test.csv'], target_path = 'dataset/', overwrite = True,show_progress = True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset for training"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "train_dataset = Dataset.Tabular.from_delimited_files(path=datastore.path('dataset/dominicks_OJ_train.csv'))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.to_pandas_dataframe().tail()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "For forecasting tasks, AutoML uses pre-processing and estimation steps that are specific to time-series. AutoML will undertake the following pre-processing steps:\n",
    "* Detect time-series sample frequency (e.g. hourly, daily, weekly) and create new records for absent time points to make the series regular. A regular time series has a well-defined frequency and has a value at every sample point in a contiguous time span \n",
    "* Impute missing values in the target (via forward-fill) and feature columns (using median column values) \n",
    "* Create features based on time series identifiers to enable fixed effects across different series\n",
    "* Create time-based features to assist in learning seasonal patterns\n",
    "* Encode categorical variables to numeric quantities\n",
    "\n",
    "In this notebook, AutoML will train a single, regression-type model across **all** time-series in a given training set. This allows the model to generalize across related series. If you're looking for training multiple models for different time-series, please see the many-models notebook.\n",
    "\n",
    "You are almost ready to start an AutoML training job. First, we need to separate the target column from the rest of the DataFrame: "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column_name = 'Quantity'"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customization\n",
    "\n",
    "The featurization customization in forecasting is an advanced feature in AutoML which allows our customers to change the default forecasting featurization behaviors and column types through `FeaturizationConfig`. The supported scenarios include:\n",
    "\n",
    "1. Column purposes update: Override feature type for the specified column. Currently supports DateTime, Categorical and Numeric. This customization can be used in the scenario that the type of the column cannot correctly reflect its purpose. Some numerical columns, for instance, can be treated as Categorical columns which need to be converted to categorical while some can be treated as epoch timestamp which need to be converted to datetime. To tell our SDK to correctly preprocess these columns, a configuration need to be add with the columns and their desired types.\n",
    "2. Transformer parameters update: Currently supports parameter change for Imputer only. User can customize imputation methods. The supported imputing methods for target column are constant and ffill (forward fill). The supported imputing methods for feature columns are mean, median, most frequent, constant and ffill (forward fill). This customization can be used for the scenario that our customers know which imputation methods fit best to the input data. For instance, some datasets use NaN to represent 0 which the correct behavior should impute all the missing value with 0. To achieve this behavior, these columns need to be configured as constant imputation with `fill_value` 0.\n",
    "3. Drop columns: Columns to drop from being featurized. These usually are the columns which are leaky or the columns contain no useful data."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "sample-featurizationconfig-remarks"
    ]
   },
   "outputs": [],
   "source": [
    "featurization_config = FeaturizationConfig()\n",
    "# Force the CPWVOL5 feature to be numeric type.\n",
    "featurization_config.add_column_purpose('CPWVOL5', 'Numeric')\n",
    "# Fill missing values in the target column, Quantity, with zeros.\n",
    "featurization_config.add_transformer_params('Imputer', ['Quantity'], {\"strategy\": \"constant\", \"fill_value\": 0})\n",
    "# Fill missing values in the INCOME column with median value.\n",
    "featurization_config.add_transformer_params('Imputer', ['INCOME'], {\"strategy\": \"median\"})\n",
    "# Fill missing values in the Price column with forward fill (last value carried forward).\n",
    "featurization_config.add_transformer_params('Imputer', ['Price'], {\"strategy\": \"ffill\"})"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting Parameters\n",
    "To define forecasting parameters for your experiment training, you can leverage the ForecastingParameters class. The table below details the forecasting parameter we will be passing into our experiment.\n",
    "\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**time_column_name**|The name of your time column.|\n",
    "|**forecast_horizon**|The forecast horizon is how many periods forward you would like to forecast. This integer horizon is in units of the timeseries frequency (e.g. daily, weekly).|\n",
    "|**time_series_id_column_names**|The column names used to uniquely identify the time series in data that has multiple rows with the same timestamp. If the time series identifiers are not defined, the data set is assumed to be one time series.|\n",
    "|**freq**|Forecast frequency. This optional parameter represents the period with which the forecast is desired, for example, daily, weekly, yearly, etc. Use this parameter for the correction of time series containing irregular data points or for padding of short time series. The frequency needs to be a pandas offset alias. Please refer to [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects) for more information."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "The [AutoMLConfig](https://docs.microsoft.com/en-us/python/api/azureml-train-automl-client/azureml.train.automl.automlconfig.automlconfig?view=azure-ml-py) object defines the settings and data for an AutoML training job. Here, we set necessary inputs like the task type, the number of AutoML iterations to try, the training data, and cross-validation parameters.\n",
    "\n",
    "For forecasting tasks, there are some additional parameters that can be set in the `ForecastingParameters` class: the name of the column holding the date/time, the timeseries id column names, and the maximum forecast horizon. A time column is required for forecasting, while the time_series_id is optional. If time_series_id columns are not given, AutoML assumes that the whole dataset is a single time-series. We also pass a list of columns to drop prior to modeling. The _logQuantity_ column is completely correlated with the target quantity, so it must be removed to prevent a target leak.\n",
    "\n",
    "The forecast horizon is given in units of the time-series frequency; for instance, the OJ series frequency is weekly, so a horizon of 20 means that a trained model will estimate sales up to 20 weeks beyond the latest date in the training data for each series. In this example, we set the forecast horizon to the number of samples per series in the test set (n_test_periods). Generally, the value of this parameter will be dictated by business needs. For example, a demand planning application that estimates the next month of sales should set the horizon according to suitable planning time-scales. Please see the [energy_demand notebook](https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand) for more discussion of forecast horizon.\n",
    "\n",
    "We note here that AutoML can sweep over two types of time-series models:\n",
    "* Models that are trained for each series such as ARIMA and Facebook's Prophet.\n",
    "* Models trained across multiple time-series using a regression approach.\n",
    "\n",
    "In the first case, AutoML loops over all time-series in your dataset and trains one model (e.g. AutoArima or Prophet, as the case may be) for each series. This can result in long runtimes to train these models if there are a lot of series in the data. One way to mitigate this problem is to fit models for different series in parallel if you have multiple compute cores available. To enable this behavior, set the `max_cores_per_iteration` parameter in your AutoMLConfig as shown in the example in the next cell. \n",
    "\n",
    "\n",
    "Finally, a note about the cross-validation (CV) procedure for time-series data. AutoML uses out-of-sample error estimates to select a best pipeline/model, so it is important that the CV fold splitting is done correctly. Time-series can violate the basic statistical assumptions of the canonical K-Fold CV strategy, so AutoML implements a [rolling origin validation](https://robjhyndman.com/hyndsight/tscv/) procedure to create CV folds for time-series data. To use this procedure, you just need to specify the desired number of CV folds in the AutoMLConfig object. It is also possible to bypass CV and use your own validation set by setting the *validation_data* parameter of AutoMLConfig.\n",
    "\n",
    "Here is a summary of AutoMLConfig parameters used for training the OJ model:\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**task**|forecasting|\n",
    "|**primary_metric**|This is the metric that you want to optimize.<br> Forecasting supports the following primary metrics <br><i>spearman_correlation</i><br><i>normalized_root_mean_squared_error</i><br><i>r2_score</i><br><i>normalized_mean_absolute_error</i>\n",
    "|**experiment_timeout_hours**|Experimentation timeout in hours.|\n",
    "|**enable_early_stopping**|If early stopping is on, training will stop when the primary metric is no longer improving.|\n",
    "|**training_data**|Input dataset, containing both features and label column.|\n",
    "|**label_column_name**|The name of the label column.|\n",
    "|**compute_target**|The remote compute for training.|\n",
    "|**n_cross_validations**|Number of cross-validation folds to use for model/pipeline selection|\n",
    "|**enable_voting_ensemble**|Allow AutoML to create a Voting ensemble of the best performing models|\n",
    "|**enable_stack_ensemble**|Allow AutoML to create a Stack ensemble of the best performing models|\n",
    "|**debug_log**|Log file path for writing debugging information|\n",
    "|**featurization**| 'auto' / 'off' / FeaturizationConfig Indicator for whether featurization step should be done automatically or not, or whether customized featurization should be used. Setting this enables AutoML to perform featurization on the input to handle *missing data*, and to perform some common *feature extraction*.|\n",
    "|**max_cores_per_iteration**|Maximum number of cores to utilize per iteration. A value of -1 indicates all available cores should be used"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.automl.core.forecasting_parameters import ForecastingParameters\n",
    "forecasting_parameters = ForecastingParameters(\n",
    "    time_column_name=time_column_name,\n",
    "    forecast_horizon=n_test_periods,\n",
    "    time_series_id_column_names=time_series_id_column_names\n",
    ")\n",
    "\n",
    "automl_config = AutoMLConfig(task='forecasting',\n",
    "                             debug_log='automl_oj_sales_errors.log',\n",
    "                             primary_metric='normalized_mean_absolute_error',\n",
    "                             experiment_timeout_hours=0.25,\n",
    "                             training_data=train_dataset,\n",
    "                             label_column_name=target_column_name,\n",
    "                             compute_target=compute_target,\n",
    "                             enable_early_stopping=True,\n",
    "                             featurization=featurization_config,\n",
    "                             n_cross_validations=3,\n",
    "                             verbosity=logging.INFO,\n",
    "                             max_cores_per_iteration=-1,\n",
    "                             forecasting_parameters=forecasting_parameters)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now submit a new training run. Depending on the data and number of iterations this operation may take several minutes.\n",
    "Information from each iteration will be printed to the console.  Validation errors and current status will be shown when setting `show_output=True` and the execution will be synchronous."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run = experiment.submit(automl_config, show_output=False)\n",
    "remote_run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Best Model\n",
    "Each run within an Experiment stores serialized (i.e. pickled) pipelines from the AutoML iterations. We can now retrieve the pipeline with the best performance on the validation dataset:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = remote_run.get_output()\n",
    "print(fitted_model.steps)\n",
    "model_name = best_run.properties['model_name']"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transparency\n",
    "\n",
    "View updated featurization summary"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_featurizer = fitted_model.named_steps['timeseriestransformer']"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_featurizer.get_featurization_summary()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting\n",
    "\n",
    "Now that we have retrieved the best pipeline/model, it can be used to make predictions on test data. First, we remove the target values from the test set:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test\n",
    "y_test = X_test.pop(target_column_name).values"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce predictions on the test set, we need to know the feature values at all dates in the test set. This requirement is somewhat reasonable for the OJ sales data since the features mainly consist of price, which is usually set in advance, and customer demographics which are approximately constant for each store over the 20 week forecast horizon in the testing data."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast returns the predictions and the featurized data, aligned to X_test.\n",
    "# This contains the assumptions that were made in the forecast\n",
    "y_predictions, X_trans = fitted_model.forecast(X_test)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are used to scikit pipelines, perhaps you expected `predict(X_test)`. However, forecasting requires a more general interface that also supplies the past target `y` values. Please use `forecast(X,y)` as `predict(X)` is reserved for internal purposes on forecasting models.\n",
    "\n",
    "The [forecast function notebook](../forecasting-forecast-function/auto-ml-forecasting-function.ipynb)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "\n",
    "To evaluate the accuracy of the forecast, we'll compare against the actual sales quantities for some select metrics, included the mean absolute percentage error (MAPE). For more metrics that can be used for evaluation after training, please see [supported metrics](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml#regressionforecasting-metrics), and [how to calculate residuals](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml#residuals).\n",
    "\n",
    "We'll add predictions and actuals into a single dataframe for convenience in calculating the metrics."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_dict = {'predicted': y_predictions, target_column_name: y_test}\n",
    "df_all = X_test.assign(**assign_dict)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.automl.core.shared import constants\n",
    "from azureml.automl.runtime.shared.score import scoring\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# use automl scoring module\n",
    "scores = scoring.score_regression(\n",
    "    y_test=df_all[target_column_name],\n",
    "    y_pred=df_all['predicted'],\n",
    "    metrics=list(constants.Metric.SCALAR_REGRESSION_SET))\n",
    "\n",
    "print(\"[Test data scores]\\n\")\n",
    "for key, value in scores.items():    \n",
    "    print('{}:   {:.3f}'.format(key, value))\n",
    "    \n",
    "# Plot outputs\n",
    "%matplotlib inline\n",
    "test_pred = plt.scatter(df_all[target_column_name], df_all['predicted'], color='b')\n",
    "test_test = plt.scatter(df_all[target_column_name], df_all[target_column_name], color='g')\n",
    "plt.legend((test_pred, test_test), ('prediction', 'truth'), loc='upper left', fontsize=8)\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operationalize"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Operationalization_ means getting the model into the cloud so that other can run it after you close the notebook. We will create a docker running on Azure Container Instances with the model."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = 'AutoML OJ forecaster'\n",
    "tags = None\n",
    "model = remote_run.register_model(model_name = model_name, description = description, tags = tags)\n",
    "\n",
    "print(remote_run.model_id)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop the scoring script\n",
    "\n",
    "For the deployment we need a function which will run the forecast on serialized data. It can be obtained from the best_run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_file_name = 'score_fcast.py'\n",
    "best_run.download_file('outputs/scoring_file_v_1_0_0.py', script_file_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model as a Web Service on Azure Container Instance"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import Model\n",
    "\n",
    "inference_config = InferenceConfig(environment = best_run.get_environment(), \n",
    "                                   entry_script = script_file_name)\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n",
    "                                               memory_gb = 2, \n",
    "                                               tags = {'type': \"automl-forecasting\"},\n",
    "                                               description = \"Automl forecasting sample service\")\n",
    "\n",
    "aci_service_name = 'automl-oj-forecast-01'\n",
    "print(aci_service_name)\n",
    "aci_service = Model.deploy(ws, aci_service_name, [model], inference_config, aciconfig)\n",
    "aci_service.wait_for_deployment(True)\n",
    "print(aci_service.state)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_service.get_logs()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the service"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "X_query = X_test.copy()\n",
    "# We have to convert datetime to string, because Timestamps cannot be serialized to JSON.\n",
    "X_query[time_column_name] = X_query[time_column_name].astype(str)\n",
    "# The Service object accept the complex dictionary, which is internally converted to JSON string.\n",
    "# The section 'data' contains the data frame in the form of dictionary.\n",
    "test_sample = json.dumps({'data': X_query.to_dict(orient='records')})\n",
    "response = aci_service.run(input_data = test_sample)\n",
    "# translate from networkese to datascientese\n",
    "try: \n",
    "    res_dict = json.loads(response)\n",
    "    y_fcst_all = pd.DataFrame(res_dict['index'])\n",
    "    y_fcst_all[time_column_name] = pd.to_datetime(y_fcst_all[time_column_name], unit = 'ms')\n",
    "    y_fcst_all['forecast'] = res_dict['forecast']    \n",
    "except:\n",
    "    print(res_dict)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fcst_all.head()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the web service if desired"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serv = Webservice(ws, 'automl-oj-forecast-01')\n",
    "serv.delete()     # don't do it accidentally"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/automated-machine-learning/regression-car-price-model-explaination-and-featurization/auto-ml-regression.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning\n",
    "_**Regression with Aml Compute**_\n",
    "\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Results](#Results)\n",
    "1. [Test](#Test)\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this example we use the Hardware Performance Dataset to showcase how you can use AutoML for a simple regression problem. The Regression goal is to predict the performance of certain combinations of hardware parts.\n",
    "After training AutoML models for this regression data set, we show how you can compute model explanations on your remote compute using a sample explainer script.\n",
    "\n",
    "If you are using an Azure Machine Learning Compute Instance, you are all set.  Otherwise, go through the [configuration](../../../configuration.ipynb)  notebook first if you haven't already to establish your connection to the AzureML Workspace. \n",
    "\n",
    "In this notebook you will learn how to:\n",
    "1. Create an `Experiment` in an existing `Workspace`.\n",
    "2. Instantiating AutoMLConfig with FeaturizationConfig for customization\n",
    "3. Train the model using remote compute.\n",
    "4. Explore the results and featurization transparency options\n",
    "5. Setup remote compute for computing the model explanations for a given AutoML model.\n",
    "6. Start an AzureML experiment on your remote compute to compute explanations for an AutoML model.\n",
    "7. Download the feature importance for engineered features and visualize the explanations for engineered features on azure portal. \n",
    "8. Download the feature importance for raw features and visualize the explanations for raw features on azure portal. \n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "As part of the setup you have already created an Azure ML `Workspace` object. For Automated ML you will need to create an `Experiment` object, which is a named object in a `Workspace` used to run experiments."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "import azureml.dataprep as dprep\n",
    "from azureml.automl.core.featurization import FeaturizationConfig\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.core.dataset import Dataset"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample notebook may use features that are not available in previous versions of the Azure ML SDK."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This notebook was created using version 1.22.0 of the Azure ML SDK\")\n",
    "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# Choose a name for the experiment.\n",
    "experiment_name = 'automl-regression-hardware-explain'\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "\n",
    "output = {}\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace Name'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Experiment Name'] = experiment.name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "outputDf = pd.DataFrame(data = output, index = [''])\n",
    "outputDf.T"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Attach existing AmlCompute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for your AutoML run. In this tutorial, you create `AmlCompute` as your training compute resource.\n",
    "\n",
    "**Creation of AmlCompute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace this code will skip the creation process.\n",
    "\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your cluster.\n",
    "amlcompute_cluster_name = \"hardware-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=4)\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Training and Test Data for AutoML experiment\n",
    "\n",
    "Load the hardware dataset from a csv file containing both training features and labels. The features are inputs to the model, while the training labels represent the expected output of the model. Next, we'll split the data using random_split and extract the training data for the model.  We also register the datasets in your workspace using a name so that these datasets may be accessed from the remote compute."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/machineData.csv'\n",
    "\n",
    "dataset = Dataset.Tabular.from_delimited_files(data)\n",
    "\n",
    "# Split the dataset into train and test datasets\n",
    "train_data, test_data = dataset.random_split(percentage=0.8, seed=223)\n",
    "\n",
    "\n",
    "# Register the train dataset with your workspace\n",
    "train_data.register(workspace = ws, name = 'machineData_train_dataset',\n",
    "                       description = 'hardware performance training data',\n",
    "                      create_new_version=True)\n",
    "\n",
    "# Register the test dataset with your workspace\n",
    "test_data.register(workspace = ws, name = 'machineData_test_dataset', description = 'hardware performance test data', create_new_version=True)\n",
    "\n",
    "label =\"ERP\"\n",
    "\n",
    "train_data.to_pandas_dataframe().head()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Instantiate an `AutoMLConfig` object to specify the settings and data used to run the experiment.\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**task**|classification, regression or forecasting|\n",
    "|**primary_metric**|This is the metric that you want to optimize. Regression supports the following primary metrics: <br><i>spearman_correlation</i><br><i>normalized_root_mean_squared_error</i><br><i>r2_score</i><br><i>normalized_mean_absolute_error</i>|\n",
    "|**experiment_timeout_hours**| Maximum amount of time in hours that all iterations combined can take before the experiment terminates.|\n",
    "|**enable_early_stopping**| Flag to enble early termination if the score is not improving in the short term.|\n",
    "|**featurization**| 'auto' / 'off' / FeaturizationConfig Indicator for whether featurization step should be done automatically or not, or whether customized featurization should be used. Setting this enables AutoML to perform featurization on the input to handle *missing data*, and to perform some common *feature extraction*. Note: If the input data is sparse, featurization cannot be turned on.|\n",
    "|**n_cross_validations**|Number of cross validation splits.|\n",
    "|**training_data**|(sparse) array-like, shape = [n_samples, n_features]|\n",
    "|**label_column_name**|(sparse) array-like, shape = [n_samples, ], targets values.|"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customization\n",
    "\n",
    "Supported customization includes:\n",
    "\n",
    "1. Column purpose update: Override feature type for the specified column.\n",
    "2. Transformer parameter update: Update parameters for the specified transformer. Currently supports Imputer and HashOneHotEncoder.\n",
    "3. Drop columns: Columns to drop from being featurized.\n",
    "4. Block transformers: Allow/Block transformers to be used on featurization process."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create FeaturizationConfig object using API calls"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "sample-featurizationconfig-remarks2"
    ]
   },
   "outputs": [],
   "source": [
    "featurization_config = FeaturizationConfig()\n",
    "featurization_config.blocked_transformers = ['LabelEncoder']\n",
    "#featurization_config.drop_columns = ['MMIN']\n",
    "featurization_config.add_column_purpose('MYCT', 'Numeric')\n",
    "featurization_config.add_column_purpose('VendorName', 'CategoricalHash')\n",
    "#default strategy mean, add transformer param for for 3 columns\n",
    "featurization_config.add_transformer_params('Imputer', ['CACH'], {\"strategy\": \"median\"})\n",
    "featurization_config.add_transformer_params('Imputer', ['CHMIN'], {\"strategy\": \"median\"})\n",
    "featurization_config.add_transformer_params('Imputer', ['PRP'], {\"strategy\": \"most_frequent\"})\n",
    "#featurization_config.add_transformer_params('HashOneHotEncoder', [], {\"number_of_bits\": 3})"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "sample-featurizationconfig-remarks3"
    ]
   },
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    \"enable_early_stopping\": True, \n",
    "    \"experiment_timeout_hours\" : 0.25,\n",
    "    \"max_concurrent_iterations\": 4,\n",
    "    \"max_cores_per_iteration\": -1,\n",
    "    \"n_cross_validations\": 5,\n",
    "    \"primary_metric\": 'normalized_root_mean_squared_error',\n",
    "    \"verbosity\": logging.INFO\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'regression',\n",
    "                             debug_log = 'automl_errors.log',\n",
    "                             compute_target=compute_target,\n",
    "                             featurization=featurization_config,\n",
    "                             training_data = train_data,\n",
    "                             label_column_name = label,\n",
    "                             **automl_settings\n",
    "                            )"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the `submit` method on the experiment object and pass the run configuration. Execution of local runs is synchronous. Depending on the data and the number of iterations this can run for a while.\n",
    "In this example, we specify `show_output = True` to print currently running iterations to the console."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run = experiment.submit(automl_config, show_output = False)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to access previous runs. Uncomment the cell below and update the run_id."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from azureml.train.automl.run import AutoMLRun\n",
    "#remote_run = AutoMLRun(experiment=experiment, run_id='<run_ID_goes_here')\n",
    "#remote_run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = remote_run.get_output()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run_customized, fitted_model_customized = remote_run.get_output()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transparency\n",
    "\n",
    "View updated featurization summary"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_featurizer = fitted_model_customized.named_steps['datatransformer']"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_featurizer.get_featurization_summary()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is_user_friendly=False allows for more detailed summary for transforms being applied"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_featurizer.get_featurization_summary(is_user_friendly=False)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_featurizer.get_stats_feature_type_summary()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Widget for Monitoring Runs\n",
    "\n",
    "The widget will first report a \"loading\" status while running the first iteration. After completing the first iteration, an auto-updating graph and table will be shown. The widget will refresh once per minute, so you should see the graph update as child runs complete.\n",
    "\n",
    "**Note:** The widget displays a link at the bottom. Use this link to open a web interface to explore the individual run details"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(remote_run).show() "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanations\n",
    "This section will walk you through the workflow to compute model explanations for an AutoML model on your remote compute.\n",
    "\n",
    "### Retrieve any AutoML Model for explanations\n",
    "\n",
    "Below we select the some AutoML pipeline from our iterations. The `get_output` method returns the a AutoML run and the fitted model for the last invocation. Overloads on `get_output` allow you to retrieve the best run and fitted model for *any* logged metric or for a particular *iteration*."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_run, fitted_model = remote_run.get_output(metric='r2_score')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup model explanation run on the remote compute\n",
    "The following section provides details on how to setup an AzureML experiment to run model explanations for an AutoML model on your remote compute."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample script used for computing explanations\n",
    "View the sample script for computing the model explanations for your AutoML model on remote compute."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_explainer.py', 'r') as cefr:\n",
    "    print(cefr.read())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Substitute values in your sample script\n",
    "The following cell shows how you change the values in the sample script so that you can change the sample script according to your experiment and dataset."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# create script folder\n",
    "script_folder = './sample_projects/automl-regression-hardware'\n",
    "if not os.path.exists(script_folder):\n",
    "    os.makedirs(script_folder)\n",
    "\n",
    "# Copy the sample script to script folder.\n",
    "shutil.copy('train_explainer.py', script_folder)\n",
    "\n",
    "# Create the explainer script that will run on the remote compute.\n",
    "script_file_name = script_folder + '/train_explainer.py'\n",
    "\n",
    "# Open the sample script for modification\n",
    "with open(script_file_name, 'r') as cefr:\n",
    "    content = cefr.read()\n",
    "\n",
    "# Replace the values in train_explainer.py file with the appropriate values\n",
    "content = content.replace('<<experiment_name>>', automl_run.experiment.name) # your experiment name.\n",
    "content = content.replace('<<run_id>>', automl_run.id) # Run-id of the AutoML run for which you want to explain the model.\n",
    "content = content.replace('<<target_column_name>>', 'ERP') # Your target column name\n",
    "content = content.replace('<<task>>', 'regression') # Training task type\n",
    "# Name of your training dataset register with your workspace\n",
    "content = content.replace('<<train_dataset_name>>', 'machineData_train_dataset') \n",
    "# Name of your test dataset register with your workspace\n",
    "content = content.replace('<<test_dataset_name>>', 'machineData_test_dataset')\n",
    "\n",
    "# Write sample file into your script folder.\n",
    "with open(script_file_name, 'w') as cefw:\n",
    "    cefw.write(content)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create conda configuration for model explanations experiment from automl_run object"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "import pkg_resources\n",
    "\n",
    "# create a new RunConfig object\n",
    "conda_run_config = RunConfiguration(framework=\"python\")\n",
    "\n",
    "# Set compute target to AmlCompute\n",
    "conda_run_config.target = compute_target\n",
    "conda_run_config.environment.docker.enabled = True\n",
    "\n",
    "# specify CondaDependencies obj\n",
    "conda_run_config.environment.python.conda_dependencies = automl_run.get_environment().python.conda_dependencies"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit the experiment for model explanations\n",
    "Submit the experiment with the above `run_config` and the sample script for computing explanations."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now submit a run on AmlCompute for model explanations\n",
    "from azureml.core.script_run_config import ScriptRunConfig\n",
    "\n",
    "script_run_config = ScriptRunConfig(source_directory=script_folder,\n",
    "                                    script='train_explainer.py',\n",
    "                                    run_config=conda_run_config)\n",
    "\n",
    "run = experiment.submit(script_run_config)\n",
    "\n",
    "# Show run details\n",
    "run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Shows output of the run on stdout.\n",
    "run.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance  and  visualizing explanation dashboard\n",
    "In this section we describe how you can download the explanation results from the explanations experiment and visualize the feature importance for your AutoML model on the azure portal."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download engineered feature importance from artifact store\n",
    "You can use *ExplanationClient* to download the engineered feature explanations from the artifact store of the *automl_run*. You can also use azure portal url to view the dash board visualization of the feature importance values of the engineered features."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.interpret import ExplanationClient\n",
    "client = ExplanationClient.from_run(automl_run)\n",
    "engineered_explanations = client.download_model_explanation(raw=False, comment='engineered explanations')\n",
    "print(engineered_explanations.get_feature_importance_dict())\n",
    "print(\"You can visualize the engineered explanations under the 'Explanations (preview)' tab in the AutoML run at:-\\n\" + automl_run.get_portal_url())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download raw feature importance from artifact store\n",
    "You can use *ExplanationClient* to download the raw feature explanations from the artifact store of the *automl_run*. You can also use azure portal url to view the dash board visualization of the feature importance values of the raw features."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_explanations = client.download_model_explanation(raw=True, comment='raw explanations')\n",
    "print(raw_explanations.get_feature_importance_dict())\n",
    "print(\"You can visualize the raw explanations under the 'Explanations (preview)' tab in the AutoML run at:-\\n\" + automl_run.get_portal_url())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operationalize\n",
    "In this section we will show how you can operationalize an AutoML model and the explainer which was used to compute the explanations in the previous section.\n",
    "\n",
    "### Register the AutoML model and the scoring explainer\n",
    "We use the *TreeScoringExplainer* from *azureml-interpret* package to create the scoring explainer which will be used to compute the raw and engineered feature importances at the inference time. \n",
    "In the cell below, we register the AutoML model and the scoring explainer with the Model Management Service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register trained automl model present in the 'outputs' folder in the artifacts\n",
    "original_model = automl_run.register_model(model_name='automl_model', \n",
    "                                           model_path='outputs/model.pkl')\n",
    "scoring_explainer_model = automl_run.register_model(model_name='scoring_explainer',\n",
    "                                                    model_path='outputs/scoring_explainer.pkl')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the conda dependencies for setting up the service\n",
    "We need to create the conda dependencies comprising of the *azureml* packages using the training environment from the *automl_run*."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda_dep = automl_run.get_environment().python.conda_dependencies\n",
    "\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(conda_dep.serialize_to_string())\n",
    "\n",
    "with open(\"myenv.yml\",\"r\") as f:\n",
    "    print(f.read())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View your scoring file"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"score_explain.py\",\"r\") as f:\n",
    "    print(f.read())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the service\n",
    "In the cell below, we deploy the service using the conda file and the scoring file from the previous steps. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags={\"data\": \"Machine Data\",  \n",
    "                                                     \"method\" : \"local_explanation\"}, \n",
    "                                               description='Get local explanations for Machine test data')\n",
    "\n",
    "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\n",
    "inference_config = InferenceConfig(entry_script=\"score_explain.py\", environment=myenv)\n",
    "\n",
    "# Use configs and models generated above\n",
    "service = Model.deploy(ws, 'model-scoring', [scoring_explainer_model, original_model], inference_config, aciconfig)\n",
    "service.wait_for_deployment(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the service logs"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.get_logs()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference using some test data\n",
    "Inference using some test data to see the predicted value from autml model, view the engineered feature importance for the predicted value and raw feature importance for the predicted value."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if service.state == 'Healthy':\n",
    "    X_test = test_data.drop_columns([label]).to_pandas_dataframe()\n",
    "    # Serialize the first row of the test data into json\n",
    "    X_test_json = X_test[:1].to_json(orient='records')\n",
    "    print(X_test_json)\n",
    "    # Call the service to get the predictions and the engineered and raw explanations\n",
    "    output = service.run(X_test_json)\n",
    "    # Print the predicted value\n",
    "    print(output['predictions'])\n",
    "    # Print the engineered feature importances for the predicted value\n",
    "    print(output['engineered_local_importance_values'])\n",
    "    # Print the raw feature importances for the predicted value\n",
    "    print(output['raw_local_importance_values'])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the service\n",
    "Delete the service once you have finished inferencing."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the first 3 rows of the dataset\n",
    "\n",
    "test_data = test_data.to_pandas_dataframe()\n",
    "y_test = test_data['ERP'].fillna(0)\n",
    "test_data = test_data.drop('ERP', 1)\n",
    "test_data = test_data.fillna(0)\n",
    "\n",
    "\n",
    "train_data = train_data.to_pandas_dataframe()\n",
    "y_train = train_data['ERP'].fillna(0)\n",
    "train_data = train_data.drop('ERP', 1)\n",
    "train_data = train_data.fillna(0)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = fitted_model.predict(train_data)\n",
    "y_residual_train = y_train - y_pred_train\n",
    "\n",
    "y_pred_test = fitted_model.predict(test_data)\n",
    "y_residual_test = y_test - y_pred_test"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Set up a multi-plot chart.\n",
    "f, (a0, a1) = plt.subplots(1, 2, gridspec_kw = {'width_ratios':[1, 1], 'wspace':0, 'hspace': 0})\n",
    "f.suptitle('Regression Residual Values', fontsize = 18)\n",
    "f.set_figheight(6)\n",
    "f.set_figwidth(16)\n",
    "\n",
    "# Plot residual values of training set.\n",
    "a0.axis([0, 360, -100, 100])\n",
    "a0.plot(y_residual_train, 'bo', alpha = 0.5)\n",
    "a0.plot([-10,360],[0,0], 'r-', lw = 3)\n",
    "a0.text(16,170,'RMSE = {0:.2f}'.format(np.sqrt(mean_squared_error(y_train, y_pred_train))), fontsize = 12)\n",
    "a0.text(16,140,'R2 score = {0:.2f}'.format(r2_score(y_train, y_pred_train)),fontsize = 12)\n",
    "a0.set_xlabel('Training samples', fontsize = 12)\n",
    "a0.set_ylabel('Residual Values', fontsize = 12)\n",
    "\n",
    "# Plot residual values of test set.\n",
    "a1.axis([0, 90, -100, 100])\n",
    "a1.plot(y_residual_test, 'bo', alpha = 0.5)\n",
    "a1.plot([-10,360],[0,0], 'r-', lw = 3)\n",
    "a1.text(5,170,'RMSE = {0:.2f}'.format(np.sqrt(mean_squared_error(y_test, y_pred_test))), fontsize = 12)\n",
    "a1.text(5,140,'R2 score = {0:.2f}'.format(r2_score(y_test, y_pred_test)),fontsize = 12)\n",
    "a1.set_xlabel('Test samples', fontsize = 12)\n",
    "a1.set_yticklabels([])\n",
    "\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "test_pred = plt.scatter(y_test, y_pred_test, color='')\n",
    "test_test = plt.scatter(y_test, y_test, color='g')\n",
    "plt.legend((test_pred, test_test), ('prediction', 'truth'), loc='upper left', fontsize=8)\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression-explanation-featurization/auto-ml-regression-explanation-featurization.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning\n",
    "_**Regression with Aml Compute**_\n",
    "\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Results](#Results)\n",
    "1. [Test](#Test)\n",
    "\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this example we use the Hardware Performance Dataset to showcase how you can use AutoML for a simple regression problem. The Regression goal is to predict the performance of certain combinations of hardware parts.\n",
    "\n",
    "If you are using an Azure Machine Learning Compute Instance, you are all set.  Otherwise, go through the [configuration](../../../configuration.ipynb)  notebook first if you haven't already to establish your connection to the AzureML Workspace. \n",
    "\n",
    "In this notebook you will learn how to:\n",
    "1. Create an `Experiment` in an existing `Workspace`.\n",
    "2. Configure AutoML using `AutoMLConfig`.\n",
    "3. Train the model using local compute.\n",
    "4. Explore the results.\n",
    "5. Test the best fitted model."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "As part of the setup you have already created an Azure ML `Workspace` object. For Automated ML you will need to create an `Experiment` object, which is a named object in a `Workspace` used to run experiments."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.train.automl import AutoMLConfig"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample notebook may use features that are not available in previous versions of the Azure ML SDK."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This notebook was created using version 1.22.0 of the Azure ML SDK\")\n",
    "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# Choose a name for the experiment.\n",
    "experiment_name = 'automl-regression'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "\n",
    "output = {}\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Run History Name'] = experiment_name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "outputDf = pd.DataFrame(data = output, index = [''])\n",
    "outputDf.T"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using AmlCompute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for your AutoML run. In this tutorial, you use `AmlCompute` as your training compute resource."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"reg-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=4)\n",
    "    compute_target = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "Load the hardware dataset from a csv file containing both training features and labels. The features are inputs to the model, while the training labels represent the expected output of the model. Next, we'll split the data using random_split and extract the training data for the model. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/machineData.csv\"\n",
    "dataset = Dataset.Tabular.from_delimited_files(data)\n",
    "\n",
    "# Split the dataset into train and test datasets\n",
    "train_data, test_data = dataset.random_split(percentage=0.8, seed=223)\n",
    "\n",
    "label = \"ERP\"\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Instantiate an `AutoMLConfig` object to specify the settings and data used to run the experiment.\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**task**|classification, regression or forecasting|\n",
    "|**primary_metric**|This is the metric that you want to optimize. Regression supports the following primary metrics: <br><i>spearman_correlation</i><br><i>normalized_root_mean_squared_error</i><br><i>r2_score</i><br><i>normalized_mean_absolute_error</i>|\n",
    "|**n_cross_validations**|Number of cross validation splits.|\n",
    "|**training_data**|(sparse) array-like, shape = [n_samples, n_features]|\n",
    "|**label_column_name**|(sparse) array-like, shape = [n_samples, ], targets values.|\n",
    "\n",
    "**_You can find more information about primary metrics_** [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-auto-train#primary-metric)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "automlconfig-remarks-sample"
    ]
   },
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    \"n_cross_validations\": 3,\n",
    "    \"primary_metric\": 'r2_score',\n",
    "    \"enable_early_stopping\": True, \n",
    "    \"experiment_timeout_hours\": 0.3, #for real scenarios we reccommend a timeout of at least one hour \n",
    "    \"max_concurrent_iterations\": 4,\n",
    "    \"max_cores_per_iteration\": -1,\n",
    "    \"verbosity\": logging.INFO,\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'regression',\n",
    "                             compute_target = compute_target,\n",
    "                             training_data = train_data,\n",
    "                             label_column_name = label,\n",
    "                             **automl_settings\n",
    "                            )"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the `submit` method on the experiment object and pass the run configuration. Execution of remote runs is asynchronous. Depending on the data and the number of iterations this can run for a while.  Validation errors and current status will be shown when setting `show_output=True` and the execution will be synchronous."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run = experiment.submit(automl_config, show_output = False)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to retrieve a run that already started, use the following code\n",
    "#from azureml.train.automl.run import AutoMLRun\n",
    "#remote_run = AutoMLRun(experiment = experiment, run_id = '<replace with your run id>')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Widget for Monitoring Runs\n",
    "\n",
    "The widget will first report a \"loading\" status while running the first iteration. After completing the first iteration, an auto-updating graph and table will be shown. The widget will refresh once per minute, so you should see the graph update as child runs complete.\n",
    "\n",
    "**Note:** The widget displays a link at the bottom. Use this link to open a web interface to explore the individual run details."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(remote_run).show() "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Best Model\n",
    "\n",
    "Below we select the best pipeline from our iterations. The `get_output` method returns the best run and the fitted model. The Model includes the pipeline and any pre-processing.  Overloads on `get_output` allow you to retrieve the best run and fitted model for *any* logged metric or for a particular *iteration*."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = remote_run.get_output()\n",
    "print(best_run)\n",
    "print(fitted_model)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Model Based on Any Other Metric\n",
    "Show the run and the model that has the smallest `root_mean_squared_error` value (which turned out to be the same as the one with largest `spearman_correlation` value):"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_metric = \"root_mean_squared_error\"\n",
    "best_run, fitted_model = remote_run.get_output(metric = lookup_metric)\n",
    "print(best_run)\n",
    "print(fitted_model)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model from a Specific Iteration\n",
    "Show the run and the model from the third iteration:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 3\n",
    "third_run, third_model = remote_run.get_output(iteration = iteration)\n",
    "print(third_run)\n",
    "print(third_model)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the first 3 rows of the dataset\n",
    "\n",
    "test_data = test_data.to_pandas_dataframe()\n",
    "y_test = test_data['ERP'].fillna(0)\n",
    "test_data = test_data.drop('ERP', 1)\n",
    "test_data = test_data.fillna(0)\n",
    "\n",
    "\n",
    "train_data = train_data.to_pandas_dataframe()\n",
    "y_train = train_data['ERP'].fillna(0)\n",
    "train_data = train_data.drop('ERP', 1)\n",
    "train_data = train_data.fillna(0)\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = fitted_model.predict(train_data)\n",
    "y_residual_train = y_train - y_pred_train\n",
    "\n",
    "y_pred_test = fitted_model.predict(test_data)\n",
    "y_residual_test = y_test - y_pred_test"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Set up a multi-plot chart.\n",
    "f, (a0, a1) = plt.subplots(1, 2, gridspec_kw = {'width_ratios':[1, 1], 'wspace':0, 'hspace': 0})\n",
    "f.suptitle('Regression Residual Values', fontsize = 18)\n",
    "f.set_figheight(6)\n",
    "f.set_figwidth(16)\n",
    "\n",
    "# Plot residual values of training set.\n",
    "a0.axis([0, 360, -100, 100])\n",
    "a0.plot(y_residual_train, 'bo', alpha = 0.5)\n",
    "a0.plot([-10,360],[0,0], 'r-', lw = 3)\n",
    "a0.text(16,170,'RMSE = {0:.2f}'.format(np.sqrt(mean_squared_error(y_train, y_pred_train))), fontsize = 12)\n",
    "a0.text(16,140,'R2 score = {0:.2f}'.format(r2_score(y_train, y_pred_train)),fontsize = 12)\n",
    "a0.set_xlabel('Training samples', fontsize = 12)\n",
    "a0.set_ylabel('Residual Values', fontsize = 12)\n",
    "\n",
    "# Plot residual values of test set.\n",
    "a1.axis([0, 90, -100, 100])\n",
    "a1.plot(y_residual_test, 'bo', alpha = 0.5)\n",
    "a1.plot([-10,360],[0,0], 'r-', lw = 3)\n",
    "a1.text(5,170,'RMSE = {0:.2f}'.format(np.sqrt(mean_squared_error(y_test, y_pred_test))), fontsize = 12)\n",
    "a1.text(5,140,'R2 score = {0:.2f}'.format(r2_score(y_test, y_pred_test)),fontsize = 12)\n",
    "a1.set_xlabel('Test samples', fontsize = 12)\n",
    "a1.set_yticklabels([])\n",
    "\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "test_pred = plt.scatter(y_test, y_pred_test, color='')\n",
    "test_test = plt.scatter(y_test, y_test, color='g')\n",
    "plt.legend((test_pred, test_test), ('prediction', 'truth'), loc='upper left', fontsize=8)\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/regression/auto-ml-regression.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/automated-machine-learning/continous-retraining/auto-ml-continuous-retraining.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning \n",
    "**Continuous retraining using Pipelines and Time-Series TabularDataset**\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup](#Setup)\n",
    "3. [Compute](#Compute)\n",
    "4. [Run Configuration](#Run-Configuration)\n",
    "5. [Data Ingestion Pipeline](#Data-Ingestion-Pipeline)\n",
    "6. [Training Pipeline](#Training-Pipeline)\n",
    "7. [Publish Retraining Pipeline and Schedule](#Publish-Retraining-Pipeline-and-Schedule)\n",
    "8. [Test Retraining](#Test-Retraining)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this example we use AutoML and Pipelines to enable contious retraining of a model based on updates to the training dataset. We will create two pipelines, the first one to demonstrate a training dataset that gets updated over time. We leverage time-series capabilities of `TabularDataset` to achieve this. The second pipeline utilizes pipeline `Schedule` to trigger continuous retraining. \n",
    "Make sure you have executed the [configuration notebook](../../../configuration.ipynb) before running this notebook.\n",
    "In this notebook you will learn how to:\n",
    "* Create an Experiment in an existing Workspace.\n",
    "* Configure AutoML using AutoMLConfig.\n",
    "* Create data ingestion pipeline to update a time-series based TabularDataset\n",
    "* Create training pipeline to prepare data, run AutoML, register the model and setup pipeline triggers.\n",
    "\n",
    "## Setup\n",
    "As part of the setup you have already created an Azure ML `Workspace` object. For AutoML you will need to create an `Experiment` object, which is a named object in a `Workspace` used to run experiments."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.train.automl import AutoMLConfig"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample notebook may use features that are not available in previous versions of the Azure ML SDK."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This notebook was created using version 1.22.0 of the Azure ML SDK\")\n",
    "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing the Azure ML workspace requires authentication with Azure.\n",
    "\n",
    "The default authentication is interactive authentication using the default tenant. Executing the ws = Workspace.from_config() line in the cell below will prompt for authentication the first time that it is run.\n",
    "\n",
    "If you have multiple Azure tenants, you can specify the tenant by replacing the ws = Workspace.from_config() line in the cell below with the following:\n",
    "```\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "auth = InteractiveLoginAuthentication(tenant_id = 'mytenantid')\n",
    "ws = Workspace.from_config(auth = auth)\n",
    "```\n",
    "If you need to run in an environment where interactive login is not possible, you can use Service Principal authentication by replacing the ws = Workspace.from_config() line in the cell below with the following:\n",
    "```\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "auth = auth = ServicePrincipalAuthentication('mytenantid', 'myappid', 'mypassword')\n",
    "ws = Workspace.from_config(auth = auth)\n",
    "```\n",
    "For more details, see aka.ms/aml-notebook-auth"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "dstor = ws.get_default_datastore()\n",
    "\n",
    "# Choose a name for the run history container in the workspace.\n",
    "experiment_name = 'retrain-noaaweather'\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "\n",
    "output = {}\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Run History Name'] = experiment_name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "outputDf = pd.DataFrame(data = output, index = [''])\n",
    "outputDf.T"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute \n",
    "\n",
    "#### Create or Attach existing AmlCompute\n",
    "\n",
    "You will need to create a compute target for your AutoML run. In this tutorial, you create AmlCompute as your training compute resource.\n",
    "#### Creation of AmlCompute takes approximately 5 minutes. \n",
    "If the AmlCompute with that name is already in your workspace this code will skip the creation process.\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "amlcompute_cluster_name = \"cont-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=4)\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Configuration"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import CondaDependencies, RunConfiguration\n",
    "\n",
    "# create a new RunConfig object\n",
    "conda_run_config = RunConfiguration(framework=\"python\")\n",
    "\n",
    "# Set compute target to AmlCompute\n",
    "conda_run_config.target = compute_target\n",
    "\n",
    "conda_run_config.environment.docker.enabled = True\n",
    "\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-sdk[automl]', 'applicationinsights', 'azureml-opendatasets', 'azureml-defaults'], \n",
    "                              conda_packages=['numpy==1.16.2'], \n",
    "                              pin_sdk_version=False)\n",
    "conda_run_config.environment.python.conda_dependencies = cd\n",
    "\n",
    "print('run config is ready')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion Pipeline \n",
    "For this demo, we will use NOAA weather data from [Azure Open Datasets](https://azure.microsoft.com/services/open-datasets/). You can replace this with your own dataset, or you can skip this pipeline if you already have a time-series based `TabularDataset`.\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name and target column of the Dataset to create \n",
    "dataset = \"NOAA-Weather-DS4\"\n",
    "target_column_name = \"temperature\""
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Upload Data Step\n",
    "The data ingestion pipeline has a single step with a script to query the latest weather data and upload it to the blob store. During the first run, the script will create and register a time-series based `TabularDataset` with the past one week of weather data. For each subsequent run, the script will create a partition in the blob store by querying NOAA for new weather data since the last modified time of the dataset (`dataset.data_changed_time`) and creating a data.csv file."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline, PipelineParameter\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "ds_name = PipelineParameter(name=\"ds_name\", default_value=dataset)\n",
    "upload_data_step = PythonScriptStep(script_name=\"upload_weather_data.py\", \n",
    "                                         allow_reuse=False,\n",
    "                                         name=\"upload_weather_data\",\n",
    "                                         arguments=[\"--ds_name\", ds_name],\n",
    "                                         compute_target=compute_target, \n",
    "                                         runconfig=conda_run_config)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit Pipeline Run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipeline = Pipeline(\n",
    "    description=\"pipeline_with_uploaddata\",\n",
    "    workspace=ws,    \n",
    "    steps=[upload_data_step])\n",
    "data_pipeline_run = experiment.submit(data_pipeline, pipeline_parameters={\"ds_name\":dataset})"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipeline_run.wait_for_completion(show_output=False)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline\n",
    "### Prepare Training Data Step\n",
    "\n",
    "Script to check if new data is available since the model was last trained. If no new data is available, we cancel the remaining pipeline steps. We need to set allow_reuse flag to False to allow the pipeline to run even when inputs don't change. We also need the name of the model to check the time the model was last trained."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "\n",
    "# The model name with which to register the trained model in the workspace.\n",
    "model_name = PipelineParameter(\"model_name\", default_value=\"noaaweatherds\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_step = PythonScriptStep(script_name=\"check_data.py\", \n",
    "                                         allow_reuse=False,\n",
    "                                         name=\"check_data\",\n",
    "                                         arguments=[\"--ds_name\", ds_name,\n",
    "                                                    \"--model_name\", model_name],\n",
    "                                         compute_target=compute_target, \n",
    "                                         runconfig=conda_run_config)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "train_ds = Dataset.get_by_name(ws, dataset)\n",
    "train_ds = train_ds.drop_columns([\"partition_date\"])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoMLStep\n",
    "Create an AutoMLConfig and a training step."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.pipeline.steps import AutoMLStep\n",
    "\n",
    "automl_settings = {\n",
    "    \"iteration_timeout_minutes\": 10,\n",
    "    \"experiment_timeout_hours\": 0.25,\n",
    "    \"n_cross_validations\": 3,\n",
    "    \"primary_metric\": 'r2_score',\n",
    "    \"max_concurrent_iterations\": 3,\n",
    "    \"max_cores_per_iteration\": -1,\n",
    "    \"verbosity\": logging.INFO,\n",
    "    \"enable_early_stopping\": True\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'regression',\n",
    "                             debug_log = 'automl_errors.log',\n",
    "                             path = \".\",\n",
    "                             compute_target=compute_target,\n",
    "                             training_data = train_ds,\n",
    "                             label_column_name = target_column_name,\n",
    "                             **automl_settings\n",
    "                            )"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData, TrainingOutput\n",
    "\n",
    "metrics_output_name = 'metrics_output'\n",
    "best_model_output_name = 'best_model_output'\n",
    "\n",
    "metrics_data = PipelineData(name='metrics_data',\n",
    "                           datastore=dstor,\n",
    "                           pipeline_output_name=metrics_output_name,\n",
    "                           training_output=TrainingOutput(type='Metrics'))\n",
    "model_data = PipelineData(name='model_data',\n",
    "                           datastore=dstor,\n",
    "                           pipeline_output_name=best_model_output_name,\n",
    "                           training_output=TrainingOutput(type='Model'))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_step = AutoMLStep(\n",
    "    name='automl_module',\n",
    "    automl_config=automl_config,\n",
    "    outputs=[metrics_data, model_data],\n",
    "    allow_reuse=False)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register Model Step\n",
    "Script to register the model to the workspace. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_model_step = PythonScriptStep(script_name=\"register_model.py\",\n",
    "                                       name=\"register_model\",\n",
    "                                       allow_reuse=False,\n",
    "                                       arguments=[\"--model_name\", model_name, \"--model_path\", model_data, \"--ds_name\", ds_name],\n",
    "                                       inputs=[model_data],\n",
    "                                       compute_target=compute_target,\n",
    "                                       runconfig=conda_run_config)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit Pipeline Run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline = Pipeline(\n",
    "    description=\"training_pipeline\",\n",
    "    workspace=ws,    \n",
    "    steps=[data_prep_step, automl_step, register_model_step])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline_run = experiment.submit(training_pipeline, pipeline_parameters={\n",
    "        \"ds_name\": dataset, \"model_name\": \"noaaweatherds\"})"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline_run.wait_for_completion(show_output=False)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish Retraining Pipeline and Schedule\n",
    "Once we are happy with the pipeline, we can publish the training pipeline to the workspace and create a schedule to trigger on blob change. The schedule polls the blob store where the data is being uploaded and runs the retraining pipeline if there is a data change. A new version of the model will be registered to the workspace once the run is complete."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = \"Retraining-Pipeline-NOAAWeather\"\n",
    "\n",
    "published_pipeline = training_pipeline.publish(\n",
    "    name=pipeline_name, \n",
    "    description=\"Pipeline that retrains AutoML model\")\n",
    "\n",
    "published_pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Schedule\n",
    "schedule = Schedule.create(workspace=ws, name=\"RetrainingSchedule\",\n",
    "                           pipeline_parameters={\"ds_name\": dataset, \"model_name\": \"noaaweatherds\"},\n",
    "                           pipeline_id=published_pipeline.id, \n",
    "                           experiment_name=experiment_name, \n",
    "                           datastore=dstor,\n",
    "                           wait_for_provisioning=True,\n",
    "                           polling_interval=1440)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Retraining\n",
    "Here we setup the data ingestion pipeline to run on a schedule, to verify that the retraining pipeline runs as expected. \n",
    "\n",
    "Note: \n",
    "* Azure NOAA Weather data is updated daily and retraining will not trigger if there is no new data available. \n",
    "* Depending on the polling interval set in the schedule, the retraining may take some time trigger after data ingestion pipeline completes."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = \"DataIngestion-Pipeline-NOAAWeather\"\n",
    "\n",
    "published_pipeline = training_pipeline.publish(\n",
    "    name=pipeline_name, \n",
    "    description=\"Pipeline that updates NOAAWeather Dataset\")\n",
    "\n",
    "published_pipeline"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Schedule\n",
    "schedule = Schedule.create(workspace=ws, name=\"RetrainingSchedule-DataIngestion\",\n",
    "                           pipeline_parameters={\"ds_name\":dataset},\n",
    "                           pipeline_id=published_pipeline.id, \n",
    "                           experiment_name=experiment_name, \n",
    "                           datastore=dstor,\n",
    "                           wait_for_provisioning=True,\n",
    "                           polling_interval=1440)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/continuous-retraining/auto-ml-continuous-retraining.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Automated Machine Learning\n",
    "**Beer Production Forecasting**\n",
    "\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Evaluate](#Evaluate)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Introduction\n",
    "This notebook demonstrates demand forecasting for Beer Production Dataset using AutoML.\n",
    "\n",
    "AutoML highlights here include using Deep Learning forecasts, Arima, Prophet,  Remote Execution and Remote Inferencing, and working with the `forecast` function. Please also look at the additional forecasting notebooks, which document lagging, rolling windows, forecast quantiles, other ways to use the forecast function, and forecaster deployment.\n",
    "\n",
    "Make sure you have executed the [configuration](../../../configuration.ipynb) before running this notebook.\n",
    "\n",
    "Notebook synopsis:\n",
    "\n",
    "1. Creating an Experiment in an existing Workspace\n",
    "2. Configuration and remote run of AutoML for a time-series model exploring Regression learners, Arima, Prophet and DNNs\n",
    "4. Evaluating the fitted model using a rolling test "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Setup\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "\n",
    "# Squash warning messages for cleaner output in the notebook\n",
    "warnings.showwarning = lambda *args, **kwargs: None\n",
    "\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from azureml.train.estimator import Estimator"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample notebook may use features that are not available in previous versions of the Azure ML SDK."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This notebook was created using version 1.22.0 of the Azure ML SDK\")\n",
    "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "As part of the setup you have already created a <b>Workspace</b>. To run AutoML, you also need to create an <b>Experiment</b>. An Experiment corresponds to a prediction problem you are trying to solve, while a Run corresponds to a specific approach to the problem."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# choose a name for the run history container in the workspace\n",
    "experiment_name = 'beer-remote-cpu'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "\n",
    "output = {}\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Run History Name'] = experiment_name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "outputDf = pd.DataFrame(data = output, index = [''])\n",
    "outputDf.T"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Using AmlCompute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for your AutoML run. In this tutorial, you use `AmlCompute` as your training compute resource."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"beer-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=4)\n",
    "    compute_target = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Data\n",
    "Read Beer demand data from file, and preview data."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Let's set up what we know about the dataset. \n",
    "\n",
    "**Target column** is what we want to forecast.\n",
    "\n",
    "**Time column** is the time axis along which to predict.\n",
    "\n",
    "**Time series identifier columns** are identified by values of the columns listed `time_series_id_column_names`, for example \"store\" and \"item\" if your data has multiple time series of sales, one series for each combination of store and item sold.\n",
    "\n",
    "**Forecast frequency (freq)** This optional parameter represents the period with which the forecast is desired, for example, daily, weekly, yearly, etc. Use this parameter for the correction of time series containing irregular data points or for padding of short time series. The frequency needs to be a pandas offset alias. Please refer to [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects) for more information.\n",
    "\n",
    "This dataset has only one time series. Please see the [orange juice notebook](https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales) for an example of a multi-time series dataset."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import Grouper\n",
    "from pandas import concat\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "register_matplotlib_converters()\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Beer Production By Year')\n",
    "df = pd.read_csv(\"Beer_no_valid_split_train.csv\", parse_dates=True, index_col= 'DATE').drop(columns='grain')\n",
    "test_df = pd.read_csv(\"Beer_no_valid_split_test.csv\", parse_dates=True, index_col= 'DATE').drop(columns='grain')\n",
    "plt.plot(df)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Beer Production By Month')\n",
    "groups = df.groupby(df.index.month)\n",
    "months = concat([DataFrame(x[1].values) for x in groups], axis=1)\n",
    "months = DataFrame(months)\n",
    "months.columns = range(1,13)\n",
    "months.boxplot()\n",
    "\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "target_column_name = 'BeerProduction'\n",
    "time_column_name = 'DATE'\n",
    "time_series_id_column_names = []\n",
    "freq = 'M' #Monthly data"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Training data into Train and Validation set and Upload to Datastores"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from helper import split_fraction_by_grain\n",
    "from helper import split_full_for_forecasting\n",
    "\n",
    "train, valid = split_full_for_forecasting(df, time_column_name)\n",
    "train.to_csv(\"train.csv\")\n",
    "valid.to_csv(\"valid.csv\")\n",
    "test_df.to_csv(\"test.csv\")\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "datastore.upload_files(files = ['./train.csv'], target_path = 'beer-dataset/tabular/', overwrite = True,show_progress = True)\n",
    "datastore.upload_files(files = ['./valid.csv'], target_path = 'beer-dataset/tabular/', overwrite = True,show_progress = True)\n",
    "datastore.upload_files(files = ['./test.csv'], target_path = 'beer-dataset/tabular/', overwrite = True,show_progress = True)\n",
    "\n",
    "from azureml.core import Dataset\n",
    "train_dataset = Dataset.Tabular.from_delimited_files(path = [(datastore, 'beer-dataset/tabular/train.csv')])\n",
    "valid_dataset = Dataset.Tabular.from_delimited_files(path = [(datastore, 'beer-dataset/tabular/valid.csv')])\n",
    "test_dataset = Dataset.Tabular.from_delimited_files(path = [(datastore, 'beer-dataset/tabular/test.csv')])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Setting forecaster maximum horizon \n",
    "\n",
    "The forecast horizon is the number of periods into the future that the model should predict. Here, we set the horizon to 12 periods (i.e. 12 months). Notice that this is much shorter than the number of months in the test set; we will need to use a rolling test to evaluate the performance on the whole test set. For more discussion of forecast horizons and guiding principles for setting them, please see the [energy demand notebook](https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand).  "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "forecast_horizon = 12"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Train\n",
    "\n",
    "Instantiate a AutoMLConfig object. This defines the settings and data used to run the experiment.\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**task**|forecasting|\n",
    "|**primary_metric**|This is the metric that you want to optimize.<br> Forecasting supports the following primary metrics <br><i>spearman_correlation</i><br><i>normalized_root_mean_squared_error</i><br><i>r2_score</i><br><i>normalized_mean_absolute_error</i>\n",
    "|**iteration_timeout_minutes**|Time limit in minutes for each iteration.|\n",
    "|**training_data**|Input dataset, containing both features and label column.|\n",
    "|**label_column_name**|The name of the label column.|\n",
    "|**enable_dnn**|Enable Forecasting DNNs|\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from azureml.automl.core.forecasting_parameters import ForecastingParameters\n",
    "forecasting_parameters = ForecastingParameters(\n",
    "    time_column_name=time_column_name, forecast_horizon=forecast_horizon\n",
    ")\n",
    "\n",
    "automl_config = AutoMLConfig(task='forecasting',                             \n",
    "                             primary_metric='normalized_root_mean_squared_error',\n",
    "                             experiment_timeout_hours = 1,\n",
    "                             training_data=train_dataset,\n",
    "                             label_column_name=target_column_name,\n",
    "                             validation_data=valid_dataset, \n",
    "                             verbosity=logging.INFO,\n",
    "                             compute_target=compute_target,\n",
    "                             max_concurrent_iterations=4,\n",
    "                             max_cores_per_iteration=-1,\n",
    "                             enable_dnn=True,\n",
    "                             forecasting_parameters=forecasting_parameters)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "We will now run the experiment, starting with 10 iterations of model search. The experiment can be continued for more iterations if more accurate results are required. Validation errors and current status will be shown when setting `show_output=True` and the execution will be synchronous."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "remote_run = experiment.submit(automl_config, show_output= False)\n",
    "remote_run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# If you need to retrieve a run that already started, use the following code\n",
    "# from azureml.train.automl.run import AutoMLRun\n",
    "# remote_run = AutoMLRun(experiment = experiment, run_id = '<replace with your run id>')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Displaying the run objects gives you links to the visual tools in the Azure Portal. Go try them!"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Retrieve the Best Model for Each Algorithm\n",
    "Below we select the best pipeline from our iterations. The get_output method on automl_classifier returns the best run and the fitted model for the last fit invocation. There are overloads on get_output that allow you to retrieve the best run and fitted model for any logged metric or a particular iteration."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from helper import get_result_df\n",
    "summary_df = get_result_df(remote_run)\n",
    "summary_df"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from azureml.core.run import Run\n",
    "from azureml.widgets import RunDetails\n",
    "forecast_model = 'TCNForecaster'\n",
    "if not forecast_model in summary_df['run_id']:\n",
    "    forecast_model = 'ForecastTCN'\n",
    "    \n",
    "best_dnn_run_id = summary_df['run_id'][forecast_model]\n",
    "best_dnn_run = Run(experiment, best_dnn_run_id)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "best_dnn_run.parent\n",
    "RunDetails(best_dnn_run.parent).show() "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "best_dnn_run\n",
    "RunDetails(best_dnn_run).show() "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Evaluate on Test Data"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "We now use the best fitted model from the AutoML Run to make forecasts for the test set.  \n",
    "\n",
    "We always score on the original dataset whose schema matches the training set schema."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "test_dataset = Dataset.Tabular.from_delimited_files(path = [(datastore, 'beer-dataset/tabular/test.csv')])\n",
    "# preview the first 3 rows of the dataset\n",
    "test_dataset.take(5).to_pandas_dataframe()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_target = ws.compute_targets['beer-cluster']\n",
    "test_experiment = Experiment(ws, experiment_name + \"_test\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "script_folder = os.path.join(os.getcwd(), 'inference')\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "shutil.copy('infer.py', script_folder)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import run_inference\n",
    "\n",
    "test_run = run_inference(test_experiment, compute_target, script_folder, best_dnn_run, test_dataset, valid_dataset, forecast_horizon,\n",
    "                 target_column_name, time_column_name, freq)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(test_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import run_multiple_inferences\n",
    "\n",
    "summary_df = run_multiple_inferences(summary_df, experiment, test_experiment, compute_target, script_folder, test_dataset, \n",
    "                  valid_dataset, forecast_horizon, target_column_name, time_column_name, freq)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "for run_name, run_summary in summary_df.iterrows():\n",
    "    print(run_name)\n",
    "    print(run_summary)\n",
    "    run_id = run_summary.run_id\n",
    "    test_run_id = run_summary.test_run_id\n",
    "    test_run = Run(test_experiment, test_run_id)\n",
    "    test_run.wait_for_completion()\n",
    "    test_score = test_run.get_metrics()[run_summary.primary_metric]\n",
    "    summary_df.loc[summary_df.run_id == run_id, 'Test Score'] = test_score\n",
    "    print(\"Test Score: \", test_score)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "summary_df"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-beer-remote/auto-ml-forecasting-beer-remote.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning\n",
    "_**Classification with Deployment using a Bank Marketing Dataset**_\n",
    "\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Train](#Train)\n",
    "1. [Results](#Results)\n",
    "1. [Deploy](#Deploy)\n",
    "1. [Test](#Test)\n",
    "1. [Acknowledgements](#Acknowledgements)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this example we use the UCI Bank Marketing dataset to showcase how you can use AutoML for a  classification problem and deploy it to an Azure Container Instance (ACI). The classification goal is to predict if the client will subscribe to a term deposit with the bank.\n",
    "\n",
    "If you are using an Azure Machine Learning Compute Instance, you are all set.  Otherwise, go through the [configuration](../../../configuration.ipynb)  notebook first if you haven't already to establish your connection to the AzureML Workspace. \n",
    "\n",
    "Please find the ONNX related documentations [here](https://github.com/onnx/onnx).\n",
    "\n",
    "In this notebook you will learn how to:\n",
    "1. Create an experiment using an existing workspace.\n",
    "2. Configure AutoML using `AutoMLConfig`.\n",
    "3. Train the model using local compute with ONNX compatible config on.\n",
    "4. Explore the results, featurization transparency options and save the ONNX model\n",
    "5. Inference with the ONNX model.\n",
    "6. Register the model.\n",
    "7. Create a container image.\n",
    "8. Create an Azure Container Instance (ACI) service.\n",
    "9. Test the ACI service.\n",
    "\n",
    "In addition this notebook showcases the following features\n",
    "- **Blocking** certain pipelines\n",
    "- Specifying **target metrics** to indicate stopping criteria\n",
    "- Handling **missing data** in the input"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "As part of the setup you have already created an Azure ML `Workspace` object. For AutoML you will need to create an `Experiment` object, which is a named object in a `Workspace` used to run experiments."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.automl.core.featurization import FeaturizationConfig\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.interpret import ExplanationClient"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample notebook may use features that are not available in previous versions of the Azure ML SDK."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This notebook was created using version 1.22.0 of the Azure ML SDK\")\n",
    "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing the Azure ML workspace requires authentication with Azure.\n",
    "\n",
    "The default authentication is interactive authentication using the default tenant.  Executing the `ws = Workspace.from_config()` line in the cell below will prompt for authentication the first time that it is run.\n",
    "\n",
    "If you have multiple Azure tenants, you can specify the tenant by replacing the `ws = Workspace.from_config()` line in the cell below with the following:\n",
    "\n",
    "```\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "auth = InteractiveLoginAuthentication(tenant_id = 'mytenantid')\n",
    "ws = Workspace.from_config(auth = auth)\n",
    "```\n",
    "\n",
    "If you need to run in an environment where interactive login is not possible, you can use Service Principal authentication by replacing the `ws = Workspace.from_config()` line in the cell below with the following:\n",
    "\n",
    "```\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "auth = auth = ServicePrincipalAuthentication('mytenantid', 'myappid', 'mypassword')\n",
    "ws = Workspace.from_config(auth = auth)\n",
    "```\n",
    "For more details, see [aka.ms/aml-notebook-auth](http://aka.ms/aml-notebook-auth)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# choose a name for experiment\n",
    "experiment_name = 'automl-classification-bmarketing-all'\n",
    "\n",
    "experiment=Experiment(ws, experiment_name)\n",
    "\n",
    "output = {}\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Experiment Name'] = experiment.name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "outputDf = pd.DataFrame(data = output, index = [''])\n",
    "outputDf.T"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Attach existing AmlCompute\n",
    "You will need to create a compute target for your AutoML run. In this tutorial, you create AmlCompute as your training compute resource.\n",
    "#### Creation of AmlCompute takes approximately 5 minutes. \n",
    "If the AmlCompute with that name is already in your workspace this code will skip the creation process.\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"cpu-cluster-4\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=6)\n",
    "    compute_target = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "Leverage azure compute to load the bank marketing dataset as a Tabular Dataset into the dataset variable. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv\")\n",
    "data.head()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add missing values in 75% of the lines.\n",
    "import numpy as np\n",
    "\n",
    "missing_rate = 0.75\n",
    "n_missing_samples = int(np.floor(data.shape[0] * missing_rate))\n",
    "missing_samples = np.hstack((np.zeros(data.shape[0] - n_missing_samples, dtype=np.bool), np.ones(n_missing_samples, dtype=np.bool)))\n",
    "rng = np.random.RandomState(0)\n",
    "rng.shuffle(missing_samples)\n",
    "missing_features = rng.randint(0, data.shape[1], n_missing_samples)\n",
    "data.values[np.where(missing_samples)[0], missing_features] = np.nan"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('data'):\n",
    "    os.mkdir('data')\n",
    "    \n",
    "# Save the train data to a csv to be uploaded to the datastore\n",
    "pd.DataFrame(data).to_csv(\"data/train_data.csv\", index=False)\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "ds.upload(src_dir='./data', target_path='bankmarketing', overwrite=True, show_progress=True)\n",
    "\n",
    " \n",
    "\n",
    "# Upload the training data as a tabular dataset for access during training on remote compute\n",
    "train_data = Dataset.Tabular.from_delimited_files(path=ds.path('bankmarketing/train_data.csv'))\n",
    "label = \"y\""
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Data"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = \"https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_validate.csv\"\n",
    "validation_dataset = Dataset.Tabular.from_delimited_files(validation_data)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = \"https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_test.csv\"\n",
    "test_dataset = Dataset.Tabular.from_delimited_files(test_data)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Instantiate a AutoMLConfig object. This defines the settings and data used to run the experiment.\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**task**|classification or regression or forecasting|\n",
    "|**primary_metric**|This is the metric that you want to optimize. Classification supports the following primary metrics: <br><i>accuracy</i><br><i>AUC_weighted</i><br><i>average_precision_score_weighted</i><br><i>norm_macro_recall</i><br><i>precision_score_weighted</i>|\n",
    "|**iteration_timeout_minutes**|Time limit in minutes for each iteration.|\n",
    "|**blocked_models** | *List* of *strings* indicating machine learning algorithms for AutoML to avoid in this run. <br><br> Allowed values for **Classification**<br><i>LogisticRegression</i><br><i>SGD</i><br><i>MultinomialNaiveBayes</i><br><i>BernoulliNaiveBayes</i><br><i>SVM</i><br><i>LinearSVM</i><br><i>KNN</i><br><i>DecisionTree</i><br><i>RandomForest</i><br><i>ExtremeRandomTrees</i><br><i>LightGBM</i><br><i>GradientBoosting</i><br><i>TensorFlowDNN</i><br><i>TensorFlowLinearClassifier</i><br><br>Allowed values for **Regression**<br><i>ElasticNet</i><br><i>GradientBoosting</i><br><i>DecisionTree</i><br><i>KNN</i><br><i>LassoLars</i><br><i>SGD</i><br><i>RandomForest</i><br><i>ExtremeRandomTrees</i><br><i>LightGBM</i><br><i>TensorFlowLinearRegressor</i><br><i>TensorFlowDNN</i><br><br>Allowed values for **Forecasting**<br><i>ElasticNet</i><br><i>GradientBoosting</i><br><i>DecisionTree</i><br><i>KNN</i><br><i>LassoLars</i><br><i>SGD</i><br><i>RandomForest</i><br><i>ExtremeRandomTrees</i><br><i>LightGBM</i><br><i>TensorFlowLinearRegressor</i><br><i>TensorFlowDNN</i><br><i>Arima</i><br><i>Prophet</i>|\n",
    "|**allowed_models** |  *List* of *strings* indicating machine learning algorithms for AutoML to use in this run. Same values listed above for **blocked_models** allowed for **allowed_models**.|\n",
    "|**experiment_exit_score**| Value indicating the target for *primary_metric*. <br>Once the target is surpassed the run terminates.|\n",
    "|**experiment_timeout_hours**| Maximum amount of time in hours that all iterations combined can take before the experiment terminates.|\n",
    "|**enable_early_stopping**| Flag to enble early termination if the score is not improving in the short term.|\n",
    "|**featurization**| 'auto' / 'off'  Indicator for whether featurization step should be done automatically or not. Note: If the input data is sparse, featurization cannot be turned on.|\n",
    "|**n_cross_validations**|Number of cross validation splits.|\n",
    "|**training_data**|Input dataset, containing both features and label column.|\n",
    "|**label_column_name**|The name of the label column.|\n",
    "\n",
    "**_You can find more information about primary metrics_** [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-auto-train#primary-metric)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    \"experiment_timeout_hours\" : 0.3,\n",
    "    \"enable_early_stopping\" : True,\n",
    "    \"iteration_timeout_minutes\": 5,\n",
    "    \"max_concurrent_iterations\": 4,\n",
    "    \"max_cores_per_iteration\": -1,\n",
    "    #\"n_cross_validations\": 2,\n",
    "    \"primary_metric\": 'AUC_weighted',\n",
    "    \"featurization\": 'auto',\n",
    "    \"verbosity\": logging.INFO,\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'classification',\n",
    "                             debug_log = 'automl_errors.log',\n",
    "                             compute_target=compute_target,\n",
    "                             experiment_exit_score = 0.9984,\n",
    "                             blocked_models = ['KNN','LinearSVM'],\n",
    "                             enable_onnx_compatible_models=True,\n",
    "                             training_data = train_data,\n",
    "                             label_column_name = label,\n",
    "                             validation_data = validation_dataset,\n",
    "                             **automl_settings\n",
    "                            )"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the `submit` method on the experiment object and pass the run configuration. Execution of local runs is synchronous. Depending on the data and the number of iterations this can run for a while. Validation errors and current status will be shown when setting `show_output=True` and the execution will be synchronous."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run = experiment.submit(automl_config, show_output = False)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to access previous runs. Uncomment the cell below and update the run_id."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from azureml.train.automl.run import AutoMLRun\n",
    "#remote_run = AutoMLRun(experiment=experiment, run_id='<run_ID_goes_here')\n",
    "#remote_run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the remote run to complete\n",
    "remote_run.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run_customized, fitted_model_customized = remote_run.get_output()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transparency\n",
    "\n",
    "View updated featurization summary"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_featurizer = fitted_model_customized.named_steps['datatransformer']\n",
    "df = custom_featurizer.get_featurization_summary()\n",
    "pd.DataFrame(data=df)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `is_user_friendly=False` to get a more detailed summary for the transforms being applied."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = custom_featurizer.get_featurization_summary(is_user_friendly=False)\n",
    "pd.DataFrame(data=df)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = custom_featurizer.get_stats_feature_type_summary()\n",
    "pd.DataFrame(data=df)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(remote_run).show() "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Best Model's explanation\n",
    "Retrieve the explanation from the best_run which includes explanations for engineered features and raw features. Make sure that the run for generating explanations for the best model is completed."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the best model explanation run to complete\n",
    "from azureml.core.run import Run\n",
    "model_explainability_run_id = remote_run.id + \"_\" + \"ModelExplain\"\n",
    "print(model_explainability_run_id)\n",
    "model_explainability_run = Run(experiment=experiment, run_id=model_explainability_run_id)\n",
    "model_explainability_run.wait_for_completion()\n",
    "\n",
    "# Get the best run object\n",
    "best_run, fitted_model = remote_run.get_output()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download engineered feature importance from artifact store\n",
    "You can use ExplanationClient to download the engineered feature explanations from the artifact store of the best_run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ExplanationClient.from_run(best_run)\n",
    "engineered_explanations = client.download_model_explanation(raw=False)\n",
    "exp_data = engineered_explanations.get_feature_importance_dict()\n",
    "exp_data"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download raw feature importance from artifact store\n",
    "You can use ExplanationClient to download the raw feature explanations from the artifact store of the best_run."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ExplanationClient.from_run(best_run)\n",
    "engineered_explanations = client.download_model_explanation(raw=True)\n",
    "exp_data = engineered_explanations.get_feature_importance_dict()\n",
    "exp_data"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Best ONNX Model\n",
    "\n",
    "Below we select the best pipeline from our iterations. The `get_output` method returns the best run and the fitted model. The Model includes the pipeline and any pre-processing.  Overloads on `get_output` allow you to retrieve the best run and fitted model for *any* logged metric or for a particular *iteration*.\n",
    "\n",
    "Set the parameter return_onnx_model=True to retrieve the best ONNX model, instead of the Python model."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, onnx_mdl = remote_run.get_output(return_onnx_model=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the best ONNX model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.automl.runtime.onnx_convert import OnnxConverter\n",
    "onnx_fl_path = \"./best_model.onnx\"\n",
    "OnnxConverter.save_onnx_model(onnx_mdl, onnx_fl_path)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with the ONNX model, using onnxruntime package"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from azureml.automl.core.onnx_convert import OnnxConvertConstants\n",
    "from azureml.train.automl import constants\n",
    "\n",
    "if sys.version_info < OnnxConvertConstants.OnnxIncompatiblePythonVersion:\n",
    "    python_version_compatible = True\n",
    "else:\n",
    "    python_version_compatible = False\n",
    "\n",
    "import onnxruntime\n",
    "from azureml.automl.runtime.onnx_convert import OnnxInferenceHelper\n",
    "\n",
    "def get_onnx_res(run):\n",
    "    res_path = 'onnx_resource.json'\n",
    "    run.download_file(name=constants.MODEL_RESOURCE_PATH_ONNX, output_file_path=res_path)\n",
    "    with open(res_path) as f:\n",
    "        onnx_res = json.load(f)\n",
    "    return onnx_res\n",
    "\n",
    "if python_version_compatible:\n",
    "    test_df = test_dataset.to_pandas_dataframe()\n",
    "    mdl_bytes = onnx_mdl.SerializeToString()\n",
    "    onnx_res = get_onnx_res(best_run)\n",
    "\n",
    "    onnxrt_helper = OnnxInferenceHelper(mdl_bytes, onnx_res)\n",
    "    pred_onnx, pred_prob_onnx = onnxrt_helper.predict(test_df)\n",
    "\n",
    "    print(pred_onnx)\n",
    "    print(pred_prob_onnx)\n",
    "else:\n",
    "    print('Please use Python version 3.6 or 3.7 to run the inference helper.')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy\n",
    "\n",
    "### Retrieve the Best Model\n",
    "\n",
    "Below we select the best pipeline from our iterations.  The `get_output` method returns the best run and the fitted model. Overloads on `get_output` allow you to retrieve the best run and fitted model for *any* logged metric or for a particular *iteration*."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Widget for Monitoring Runs\n",
    "\n",
    "The widget will first report a \"loading\" status while running the first iteration. After completing the first iteration, an auto-updating graph and table will be shown. The widget will refresh once per minute, so you should see the graph update as child runs complete.\n",
    "\n",
    "**Note:** The widget displays a link at the bottom. Use this link to open a web interface to explore the individual run details"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = remote_run.get_output()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = best_run.properties['model_name']\n",
    "\n",
    "script_file_name = 'inference/score.py'\n",
    "\n",
    "best_run.download_file('outputs/scoring_file_v_1_0_0.py', 'inference/score.py')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the Fitted Model for Deployment\n",
    "If neither `metric` nor `iteration` are specified in the `register_model` call, the iteration with the best primary metric is registered."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = 'AutoML Model trained on bank marketing data to predict if a client will subscribe to a term deposit'\n",
    "tags = None\n",
    "model = remote_run.register_model(model_name = model_name, description = description, tags = tags)\n",
    "\n",
    "print(remote_run.model_id) # This will be written to the script file later in the notebook."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model as a Web Service on Azure Container Instance"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "inference_config = InferenceConfig(entry_script=script_file_name)\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n",
    "                                               memory_gb = 1, \n",
    "                                               tags = {'area': \"bmData\", 'type': \"automl_classification\"}, \n",
    "                                               description = 'sample service for Automl Classification')\n",
    "\n",
    "aci_service_name = 'automl-sample-bankmarketing-all'\n",
    "print(aci_service_name)\n",
    "aci_service = Model.deploy(ws, aci_service_name, [model], inference_config, aciconfig)\n",
    "aci_service.wait_for_deployment(True)\n",
    "print(aci_service.state)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Logs from a Deployed Web Service\n",
    "\n",
    "Gets logs from a deployed web service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aci_service.get_logs()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "Now that the model is trained, run the test data through the trained model to get the predicted values.  This calls the ACI web service to do the prediction.\n",
    "\n",
    "Note that the JSON passed to the ACI web service is an array of rows of data.  Each row should either be an array of values in the same order that was used for training or a dictionary where the keys are the same as the column names used for training.  The example below uses dictionary rows."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the bank marketing datasets.\n",
    "from numpy import array"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_dataset.drop_columns(columns=['y'])\n",
    "y_test = test_dataset.keep_columns(columns=['y'], validate=True)\n",
    "test_dataset.take(5).to_pandas_dataframe()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.to_pandas_dataframe()\n",
    "y_test = y_test.to_pandas_dataframe()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "X_test_json = X_test.to_json(orient='records')\n",
    "data = \"{\\\"data\\\": \" + X_test_json +\"}\"\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "resp = requests.post(aci_service.scoring_uri, data, headers=headers)\n",
    "\n",
    "y_pred = json.loads(json.loads(resp.text))['result']"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = array(y_test)\n",
    "actual = actual[:,0]\n",
    "print(len(y_pred), \" \", len(actual))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics for the prediction\n",
    "\n",
    "Now visualize the data as a confusion matrix that compared the predicted values against the actual values.\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "cf =confusion_matrix(actual,y_pred)\n",
    "plt.imshow(cf,cmap=plt.cm.Blues,interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "class_labels = ['no','yes']\n",
    "tick_marks = np.arange(len(class_labels))\n",
    "plt.xticks(tick_marks,class_labels)\n",
    "plt.yticks([-0.5,0,1,1.5],['','no','yes',''])\n",
    "# plotting text value inside cells\n",
    "thresh = cf.max() / 2.\n",
    "for i,j in itertools.product(range(cf.shape[0]),range(cf.shape[1])):\n",
    "    plt.text(j,i,format(cf[i,j],'d'),horizontalalignment='center',color='white' if cf[i,j] >thresh else 'black')\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete a Web Service\n",
    "\n",
    "Deletes the specified web service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Bank Marketing dataset is made available under the Creative Commons (CCO: Public Domain) License: https://creativecommons.org/publicdomain/zero/1.0/. Any rights in individual contents of the database are licensed under the Database Contents License: https://creativecommons.org/publicdomain/zero/1.0/ and is available at: https://www.kaggle.com/janiobachmann/bank-marketing-dataset .\n",
    "\n",
    "_**Acknowledgements**_\n",
    "This data set is originally available within the UCI Machine Learning Database: https://archive.ics.uci.edu/ml/datasets/bank+marketing\n",
    "\n",
    "[Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-bank-marketing-all-features/auto-ml-classification-bank-marketing-all-features.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning\n",
    "**BikeShare Demand Forecasting**\n",
    "\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Compute](#Compute)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Featurization](#Featurization)\n",
    "1. [Evaluate](#Evaluate)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook demonstrates demand forecasting for a bike-sharing service using AutoML.\n",
    "\n",
    "AutoML highlights here include built-in holiday featurization, accessing engineered feature names, and working with the `forecast` function. Please also look at the additional forecasting notebooks, which document lagging, rolling windows, forecast quantiles, other ways to use the forecast function, and forecaster deployment.\n",
    "\n",
    "Make sure you have executed the [configuration notebook](../../../configuration.ipynb) before running this notebook.\n",
    "\n",
    "Notebook synopsis:\n",
    "1. Creating an Experiment in an existing Workspace\n",
    "2. Configuration and local run of AutoML for a time-series model with lag and holiday features \n",
    "3. Viewing the engineered names for featurized data and featurization summary for all raw features\n",
    "4. Evaluating the fitted model using a rolling test "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from azureml.core import Workspace, Experiment, Dataset\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from datetime import datetime"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample notebook may use features that are not available in previous versions of the Azure ML SDK."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This notebook was created using version 1.22.0 of the Azure ML SDK\")\n",
    "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the setup you have already created a <b>Workspace</b>. To run AutoML, you also need to create an <b>Experiment</b>. An Experiment corresponds to a prediction problem you are trying to solve, while a Run corresponds to a specific approach to the problem."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# choose a name for the run history container in the workspace\n",
    "experiment_name = 'automl-bikeshareforecasting'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "\n",
    "output = {}\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace'] = ws.name\n",
    "output['SKU'] = ws.sku\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Run History Name'] = experiment_name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "outputDf = pd.DataFrame(data = output, index = [''])\n",
    "outputDf.T"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute) for your AutoML run. In this tutorial, you create AmlCompute as your training compute resource.\n",
    "#### Creation of AmlCompute takes approximately 5 minutes. \n",
    "If the AmlCompute with that name is already in your workspace this code will skip the creation process.\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your cluster.\n",
    "amlcompute_cluster_name = \"bike-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=4)\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The [Machine Learning service workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-workspace) is paired with the storage account, which contains the default data store. We will use it to upload the bike share data and create [tabular dataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabulardataset?view=azure-ml-py) for training. A tabular dataset defines a series of lazily-evaluated, immutable operations to load data from the data source into tabular representation."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = ws.get_default_datastore()\n",
    "datastore.upload_files(files = ['./bike-no.csv'], target_path = 'dataset/', overwrite = True,show_progress = True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up what we know about the dataset. \n",
    "\n",
    "**Target column** is what we want to forecast.\n",
    "\n",
    "**Time column** is the time axis along which to predict."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column_name = 'cnt'\n",
    "time_column_name = 'date'"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.Tabular.from_delimited_files(path = [(datastore, 'dataset/bike-no.csv')]).with_timestamp_columns(fine_grain_timestamp=time_column_name) \n",
    "\n",
    "# Drop the columns 'casual' and 'registered' as these columns are a breakdown of the total and therefore a leak.\n",
    "dataset = dataset.drop_columns(columns=['casual', 'registered'])\n",
    "\n",
    "dataset.take(5).to_pandas_dataframe().reset_index(drop=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data\n",
    "\n",
    "The first split we make is into train and test sets. Note we are splitting on time. Data before 9/1 will be used for training, and data after and including 9/1 will be used for testing."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select data that occurs before a specified date\n",
    "train = dataset.time_before(datetime(2012, 8, 31), include_boundary=True)\n",
    "train.to_pandas_dataframe().tail(5).reset_index(drop=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dataset.time_after(datetime(2012, 9, 1), include_boundary=True)\n",
    "test.to_pandas_dataframe().head(5).reset_index(drop=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting Parameters\n",
    "To define forecasting parameters for your experiment training, you can leverage the ForecastingParameters class. The table below details the forecasting parameter we will be passing into our experiment.\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**time_column_name**|The name of your time column.|\n",
    "|**forecast_horizon**|The forecast horizon is how many periods forward you would like to forecast. This integer horizon is in units of the timeseries frequency (e.g. daily, weekly).|\n",
    "|**country_or_region_for_holidays**|The country/region used to generate holiday features. These should be ISO 3166 two-letter country/region codes (i.e. 'US', 'GB').|\n",
    "|**target_lags**|The target_lags specifies how far back we will construct the lags of the target variable.|\n",
    "|**freq**|Forecast frequency. This optional parameter represents the period with which the forecast is desired, for example, daily, weekly, yearly, etc. Use this parameter for the correction of time series containing irregular data points or for padding of short time series. The frequency needs to be a pandas offset alias. Please refer to [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects) for more information."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Instantiate a AutoMLConfig object. This defines the settings and data used to run the experiment.\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**task**|forecasting|\n",
    "|**primary_metric**|This is the metric that you want to optimize.<br> Forecasting supports the following primary metrics <br><i>spearman_correlation</i><br><i>normalized_root_mean_squared_error</i><br><i>r2_score</i><br><i>normalized_mean_absolute_error</i>\n",
    "|**blocked_models**|Models in blocked_models won't be used by AutoML. All supported models can be found at [here](https://docs.microsoft.com/en-us/python/api/azureml-train-automl-client/azureml.train.automl.constants.supportedmodels.forecasting?view=azure-ml-py).|\n",
    "|**experiment_timeout_hours**|Experimentation timeout in hours.|\n",
    "|**training_data**|Input dataset, containing both features and label column.|\n",
    "|**label_column_name**|The name of the label column.|\n",
    "|**compute_target**|The remote compute for training.|\n",
    "|**n_cross_validations**|Number of cross validation splits.|\n",
    "|**enable_early_stopping**|If early stopping is on, training will stop when the primary metric is no longer improving.|\n",
    "|**forecasting_parameters**|A class that holds all the forecasting related parameters.|\n",
    "\n",
    "This notebook uses the blocked_models parameter to exclude some models that take a longer time to train on this dataset. You can choose to remove models from the blocked_models list but you may need to increase the experiment_timeout_hours parameter value to get results."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting forecaster maximum horizon \n",
    "\n",
    "The forecast horizon is the number of periods into the future that the model should predict. Here, we set the horizon to 14 periods (i.e. 14 days). Notice that this is much shorter than the number of days in the test set; we will need to use a rolling test to evaluate the performance on the whole test set. For more discussion of forecast horizons and guiding principles for setting them, please see the [energy demand notebook](https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand).  "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_horizon = 14"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config AutoML"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.automl.core.forecasting_parameters import ForecastingParameters\n",
    "forecasting_parameters = ForecastingParameters(\n",
    "    time_column_name=time_column_name,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    country_or_region_for_holidays='US', # set country_or_region will trigger holiday featurizer\n",
    "    target_lags='auto' # use heuristic based lag setting    \n",
    ")\n",
    "\n",
    "automl_config = AutoMLConfig(task='forecasting',                             \n",
    "                             primary_metric='normalized_root_mean_squared_error',\n",
    "                             blocked_models = ['ExtremeRandomTrees'],                             \n",
    "                             experiment_timeout_hours=0.3,\n",
    "                             training_data=train,\n",
    "                             label_column_name=target_column_name,\n",
    "                             compute_target=compute_target,\n",
    "                             enable_early_stopping=True,\n",
    "                             n_cross_validations=3, \n",
    "                             max_concurrent_iterations=4,\n",
    "                             max_cores_per_iteration=-1,\n",
    "                             verbosity=logging.INFO,\n",
    "                             forecasting_parameters=forecasting_parameters)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run the experiment, you can go to Azure ML portal to view the run details. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run = experiment.submit(automl_config, show_output=False)\n",
    "remote_run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Best Model\n",
    "Below we select the best model from all the training iterations using get_output method."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = remote_run.get_output()\n",
    "fitted_model.steps"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurization\n",
    "\n",
    "You can access the engineered feature names generated in time-series featurization. Note that a number of named holiday periods are represented. We recommend that you have at least one year of data when using this feature to ensure that all yearly holidays are captured in the training featurization."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model.named_steps['timeseriestransformer'].get_engineered_feature_names()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the featurization summary\n",
    "\n",
    "You can also see what featurization steps were performed on different raw features in the user data. For each raw feature in the user data, the following information is displayed:\n",
    "\n",
    "- Raw feature name\n",
    "- Number of engineered features formed out of this raw feature\n",
    "- Type detected\n",
    "- If feature was dropped\n",
    "- List of feature transformations for the raw feature"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the featurization summary as a list of JSON\n",
    "featurization_summary = fitted_model.named_steps['timeseriestransformer'].get_featurization_summary()\n",
    "# View the featurization summary as a pandas dataframe\n",
    "pd.DataFrame.from_records(featurization_summary)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the best fitted model from the AutoML Run to make forecasts for the test set. We will do batch scoring on the test dataset which should have the same schema as training dataset.\n",
    "\n",
    "The scoring will run on a remote compute. In this example, it will reuse the training compute."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_experiment = Experiment(ws, experiment_name + \"_test\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving forecasts from the model\n",
    "To run the forecast on the remote compute we will use a helper script: forecasting_script. This script contains the utility methods which will be used by the remote estimator. We copy the script to the project folder to upload it to remote compute."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "script_folder = os.path.join(os.getcwd(), 'forecast')\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "shutil.copy('forecasting_script.py', script_folder)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For brevity, we have created a function called run_forecast that submits the test data to the best model determined during the training run and retrieves forecasts. The test set is longer than the forecast horizon specified at train time, so the forecasting script uses a so-called rolling evaluation to generate predictions over the whole test set. A rolling evaluation iterates the forecaster over the test set, using the actuals in the test set to make lag features as needed. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_forecast import run_rolling_forecast\n",
    "\n",
    "remote_run = run_rolling_forecast(test_experiment, compute_target, best_run, test, target_column_name)\n",
    "remote_run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run.wait_for_completion(show_output=False)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the prediction result for metrics calcuation\n",
    "The test data with predictions are saved in artifact outputs/predictions.csv. You can download it and calculation some error metrics for the forecasts and vizualize the predictions vs. the actuals."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run.download_file('outputs/predictions.csv', 'predictions.csv')\n",
    "df_all = pd.read_csv('predictions.csv')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.automl.core.shared import constants\n",
    "from azureml.automl.runtime.shared.score import scoring\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# use automl metrics module\n",
    "scores = scoring.score_regression(\n",
    "    y_test=df_all[target_column_name],\n",
    "    y_pred=df_all['predicted'],\n",
    "    metrics=list(constants.Metric.SCALAR_REGRESSION_SET))\n",
    "\n",
    "print(\"[Test data scores]\\n\")\n",
    "for key, value in scores.items():    \n",
    "    print('{}:   {:.3f}'.format(key, value))\n",
    "    \n",
    "# Plot outputs\n",
    "%matplotlib inline\n",
    "test_pred = plt.scatter(df_all[target_column_name], df_all['predicted'], color='b')\n",
    "test_test = plt.scatter(df_all[target_column_name], df_all[target_column_name], color='g')\n",
    "plt.legend((test_pred, test_test), ('prediction', 'truth'), loc='upper left', fontsize=8)\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details on what metrics are included and how they are calculated, please refer to [supported metrics](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml#regressionforecasting-metrics). You could also calculate residuals, like described [here](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml#residuals).\n",
    "\n",
    "\n",
    "Since we did a rolling evaluation on the test set, we can analyze the predictions by their forecast horizon relative to the rolling origin. The model was initially trained at a forecast horizon of 14, so each prediction from the model is associated with a horizon value from 1 to 14. The horizon values are in a column named, \"horizon_origin,\" in the prediction set. For example, we can calculate some of the error metrics grouped by the horizon:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics_helper import MAPE, APE\n",
    "df_all.groupby('horizon_origin').apply(\n",
    "    lambda df: pd.Series({'MAPE': MAPE(df[target_column_name], df['predicted']),\n",
    "                          'RMSE': np.sqrt(mean_squared_error(df[target_column_name], df['predicted'])),\n",
    "                          'MAE': mean_absolute_error(df[target_column_name], df['predicted'])}))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To drill down more, we can look at the distributions of APE (absolute percentage error) by horizon. From the chart, it is clear that the overall MAPE is being skewed by one particular point where the actual value is of small absolute value."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_APE = df_all.assign(APE=APE(df_all[target_column_name], df_all['predicted']))\n",
    "APEs = [df_all_APE[df_all['horizon_origin'] == h].APE.values for h in range(1, forecast_horizon + 1)]\n",
    "\n",
    "%matplotlib inline\n",
    "plt.boxplot(APEs)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('horizon')\n",
    "plt.ylabel('APE (%)')\n",
    "plt.title('Absolute Percentage Errors by Forecast Horizon')\n",
    "\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-bike-share/auto-ml-forecasting-bike-share.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning\n",
    "_**Text Classification Using Deep Learning**_\n",
    "\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Evaluate](#Evaluate)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook demonstrates classification with text data using deep learning in AutoML.\n",
    "\n",
    "AutoML highlights here include using deep neural networks (DNNs) to create embedded features from text data. Depending on the compute cluster the user provides, AutoML tried out Bidirectional Encoder Representations from Transformers (BERT) when a GPU compute is used, and Bidirectional Long-Short Term neural network (BiLSTM) when a CPU compute is used, thereby optimizing the choice of DNN for the uesr's setup.\n",
    "\n",
    "Make sure you have executed the [configuration](../../../configuration.ipynb) before running this notebook.\n",
    "\n",
    "Notebook synopsis:\n",
    "\n",
    "1. Creating an Experiment in an existing Workspace\n",
    "2. Configuration and remote run of AutoML for a text dataset (20 Newsgroups dataset from scikit-learn) for classification\n",
    "3. Registering the best model for future use\n",
    "4. Evaluating the final model on a test set"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.run import Run\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core.model import Model \n",
    "from helper import run_inference, get_result_df\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample notebook may use features that are not available in previous versions of the Azure ML SDK."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This notebook was created using version 1.22.0 of the Azure ML SDK\")\n",
    "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the setup you have already created a <b>Workspace</b>. To run AutoML, you also need to create an <b>Experiment</b>. An Experiment corresponds to a prediction problem you are trying to solve, while a Run corresponds to a specific approach to the problem."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# Choose an experiment name.\n",
    "experiment_name = 'automl-classification-text-dnn'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "\n",
    "output = {}\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace Name'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Experiment Name'] = experiment.name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "outputDf = pd.DataFrame(data = output, index = [''])\n",
    "outputDf.T"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a compute cluster\n",
    "This section uses a user-provided compute cluster (named \"dnntext-cluster\" in this example). If a cluster with this name does not exist in the user's workspace, the below code will create a new cluster. You can choose the parameters of the cluster as mentioned in the comments.\n",
    "\n",
    "Whether you provide/select a CPU or GPU cluster, AutoML will choose the appropriate DNN for that setup - BiLSTM or BERT text featurizer will be included in the candidate featurizers on CPU and GPU respectively.  If your goal is to obtain the most accurate model, we recommend you use GPU clusters since BERT featurizers usually outperform BiLSTM featurizers."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "num_nodes = 2\n",
    "\n",
    "# Choose a name for your cluster.\n",
    "amlcompute_cluster_name = \"dnntext-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_NC6\", # CPU for BiLSTM, such as \"STANDARD_D2_V2\" \n",
    "                                                           # To use BERT (this is recommended for best performance), select a GPU such as \"STANDARD_NC6\" \n",
    "                                                           # or similar GPU option\n",
    "                                                           # available in your workspace\n",
    "                                                           max_nodes = num_nodes)\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data\n",
    "For this notebook we will use 20 Newsgroups data from scikit-learn. We filter the data to contain four classes and take a sample as training data. Please note that for accuracy improvement, more data is needed. For this notebook we provide a small-data example so that you can use this template to use with your larger sized data."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"text-dnn-data\" # Local directory to store data\n",
    "blobstore_datadir = data_dir # Blob store directory to store data in\n",
    "target_column_name = 'y'\n",
    "feature_column_name = 'X'\n",
    "\n",
    "def get_20newsgroups_data():\n",
    "    '''Fetches 20 Newsgroups data from scikit-learn\n",
    "       Returns them in form of pandas dataframes\n",
    "    '''\n",
    "    remove = ('headers', 'footers', 'quotes')\n",
    "    categories = [\n",
    "        'rec.sport.baseball',\n",
    "        'rec.sport.hockey',\n",
    "        'comp.graphics',\n",
    "        'sci.space',\n",
    "        ]\n",
    "\n",
    "    data = fetch_20newsgroups(subset = 'train', categories = categories,\n",
    "                                    shuffle = True, random_state = 42,\n",
    "                                    remove = remove)\n",
    "    data = pd.DataFrame({feature_column_name: data.data, target_column_name: data.target})\n",
    "\n",
    "    data_train = data[:200]\n",
    "    data_test = data[200:300]    \n",
    "\n",
    "    data_train = remove_blanks_20news(data_train, feature_column_name, target_column_name)\n",
    "    data_test = remove_blanks_20news(data_test, feature_column_name, target_column_name)\n",
    "    \n",
    "    return data_train, data_test\n",
    "    \n",
    "def remove_blanks_20news(data, feature_column_name, target_column_name):\n",
    "    \n",
    "    data[feature_column_name] = data[feature_column_name].replace(r'\\n', ' ', regex=True).apply(lambda x: x.strip())\n",
    "    data = data[data[feature_column_name] != '']\n",
    "    \n",
    "    return data"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch data and upload to datastore for use in training"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = get_20newsgroups_data()\n",
    "\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "    \n",
    "train_data_fname = data_dir + '/train_data.csv'\n",
    "test_data_fname = data_dir + '/test_data.csv'\n",
    "\n",
    "data_train.to_csv(train_data_fname, index=False)\n",
    "data_test.to_csv(test_data_fname, index=False)\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "datastore.upload(src_dir=data_dir, target_path=blobstore_datadir,\n",
    "                    overwrite=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.Tabular.from_delimited_files(path = [(datastore, blobstore_datadir + '/train_data.csv')])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare AutoML run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the blocked_models parameter to exclude some models that can take a longer time to train on some text datasets. You can choose to remove models from the blocked_models list but you may need to increase the experiment_timeout_hours parameter value to get results."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    \"experiment_timeout_minutes\": 20,\n",
    "    \"primary_metric\": 'accuracy',\n",
    "    \"max_concurrent_iterations\": num_nodes, \n",
    "    \"max_cores_per_iteration\": -1,\n",
    "    \"enable_dnn\": True,\n",
    "    \"enable_early_stopping\": True,\n",
    "    \"validation_size\": 0.3,\n",
    "    \"verbosity\": logging.INFO,\n",
    "    \"enable_voting_ensemble\": False,\n",
    "    \"enable_stack_ensemble\": False,\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'classification',\n",
    "                             debug_log = 'automl_errors.log',\n",
    "                             compute_target=compute_target,\n",
    "                             training_data=train_dataset,\n",
    "                             label_column_name=target_column_name,\n",
    "                             blocked_models = ['LightGBM', 'XGBoostClassifier'],\n",
    "                             **automl_settings\n",
    "                            )"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit AutoML Run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_run = experiment.submit(automl_config, show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the run objects gives you links to the visual tools in the Azure Portal. Go try them!"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Best Model\n",
    "Below we select the best model pipeline from our iterations, use it to test on test data on the same compute cluster."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test the model locally to get a feel of the input/output. When the model contains BERT, this step will require pytorch and pytorch-transformers installed in your local environment. The exact versions of these packages can be found in the **automl_env.yml** file located in the local copy of your MachineLearningNotebooks folder here:\n",
    "MachineLearningNotebooks/how-to-use-azureml/automated-machine-learning/automl_env.yml"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = automl_run.get_output()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now see what text transformations are used to convert text data to features for this dataset, including deep learning transformations based on BiLSTM or Transformer (BERT is one implementation of a Transformer) models."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transformations_used = []\n",
    "for column_group in fitted_model.named_steps['datatransformer'].get_featurization_summary():\n",
    "    text_transformations_used.extend(column_group['Transformations'])\n",
    "text_transformations_used"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering the best model\n",
    "We now register the best fitted model from the AutoML Run for use in future deployments.  "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get results stats, extract the best model from AutoML run, download and register the resultant best model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = get_result_df(automl_run)\n",
    "best_dnn_run_id = summary_df['run_id'].iloc[0]\n",
    "best_dnn_run = Run(experiment, best_dnn_run_id)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'Model' # Local folder where the model will be stored temporarily\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "    \n",
    "best_dnn_run.download_file('outputs/model.pkl', model_dir + '/model.pkl')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the model in your Azure Machine Learning Workspace. If you previously registered a model, please make sure to delete it so as to replace it with this new model."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the model\n",
    "model_name = 'textDNN-20News'\n",
    "model = Model.register(model_path = model_dir + '/model.pkl',\n",
    "                       model_name = model_name,\n",
    "                       tags=None,\n",
    "                       workspace=ws)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Data"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the best fitted model from the AutoML Run to make predictions on the test set.  \n",
    "\n",
    "Test set schema should match that of the training set."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset.Tabular.from_delimited_files(path = [(datastore, blobstore_datadir + '/test_data.csv')])\n",
    "\n",
    "# preview the first 3 rows of the dataset\n",
    "test_dataset.take(3).to_pandas_dataframe()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_experiment = Experiment(ws, experiment_name + \"_test\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_folder = os.path.join(os.getcwd(), 'inference')\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "shutil.copy('infer.py', script_folder)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run = run_inference(test_experiment, compute_target, script_folder, best_dnn_run,\n",
    "                         train_dataset, test_dataset, target_column_name, model_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display computed metrics"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(test_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(test_run.get_metrics())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning\n",
    "_**Forecasting using the Energy Demand Dataset**_\n",
    "\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data and Forecasting Configurations](#Data)\n",
    "1. [Train](#Train)\n",
    "\n",
    "Advanced Forecasting\n",
    "1. [Advanced Training](#advanced_training)\n",
    "1. [Advanced Results](#advanced_results)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this example we use the associated New York City energy demand dataset to showcase how you can use AutoML for a simple forecasting problem and explore the results. The goal is predict the energy demand for the next 48 hours based on historic time-series data.\n",
    "\n",
    "If you are using an Azure Machine Learning Compute Instance, you are all set. Otherwise, go through the [configuration notebook](../../../configuration.ipynb) first, if you haven't already, to establish your connection to the AzureML Workspace.\n",
    "\n",
    "In this notebook you will learn how to:\n",
    "1. Creating an Experiment using an existing Workspace\n",
    "1. Configure AutoML using 'AutoMLConfig'\n",
    "1. Train the model using AmlCompute\n",
    "1. Explore the engineered features and results\n",
    "1. Configuration and remote run of AutoML for a time-series model with lag and rolling window features\n",
    "1. Run and explore the forecast"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Squash warning messages for cleaner output in the notebook\n",
    "warnings.showwarning = lambda *args, **kwargs: None\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Experiment, Workspace, Dataset\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from datetime import datetime"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample notebook may use features that are not available in previous versions of the Azure ML SDK."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This notebook was created using version 1.22.0 of the Azure ML SDK\")\n",
    "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the setup you have already created an Azure ML `Workspace` object. For Automated ML you will need to create an `Experiment` object, which is a named object in a `Workspace` used to run experiments."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# choose a name for the run history container in the workspace\n",
    "experiment_name = 'automl-forecasting-energydemand'\n",
    "\n",
    "# # project folder\n",
    "# project_folder = './sample_projects/automl-forecasting-energy-demand'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "\n",
    "output = {}\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Run History Name'] = experiment_name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "outputDf = pd.DataFrame(data = output, index = [''])\n",
    "outputDf.T"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Attach existing AmlCompute\n",
    "A compute target is required to execute a remote Automated ML run. \n",
    "\n",
    "[Azure Machine Learning Compute](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute) is a managed-compute infrastructure that allows the user to easily create a single or multi-node compute. In this tutorial, you create AmlCompute as your training compute resource.\n",
    "\n",
    "#### Creation of AmlCompute takes approximately 5 minutes. \n",
    "If the AmlCompute with that name is already in your workspace this code will skip the creation process.\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your cluster.\n",
    "amlcompute_cluster_name = \"energy-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS12_V2',\n",
    "                                                           max_nodes=6)\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We will use energy consumption [data from New York City](http://mis.nyiso.com/public/P-58Blist.htm) for model training. The data is stored in a tabular format and includes energy demand and basic weather data at an hourly frequency. \n",
    "\n",
    "With Azure Machine Learning datasets you can keep a single copy of data in your storage, easily access data during model training, share data and collaborate with other users. Below, we will upload the datatset and create a [tabular dataset](https://docs.microsoft.com/bs-latn-ba/azure/machine-learning/service/how-to-create-register-datasets#dataset-types) to be used training and prediction."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up what we know about the dataset.\n",
    "\n",
    "<b>Target column</b> is what we want to forecast.<br></br>\n",
    "<b>Time column</b> is the time axis along which to predict.\n",
    "\n",
    "The other columns, \"temp\" and \"precip\", are implicitly designated as features."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column_name = 'demand'\n",
    "time_column_name = 'timeStamp'"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.Tabular.from_delimited_files(path = \"https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/nyc_energy.csv\").with_timestamp_columns(fine_grain_timestamp=time_column_name) \n",
    "dataset.take(5).to_pandas_dataframe().reset_index(drop=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NYC Energy dataset is missing energy demand values for all datetimes later than August 10th, 2017 5AM. Below, we trim the rows containing these missing values from the end of the dataset."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut off the end of the dataset due to large number of nan values\n",
    "dataset = dataset.time_before(datetime(2017, 10, 10, 5))"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into train and test sets"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first split we make is into train and test sets. Note that we are splitting on time. Data before and including August 8th, 2017 5AM will be used for training, and data after will be used for testing."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train based on time\n",
    "train = dataset.time_before(datetime(2017, 8, 8, 5), include_boundary=True)\n",
    "train.to_pandas_dataframe().reset_index(drop=True).sort_values(time_column_name).tail(5)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into test based on time\n",
    "test = dataset.time_between(datetime(2017, 8, 8, 6), datetime(2017, 8, 10, 5))\n",
    "test.to_pandas_dataframe().reset_index(drop=True).head(5)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the maximum forecast horizon\n",
    "\n",
    "The forecast horizon is the number of periods into the future that the model should predict. It is generally recommend that users set forecast horizons to less than 100 time periods (i.e. less than 100 hours in the NYC energy example). Furthermore, **AutoML's memory use and computation time increase in proportion to the length of the horizon**, so consider carefully how this value is set. If a long horizon forecast really is necessary, consider aggregating the series to a coarser time scale. \n",
    "\n",
    "Learn more about forecast horizons in our [Auto-train a time-series forecast model](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-auto-train-forecast#configure-and-run-experiment) guide.\n",
    "\n",
    "In this example, we set the horizon to 48 hours."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_horizon = 48"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting Parameters\n",
    "To define forecasting parameters for your experiment training, you can leverage the ForecastingParameters class. The table below details the forecasting parameter we will be passing into our experiment.\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**time_column_name**|The name of your time column.|\n",
    "|**forecast_horizon**|The forecast horizon is how many periods forward you would like to forecast. This integer horizon is in units of the timeseries frequency (e.g. daily, weekly).|\n",
    "|**freq**|Forecast frequency. This optional parameter represents the period with which the forecast is desired, for example, daily, weekly, yearly, etc. Use this parameter for the correction of time series containing irregular data points or for padding of short time series. The frequency needs to be a pandas offset alias. Please refer to [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects) for more information."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Instantiate an AutoMLConfig object. This config defines the settings and data used to run the experiment. We can provide extra configurations within 'automl_settings', for this forecasting task we add the forecasting parameters to hold all the additional forecasting parameters.\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**task**|forecasting|\n",
    "|**primary_metric**|This is the metric that you want to optimize.<br> Forecasting supports the following primary metrics <br><i>spearman_correlation</i><br><i>normalized_root_mean_squared_error</i><br><i>r2_score</i><br><i>normalized_mean_absolute_error</i>|\n",
    "|**blocked_models**|Models in blocked_models won't be used by AutoML. All supported models can be found at [here](https://docs.microsoft.com/en-us/python/api/azureml-train-automl-client/azureml.train.automl.constants.supportedmodels.forecasting?view=azure-ml-py).|\n",
    "|**experiment_timeout_hours**|Maximum amount of time in hours that the experiment take before it terminates.|\n",
    "|**training_data**|The training data to be used within the experiment.|\n",
    "|**label_column_name**|The name of the label column.|\n",
    "|**compute_target**|The remote compute for training.|\n",
    "|**n_cross_validations**|Number of cross validation splits. Rolling Origin Validation is used to split time-series in a temporally consistent way.|\n",
    "|**enable_early_stopping**|Flag to enble early termination if the score is not improving in the short term.|\n",
    "|**forecasting_parameters**|A class holds all the forecasting related parameters.|\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the blocked_models parameter to exclude some models that take a longer time to train on this dataset. You can choose to remove models from the blocked_models list but you may need to increase the experiment_timeout_hours parameter value to get results."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.automl.core.forecasting_parameters import ForecastingParameters\n",
    "forecasting_parameters = ForecastingParameters(\n",
    "    time_column_name=time_column_name, forecast_horizon=forecast_horizon\n",
    ")\n",
    "\n",
    "automl_config = AutoMLConfig(task='forecasting',                             \n",
    "                             primary_metric='normalized_root_mean_squared_error',\n",
    "                             blocked_models = ['ExtremeRandomTrees', 'AutoArima', 'Prophet'],                             \n",
    "                             experiment_timeout_hours=0.3,\n",
    "                             training_data=train,\n",
    "                             label_column_name=target_column_name,\n",
    "                             compute_target=compute_target,\n",
    "                             enable_early_stopping=True,\n",
    "                             n_cross_validations=3,                             \n",
    "                             verbosity=logging.INFO,\n",
    "                             forecasting_parameters=forecasting_parameters)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the `submit` method on the experiment object and pass the run configuration. Depending on the data and the number of iterations this can run for a while.\n",
    "One may specify `show_output = True` to print currently running iterations to the console."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run = experiment.submit(automl_config, show_output=False)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the Best Model\n",
    "Below we select the best model from all the training iterations using get_output method."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = remote_run.get_output()\n",
    "fitted_model.steps"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurization\n",
    "You can access the engineered feature names generated in time-series featurization."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model.named_steps['timeseriestransformer'].get_engineered_feature_names()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View featurization summary\n",
    "You can also see what featurization steps were performed on different raw features in the user data. For each raw feature in the user data, the following information is displayed:\n",
    "\n",
    "+ Raw feature name\n",
    "+ Number of engineered features formed out of this raw feature\n",
    "+ Type detected\n",
    "+ If feature was dropped\n",
    "+ List of feature transformations for the raw feature"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the featurization summary as a list of JSON\n",
    "featurization_summary = fitted_model.named_steps['timeseriestransformer'].get_featurization_summary()\n",
    "# View the featurization summary as a pandas dataframe\n",
    "pd.DataFrame.from_records(featurization_summary)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting\n",
    "\n",
    "Now that we have retrieved the best pipeline/model, it can be used to make predictions on test data. First, we remove the target values from the test set:"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.to_pandas_dataframe().reset_index(drop=True)\n",
    "y_test = X_test.pop(target_column_name).values"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast Function\n",
    "For forecasting, we will use the forecast function instead of the predict function. Using the predict method would result in getting predictions for EVERY horizon the forecaster can predict at. This is useful when training and evaluating the performance of the forecaster at various horizons, but the level of detail is excessive for normal use. Forecast function also can handle more complicated scenarios, see the [forecast function notebook](../forecasting-forecast-function/auto-ml-forecasting-function.ipynb)."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The featurized data, aligned to y, will also be returned.\n",
    "# This contains the assumptions that were made in the forecast\n",
    "# and helps align the forecast to the original data\n",
    "y_predictions, X_trans = fitted_model.forecast(X_test)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "To evaluate the accuracy of the forecast, we'll compare against the actual sales quantities for some select metrics, included the mean absolute percentage error (MAPE). For more metrics that can be used for evaluation after training, please see [supported metrics](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml#regressionforecasting-metrics), and [how to calculate residuals](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml#residuals).\n",
    "\n",
    "It is a good practice to always align the output explicitly to the input, as the count and order of the rows may have changed during transformations that span multiple rows."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forecasting_helper import align_outputs\n",
    "\n",
    "df_all = align_outputs(y_predictions, X_trans, X_test, y_test, target_column_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.automl.core.shared import constants\n",
    "from azureml.automl.runtime.shared.score import scoring\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# use automl metrics module\n",
    "scores = scoring.score_regression(\n",
    "    y_test=df_all[target_column_name],\n",
    "    y_pred=df_all['predicted'],\n",
    "    metrics=list(constants.Metric.SCALAR_REGRESSION_SET))\n",
    "\n",
    "print(\"[Test data scores]\\n\")\n",
    "for key, value in scores.items():    \n",
    "    print('{}:   {:.3f}'.format(key, value))\n",
    "    \n",
    "# Plot outputs\n",
    "%matplotlib inline\n",
    "test_pred = plt.scatter(df_all[target_column_name], df_all['predicted'], color='b')\n",
    "test_test = plt.scatter(df_all[target_column_name], df_all[target_column_name], color='g')\n",
    "plt.legend((test_pred, test_test), ('prediction', 'truth'), loc='upper left', fontsize=8)\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at `X_trans` is also useful to see what featurization happened to the data."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trans"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Training <a id=\"advanced_training\"></a>\n",
    "We did not use lags in the previous model specification. In effect, the prediction was the result of a simple regression on date, time series identifier columns and any additional features. This is often a very good prediction as common time series patterns like seasonality and trends can be captured in this manner. Such simple regression is horizon-less: it doesn't matter how far into the future we are predicting, because we are not using past data. In the previous example, the horizon was only used to split the data for cross-validation."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using lags and rolling window features\n",
    "Now we will configure the target lags, that is the previous values of the target variables, meaning the prediction is no longer horizon-less. We therefore must still specify the `forecast_horizon` that the model will learn to forecast. The `target_lags` keyword specifies how far back we will construct the lags of the target variable, and the `target_rolling_window_size` specifies the size of the rolling window over which we will generate the `max`, `min` and `sum` features.\n",
    "\n",
    "This notebook uses the blocked_models parameter to exclude some models that take a longer time to train on this dataset.  You can choose to remove models from the blocked_models list but you may need to increase the iteration_timeout_minutes parameter value to get results."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_forecasting_parameters = ForecastingParameters(\n",
    "    time_column_name=time_column_name, forecast_horizon=forecast_horizon,\n",
    "    target_lags=12, target_rolling_window_size=4\n",
    ")\n",
    "\n",
    "automl_config = AutoMLConfig(task='forecasting',                             \n",
    "                             primary_metric='normalized_root_mean_squared_error',\n",
    "                             blocked_models = ['ElasticNet','ExtremeRandomTrees','GradientBoosting','XGBoostRegressor','ExtremeRandomTrees', 'AutoArima', 'Prophet'], #These models are blocked for tutorial purposes, remove this for real use cases.                            \n",
    "                             experiment_timeout_hours=0.3,\n",
    "                             training_data=train,\n",
    "                             label_column_name=target_column_name,\n",
    "                             compute_target=compute_target,\n",
    "                             enable_early_stopping = True,\n",
    "                             n_cross_validations=3,                             \n",
    "                             verbosity=logging.INFO,\n",
    "                             forecasting_parameters=advanced_forecasting_parameters)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now start a new remote run, this time with lag and rolling window featurization. AutoML applies featurizations in the setup stage, prior to iterating over ML models. The full training set is featurized first, followed by featurization of each of the CV splits. Lag and rolling window features introduce additional complexity, so the run will take longer than in the previous example that lacked these featurizations."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_remote_run = experiment.submit(automl_config, show_output=False)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_remote_run.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Best Model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run_lags, fitted_model_lags = advanced_remote_run.get_output()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Results<a id=\"advanced_results\"></a>\n",
    "We did not use lags in the previous model specification. In effect, the prediction was the result of a simple regression on date, time series identifier columns and any additional features. This is often a very good prediction as common time series patterns like seasonality and trends can be captured in this manner. Such simple regression is horizon-less: it doesn't matter how far into the future we are predicting, because we are not using past data. In the previous example, the horizon was only used to split the data for cross-validation."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The featurized data, aligned to y, will also be returned.\n",
    "# This contains the assumptions that were made in the forecast\n",
    "# and helps align the forecast to the original data\n",
    "y_predictions, X_trans = fitted_model_lags.forecast(X_test)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forecasting_helper import align_outputs\n",
    "\n",
    "df_all = align_outputs(y_predictions, X_trans, X_test, y_test, target_column_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.automl.core.shared import constants\n",
    "from azureml.automl.runtime.shared.score import scoring\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# use automl metrics module\n",
    "scores = scoring.score_regression(\n",
    "    y_test=df_all[target_column_name],\n",
    "    y_pred=df_all['predicted'],\n",
    "    metrics=list(constants.Metric.SCALAR_REGRESSION_SET))\n",
    "\n",
    "print(\"[Test data scores]\\n\")\n",
    "for key, value in scores.items():    \n",
    "    print('{}:   {:.3f}'.format(key, value))\n",
    "    \n",
    "# Plot outputs\n",
    "%matplotlib inline\n",
    "test_pred = plt.scatter(df_all[target_column_name], df_all['predicted'], color='b')\n",
    "test_test = plt.scatter(df_all[target_column_name], df_all[target_column_name], color='g')\n",
    "plt.legend((test_pred, test_test), ('prediction', 'truth'), loc='upper left', fontsize=8)\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning\n",
    "_**Regression with Aml Compute**_\n",
    "\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Results](#Results)\n",
    "1. [Test](#Test)\n",
    "\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this example we use an experimental feature, Model Proxy, to do a predict on the best generated model without downloading the model locally. The prediction will happen on same compute and environment that was used to train the model. This feature is currently in the experimental state, which means that the API is prone to changing, please make sure to run on the latest version of this notebook if you face any issues.\n",
    "\n",
    "If you are using an Azure Machine Learning Compute Instance, you are all set.  Otherwise, go through the [configuration](../../../../configuration.ipynb)  notebook first if you haven't already to establish your connection to the AzureML Workspace. \n",
    "\n",
    "In this notebook you will learn how to:\n",
    "1. Create an `Experiment` in an existing `Workspace`.\n",
    "2. Configure AutoML using `AutoMLConfig`.\n",
    "3. Train the model using remote compute.\n",
    "4. Explore the results.\n",
    "5. Test the best fitted model."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "As part of the setup you have already created an Azure ML `Workspace` object. For Automated ML you will need to create an `Experiment` object, which is a named object in a `Workspace` used to run experiments."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.train.automl import AutoMLConfig"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample notebook may use features that are not available in previous versions of the Azure ML SDK."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This notebook was created using version 1.22.0 of the Azure ML SDK\")\n",
    "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# Choose a name for the experiment.\n",
    "experiment_name = 'automl-regression-model-proxy'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "\n",
    "output = {}\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Run History Name'] = experiment_name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "outputDf = pd.DataFrame(data = output, index = [''])\n",
    "outputDf.T"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using AmlCompute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for your AutoML run. In this tutorial, you use `AmlCompute` as your training compute resource."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "# Try to ensure that the cluster name is unique across the notebooks\n",
    "cpu_cluster_name = \"reg-model-proxy\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=4)\n",
    "    compute_target = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "Load the hardware dataset from a csv file containing both training features and labels. The features are inputs to the model, while the training labels represent the expected output of the model. Next, we'll split the data using random_split and extract the training data for the model. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/machineData.csv\"\n",
    "dataset = Dataset.Tabular.from_delimited_files(data)\n",
    "\n",
    "# Split the dataset into train and test datasets\n",
    "train_data, test_data = dataset.random_split(percentage=0.8, seed=223)\n",
    "\n",
    "label = \"ERP\"\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Instantiate an `AutoMLConfig` object to specify the settings and data used to run the experiment.\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**task**|classification, regression or forecasting|\n",
    "|**primary_metric**|This is the metric that you want to optimize. Regression supports the following primary metrics: <br><i>spearman_correlation</i><br><i>normalized_root_mean_squared_error</i><br><i>r2_score</i><br><i>normalized_mean_absolute_error</i>|\n",
    "|**n_cross_validations**|Number of cross validation splits.|\n",
    "|**training_data**|(sparse) array-like, shape = [n_samples, n_features]|\n",
    "|**label_column_name**|(sparse) array-like, shape = [n_samples, ], targets values.|\n",
    "|**scenario**|We need to set this parameter to 'Latest' to enable some experimental features. This parameter should not be set outside of this experimental notebook.|\n",
    "\n",
    "**_You can find more information about primary metrics_** [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-auto-train#primary-metric)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "automlconfig-remarks-sample"
    ]
   },
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    \"n_cross_validations\": 3,\n",
    "    \"primary_metric\": 'r2_score',\n",
    "    \"enable_early_stopping\": True, \n",
    "    \"experiment_timeout_hours\": 0.3, #for real scenarios we reccommend a timeout of at least one hour \n",
    "    \"max_concurrent_iterations\": 4,\n",
    "    \"max_cores_per_iteration\": -1,\n",
    "    \"verbosity\": logging.INFO,\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'regression',\n",
    "                             compute_target = compute_target,\n",
    "                             training_data = train_data,\n",
    "                             label_column_name = label,\n",
    "                             scenario='Latest',\n",
    "                             **automl_settings\n",
    "                            )"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the `submit` method on the experiment object and pass the run configuration. Execution of remote runs is asynchronous. Depending on the data and the number of iterations this can run for a while.  Validation errors and current status will be shown when setting `show_output=True` and the execution will be synchronous."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run = experiment.submit(automl_config, show_output = False)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to retrieve a run that already started, use the following code\n",
    "#from azureml.train.automl.run import AutoMLRun\n",
    "#remote_run = AutoMLRun(experiment = experiment, run_id = '<replace with your run id>')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Widget for Monitoring Runs\n",
    "\n",
    "The widget will first report a \"loading\" status while running the first iteration. After completing the first iteration, an auto-updating graph and table will be shown. The widget will refresh once per minute, so you should see the graph update as child runs complete.\n",
    "\n",
    "**Note:** The widget displays a link at the bottom. Use this link to open a web interface to explore the individual run details."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(remote_run).show() "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run.wait_for_completion()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Best Child Run\n",
    "\n",
    "Below we select the best pipeline from our iterations. The `get_best_child` method returns the best run. Overloads on `get_best_child` allow you to retrieve the best run for *any* logged metric."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = remote_run.get_best_child()\n",
    "print(best_run)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show hyperparameters\n",
    "Show the model pipeline used for the best run with its hyperparameters."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_properties = json.loads(best_run.get_details()['properties']['pipeline_script'])\n",
    "print(json.dumps(run_properties, indent = 1)) "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Child Run Based on Any Other Metric\n",
    "Show the run and the model that has the smallest `root_mean_squared_error` value (which turned out to be the same as the one with largest `spearman_correlation` value):"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_metric = \"root_mean_squared_error\"\n",
    "best_run = remote_run.get_best_child(metric = lookup_metric)\n",
    "print(best_run)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the first 3 rows of the dataset\n",
    "\n",
    "test_data = test_data.to_pandas_dataframe()\n",
    "y_test = test_data['ERP'].fillna(0)\n",
    "test_data = test_data.drop('ERP', 1)\n",
    "test_data = test_data.fillna(0)\n",
    "\n",
    "\n",
    "train_data = train_data.to_pandas_dataframe()\n",
    "y_train = train_data['ERP'].fillna(0)\n",
    "train_data = train_data.drop('ERP', 1)\n",
    "train_data = train_data.fillna(0)\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating ModelProxy for submitting prediction runs to the training environment.\n",
    "We will create a ModelProxy for the best child run, which will allow us to submit a run that does the prediction in the training environment. Unlike the local client, which can have different versions of some libraries, the training environment will have all the compatible libraries for the model already."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.automl.model_proxy import ModelProxy\n",
    "best_model_proxy = ModelProxy(best_run)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = best_model_proxy.predict(train_data).to_pandas_dataframe().values.flatten()\n",
    "y_residual_train = y_train - y_pred_train\n",
    "\n",
    "y_pred_test = best_model_proxy.predict(test_data).to_pandas_dataframe().values.flatten()\n",
    "y_residual_test = y_test - y_pred_test"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Set up a multi-plot chart.\n",
    "f, (a0, a1) = plt.subplots(1, 2, gridspec_kw = {'width_ratios':[1, 1], 'wspace':0, 'hspace': 0})\n",
    "f.suptitle('Regression Residual Values', fontsize = 18)\n",
    "f.set_figheight(6)\n",
    "f.set_figwidth(16)\n",
    "\n",
    "# Plot residual values of training set.\n",
    "a0.axis([0, 360, -100, 100])\n",
    "a0.plot(y_residual_train, 'bo', alpha = 0.5)\n",
    "a0.plot([-10,360],[0,0], 'r-', lw = 3)\n",
    "a0.text(16,170,'RMSE = {0:.2f}'.format(np.sqrt(mean_squared_error(y_train, y_pred_train))), fontsize = 12)\n",
    "a0.text(16,140,'R2 score = {0:.2f}'.format(r2_score(y_train, y_pred_train)),fontsize = 12)\n",
    "a0.set_xlabel('Training samples', fontsize = 12)\n",
    "a0.set_ylabel('Residual Values', fontsize = 12)\n",
    "\n",
    "# Plot residual values of test set.\n",
    "a1.axis([0, 90, -100, 100])\n",
    "a1.plot(y_residual_test, 'bo', alpha = 0.5)\n",
    "a1.plot([-10,360],[0,0], 'r-', lw = 3)\n",
    "a1.text(5,170,'RMSE = {0:.2f}'.format(np.sqrt(mean_squared_error(y_test, y_pred_test))), fontsize = 12)\n",
    "a1.text(5,140,'R2 score = {0:.2f}'.format(r2_score(y_test, y_pred_test)),fontsize = 12)\n",
    "a1.set_xlabel('Test samples', fontsize = 12)\n",
    "a1.set_yticklabels([])\n",
    "\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "test_pred = plt.scatter(y_test, y_pred_test, color='')\n",
    "test_test = plt.scatter(y_test, y_test, color='g')\n",
    "plt.legend((test_pred, test_test), ('prediction', 'truth'), loc='upper left', fontsize=8)\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/experimental/regression-model-proxy/auto-ml-regression-model-proxy.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning\n",
    "_**Classification of credit card fraudulent transactions with local run **_\n",
    "\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Train](#Train)\n",
    "1. [Results](#Results)\n",
    "1. [Test](#Tests)\n",
    "1. [Explanation](#Explanation)\n",
    "1. [Acknowledgements](#Acknowledgements)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this example we use the associated credit card dataset to showcase how you can use AutoML for a simple classification problem. The goal is to predict if a credit card transaction is considered a fraudulent charge.\n",
    "\n",
    "This notebook is using the local machine compute to train the model.\n",
    "\n",
    "If you are using an Azure Machine Learning Compute Instance, you are all set. Otherwise, go through the [configuration](../../../configuration.ipynb) notebook first if you haven't already to establish your connection to the AzureML Workspace. \n",
    "\n",
    "In this notebook you will learn how to:\n",
    "1. Create an experiment using an existing workspace.\n",
    "2. Configure AutoML using `AutoMLConfig`.\n",
    "3. Train the model.\n",
    "4. Explore the results.\n",
    "5. Test the fitted model.\n",
    "6. Explore any model's explanation and explore feature importance in azure portal.\n",
    "7. Create an AKS cluster, deploy the webservice of AutoML scoring model and the explainer model to the AKS and consume the web service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "As part of the setup you have already created an Azure ML `Workspace` object. For Automated ML you will need to create an `Experiment` object, which is a named object in a `Workspace` used to run experiments."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.interpret import ExplanationClient"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample notebook may use features that are not available in previous versions of the Azure ML SDK."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This notebook was created using version 1.22.0 of the Azure ML SDK\")\n",
    "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# choose a name for experiment\n",
    "experiment_name = 'automl-classification-ccard-local'\n",
    "\n",
    "experiment=Experiment(ws, experiment_name)\n",
    "\n",
    "output = {}\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Experiment Name'] = experiment.name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "outputDf = pd.DataFrame(data = output, index = [''])\n",
    "outputDf.T"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "Load the credit card dataset from a csv file containing both training features and labels. The features are inputs to the model, while the training labels represent the expected output of the model. Next, we'll split the data using random_split and extract the training data for the model."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/creditcard.csv\"\n",
    "dataset = Dataset.Tabular.from_delimited_files(data)\n",
    "training_data, validation_data = dataset.random_split(percentage=0.8, seed=223)\n",
    "label_column_name = 'Class'"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Instantiate a AutoMLConfig object. This defines the settings and data used to run the experiment.\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**task**|classification or regression|\n",
    "|**primary_metric**|This is the metric that you want to optimize. Classification supports the following primary metrics: <br><i>accuracy</i><br><i>AUC_weighted</i><br><i>average_precision_score_weighted</i><br><i>norm_macro_recall</i><br><i>precision_score_weighted</i>|\n",
    "|**enable_early_stopping**|Stop the run if the metric score is not showing improvement.|\n",
    "|**n_cross_validations**|Number of cross validation splits.|\n",
    "|**training_data**|Input dataset, containing both features and label column.|\n",
    "|**label_column_name**|The name of the label column.|\n",
    "\n",
    "**_You can find more information about primary metrics_** [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-auto-train#primary-metric)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    \"n_cross_validations\": 3,\n",
    "    \"primary_metric\": 'average_precision_score_weighted',\n",
    "    \"experiment_timeout_hours\": 0.25, # This is a time limit for testing purposes, remove it for real use cases, this will drastically limit ability to find the best model possible\n",
    "    \"verbosity\": logging.INFO,\n",
    "    \"enable_stack_ensemble\": False\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'classification',\n",
    "                             debug_log = 'automl_errors.log',\n",
    "                             training_data = training_data,\n",
    "                             label_column_name = label_column_name,\n",
    "                             **automl_settings\n",
    "                            )"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the `submit` method on the experiment object and pass the run configuration. Depending on the data and the number of iterations this can run for a while.\n",
    "In this example, we specify `show_output = True` to print currently running iterations to the console."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_run = experiment.submit(automl_config, show_output = True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to retrieve a run that already started, use the following code\n",
    "#from azureml.train.automl.run import AutoMLRun\n",
    "#local_run = AutoMLRun(experiment = experiment, run_id = '<replace with your run id>')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Widget for Monitoring Runs\n",
    "\n",
    "The widget will first report a \"loading\" status while running the first iteration. After completing the first iteration, an auto-updating graph and table will be shown. The widget will refresh once per minute, so you should see the graph update as child runs complete.\n",
    "\n",
    "**Note:** The widget displays a link at the bottom. Use this link to open a web interface to explore the individual run details"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(local_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze results\n",
    "\n",
    "#### Retrieve the Best Model\n",
    "\n",
    "Below we select the best pipeline from our iterations. The `get_output` method on `automl_classifier` returns the best run and the fitted model for the last invocation. Overloads on `get_output` allow you to retrieve the best run and fitted model for *any* logged metric or for a particular *iteration*."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = local_run.get_output()\n",
    "fitted_model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the properties of the model\n",
    "The fitted_model is a python object and you can read the different properties of the object.\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "Now that the model is trained, split the data in the same way the data was split for training (The difference here is the data is being split locally) and then run the test data through the trained model to get the predicted values."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the test data to dataframe\n",
    "X_test_df = validation_data.drop_columns(columns=[label_column_name]).to_pandas_dataframe()\n",
    "y_test_df = validation_data.keep_columns(columns=[label_column_name], validate=True).to_pandas_dataframe()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the predict functions on the model\n",
    "y_pred = fitted_model.predict(X_test_df)\n",
    "y_pred"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics for the prediction\n",
    "\n",
    "Now visualize the data on a scatter plot to show what our truth (actual) values are compared to the predicted values \n",
    "from the trained model that was returned."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "cf =confusion_matrix(y_test_df.values,y_pred)\n",
    "plt.imshow(cf,cmap=plt.cm.Blues,interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "class_labels = ['False','True']\n",
    "tick_marks = np.arange(len(class_labels))\n",
    "plt.xticks(tick_marks,class_labels)\n",
    "plt.yticks([-0.5,0,1,1.5],['','False','True',''])\n",
    "# plotting text value inside cells\n",
    "thresh = cf.max() / 2.\n",
    "for i,j in itertools.product(range(cf.shape[0]),range(cf.shape[1])):\n",
    "    plt.text(j,i,format(cf[i,j],'d'),horizontalalignment='center',color='white' if cf[i,j] >thresh else 'black')\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "In this section, we will show how to compute model explanations and visualize the explanations using azureml-interpret package. We will also show how to run the automl model and the explainer model through deploying an AKS web service.\n",
    "\n",
    "Besides retrieving an existing model explanation for an AutoML model, you can also explain your AutoML model with different test data. The following steps will allow you to compute and visualize engineered feature importance based on your test data.\n",
    "\n",
    "### Run the explanation\n",
    "#### Download the engineered feature importance from artifact store\n",
    "You can use ExplanationClient to download the engineered feature explanations from the artifact store of the best_run. You can also use azure portal url to view the dash board visualization of the feature importance values of the engineered features."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ExplanationClient.from_run(best_run)\n",
    "engineered_explanations = client.download_model_explanation(raw=False)\n",
    "print(engineered_explanations.get_feature_importance_dict())\n",
    "print(\"You can visualize the engineered explanations under the 'Explanations (preview)' tab in the AutoML run at:-\\n\" + best_run.get_portal_url())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the raw feature importance from artifact store\n",
    "You can use ExplanationClient to download the raw feature explanations from the artifact store of the best_run. You can also use azure portal url to view the dash board visualization of the feature importance values of the raw features."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_explanations = client.download_model_explanation(raw=True)\n",
    "print(raw_explanations.get_feature_importance_dict())\n",
    "print(\"You can visualize the raw explanations under the 'Explanations (preview)' tab in the AutoML run at:-\\n\" + best_run.get_portal_url())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve any other AutoML model from training"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_run, fitted_model = local_run.get_output(metric='accuracy')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the model explanations for AutoML models\n",
    "The fitted_model can generate the following which will be used for getting the engineered explanations using automl_setup_model_explanations:-\n",
    "\n",
    "1. Featurized data from train samples/test samples\n",
    "2. Gather engineered name lists\n",
    "3. Find the classes in your labeled column in classification scenarios\n",
    "\n",
    "The automl_explainer_setup_obj contains all the structures from above list."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_data.drop_columns(columns=[label_column_name])\n",
    "y_train = training_data.keep_columns(columns=[label_column_name], validate=True)\n",
    "X_test = validation_data.drop_columns(columns=[label_column_name])"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.automl.runtime.automl_explain_utilities import automl_setup_model_explanations\n",
    "\n",
    "automl_explainer_setup_obj = automl_setup_model_explanations(fitted_model, X=X_train, \n",
    "                                                             X_test=X_test, y=y_train, \n",
    "                                                             task='classification')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the Mimic Explainer for feature importance\n",
    "For explaining the AutoML models, use the MimicWrapper from azureml-interpret package. The MimicWrapper can be initialized with fields in automl_explainer_setup_obj, your workspace and a surrogate model to explain the AutoML model (fitted_model here). The MimicWrapper also takes the automl_run object where engineered explanations will be uploaded."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.ext.glassbox import LGBMExplainableModel\n",
    "from azureml.interpret.mimic_wrapper import MimicWrapper\n",
    "explainer = MimicWrapper(ws, automl_explainer_setup_obj.automl_estimator,\n",
    "                         explainable_model=automl_explainer_setup_obj.surrogate_model, \n",
    "                         init_dataset=automl_explainer_setup_obj.X_transform, run=automl_run,\n",
    "                         features=automl_explainer_setup_obj.engineered_feature_names, \n",
    "                         feature_maps=[automl_explainer_setup_obj.feature_map],\n",
    "                         classes=automl_explainer_setup_obj.classes,\n",
    "                         explainer_kwargs=automl_explainer_setup_obj.surrogate_model_params)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Mimic Explainer for computing and visualizing engineered feature importance\n",
    "The explain() method in MimicWrapper can be called with the transformed test samples to get the feature importance for the generated engineered features. You can also use azure portal url to view the dash board visualization of the feature importance values of the engineered features."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the engineered explanations\n",
    "engineered_explanations = explainer.explain(['local', 'global'], eval_dataset=automl_explainer_setup_obj.X_test_transform)\n",
    "print(engineered_explanations.get_feature_importance_dict())\n",
    "print(\"You can visualize the engineered explanations under the 'Explanations (preview)' tab in the AutoML run at:-\\n\" + automl_run.get_portal_url())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Mimic Explainer for computing and visualizing raw feature importance\n",
    "The explain() method in MimicWrapper can be called with the transformed test samples to get the feature importance for the original features in your data. You can also use azure portal url to view the dash board visualization of the feature importance values of the original/raw features."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the raw explanations\n",
    "raw_explanations = explainer.explain(['local', 'global'], get_raw=True,\n",
    "                                     raw_feature_names=automl_explainer_setup_obj.raw_feature_names,\n",
    "                                     eval_dataset=automl_explainer_setup_obj.X_test_transform,\n",
    "                                     raw_eval_dataset=automl_explainer_setup_obj.X_test_raw)\n",
    "print(raw_explanations.get_feature_importance_dict())\n",
    "print(\"You can visualize the raw explanations under the 'Explanations (preview)' tab in the AutoML run at:-\\n\" + automl_run.get_portal_url())"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the scoring Explainer, save and upload it for later use in scoring explanation"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.interpret.scoring.scoring_explainer import TreeScoringExplainer\n",
    "import joblib\n",
    "\n",
    "# Initialize the ScoringExplainer\n",
    "scoring_explainer = TreeScoringExplainer(explainer.explainer, feature_maps=[automl_explainer_setup_obj.feature_map])\n",
    "\n",
    "# Pickle scoring explainer locally to './scoring_explainer.pkl'\n",
    "scoring_explainer_file_name = 'scoring_explainer.pkl'\n",
    "with open(scoring_explainer_file_name, 'wb') as stream:\n",
    "    joblib.dump(scoring_explainer, stream)\n",
    "\n",
    "# Upload the scoring explainer to the automl run\n",
    "automl_run.upload_file('outputs/scoring_explainer.pkl', scoring_explainer_file_name)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the scoring and explainer models to a web service to Azure Kubernetes Service (AKS)\n",
    "\n",
    "We use the TreeScoringExplainer from azureml.interpret package to create the scoring explainer which will be used to compute the raw and engineered feature importances at the inference time. In the cell below, we register the AutoML model and the scoring explainer with the Model Management Service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register trained automl model present in the 'outputs' folder in the artifacts\n",
    "original_model = automl_run.register_model(model_name='automl_model', \n",
    "                                           model_path='outputs/model.pkl')\n",
    "scoring_explainer_model = automl_run.register_model(model_name='scoring_explainer',\n",
    "                                                    model_path='outputs/scoring_explainer.pkl')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the conda dependencies for setting up the service\n",
    "\n",
    "We need to download the conda dependencies using the automl_run object."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.automl.core.shared import constants\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "automl_run.download_file(constants.CONDA_ENV_FILE_PATH, 'myenv.yml')\n",
    "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\n",
    "myenv"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the Entry Script\n",
    "Write the script that will be used to predict on your model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from azureml.core.model import Model\n",
    "from azureml.train.automl.runtime.automl_explain_utilities import automl_setup_model_explanations\n",
    "\n",
    "\n",
    "def init():\n",
    "    global automl_model\n",
    "    global scoring_explainer\n",
    "\n",
    "    # Retrieve the path to the model file using the model name\n",
    "    # Assume original model is named original_prediction_model\n",
    "    automl_model_path = Model.get_model_path('automl_model')\n",
    "    scoring_explainer_path = Model.get_model_path('scoring_explainer')\n",
    "\n",
    "    automl_model = joblib.load(automl_model_path)\n",
    "    scoring_explainer = joblib.load(scoring_explainer_path)\n",
    "\n",
    "\n",
    "def run(raw_data):\n",
    "    data = pd.read_json(raw_data, orient='records')    \n",
    "    # Make prediction\n",
    "    predictions = automl_model.predict(data)\n",
    "    # Setup for inferencing explanations\n",
    "    automl_explainer_setup_obj = automl_setup_model_explanations(automl_model,\n",
    "                                                                 X_test=data, task='classification')\n",
    "    # Retrieve model explanations for engineered explanations\n",
    "    engineered_local_importance_values = scoring_explainer.explain(automl_explainer_setup_obj.X_test_transform)\n",
    "    # Retrieve model explanations for raw explanations\n",
    "    raw_local_importance_values = scoring_explainer.explain(automl_explainer_setup_obj.X_test_transform, get_raw=True)\n",
    "    # You can return any data type as long as it is JSON-serializable\n",
    "    return {'predictions': predictions.tolist(),\n",
    "            'engineered_local_importance_values': engineered_local_importance_values,\n",
    "            'raw_local_importance_values': raw_local_importance_values}\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the InferenceConfig \n",
    "Create the inference config that will be used when deploying the model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inf_config = InferenceConfig(entry_script='score.py', environment=myenv)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provision the AKS Cluster\n",
    "This is a one time setup. You can reuse this cluster for multiple deployments after it has been created. If you delete the cluster or the resource group that contains it, then you would have to recreate it."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AksCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your cluster.\n",
    "aks_name = 'scoring-explain'\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    aks_target = ComputeTarget(workspace=ws, name=aks_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    prov_config = AksCompute.provisioning_configuration(vm_size='STANDARD_D3_V2')\n",
    "    aks_target = ComputeTarget.create(workspace=ws, \n",
    "                                      name=aks_name,\n",
    "                                      provisioning_configuration=prov_config)\n",
    "aks_target.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy web service to AKS"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the web service configuration (using default here)\n",
    "from azureml.core.webservice import AksWebservice\n",
    "from azureml.core.model import Model\n",
    "\n",
    "aks_config = AksWebservice.deploy_configuration()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aks_service_name ='model-scoring-local-aks'\n",
    "\n",
    "aks_service = Model.deploy(workspace=ws,\n",
    "                           name=aks_service_name,\n",
    "                           models=[scoring_explainer_model, original_model],\n",
    "                           inference_config=inf_config,\n",
    "                           deployment_config=aks_config,\n",
    "                           deployment_target=aks_target)\n",
    "\n",
    "aks_service.wait_for_deployment(show_output = True)\n",
    "print(aks_service.state)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the service logs"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aks_service.get_logs()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consume the web service using run method to do the scoring and explanation of scoring.\n",
    "We test the web sevice by passing data. Run() method retrieves API keys behind the scenes to make sure that call is authenticated."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize the first row of the test data into json\n",
    "X_test_json = X_test_df[:1].to_json(orient='records')\n",
    "print(X_test_json)\n",
    "\n",
    "# Call the service to get the predictions and the engineered and raw explanations\n",
    "output = aks_service.run(X_test_json)\n",
    "\n",
    "# Print the predicted value\n",
    "print('predictions:\\n{}\\n'.format(output['predictions']))\n",
    "# Print the engineered feature importances for the predicted value\n",
    "print('engineered_local_importance_values:\\n{}\\n'.format(output['engineered_local_importance_values']))\n",
    "# Print the raw feature importances for the predicted value\n",
    "print('raw_local_importance_values:\\n{}\\n'.format(output['raw_local_importance_values']))\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up\n",
    "Delete the service."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aks_service.delete()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Credit Card fraud Detection dataset is made available under the Open Database License: http://opendatacommons.org/licenses/odbl/1.0/. Any rights in individual contents of the database are licensed under the Database Contents License: http://opendatacommons.org/licenses/dbcl/1.0/ and is available at: https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
    "\n",
    "\n",
    "The dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Universit\u00c3\u0192\u00c2\u00a9 Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on https://www.researchgate.net/project/Fraud-detection-5 and the page of the DefeatFraud project\n",
    "Please cite the following works: \n",
    "\u00c3\u00a2\u00e2\u201a\u00ac\u00c2\u00a2\tAndrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n",
    "\u00c3\u00a2\u00e2\u201a\u00ac\u00c2\u00a2\tDal Pozzolo, Andrea; Caelen, Olivier; Le Borgne, Yann-Ael; Waterschoot, Serge; Bontempi, Gianluca. Learned lessons in credit card fraud detection from a practitioner perspective, Expert systems with applications,41,10,4915-4928,2014, Pergamon\n",
    "\u00c3\u00a2\u00e2\u201a\u00ac\u00c2\u00a2\tDal Pozzolo, Andrea; Boracchi, Giacomo; Caelen, Olivier; Alippi, Cesare; Bontempi, Gianluca. Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE transactions on neural networks and learning systems,29,8,3784-3797,2018,IEEE\n",
    "o\tDal Pozzolo, Andrea Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n",
    "\u00c3\u00a2\u00e2\u201a\u00ac\u00c2\u00a2\tCarcillo, Fabrizio; Dal Pozzolo, Andrea; Le Borgne, Yann-A\u00c3\u0192\u00c2\u00abl; Caelen, Olivier; Mazzer, Yannis; Bontempi, Gianluca. Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information fusion,41, 182-194,2018,Elsevier\n",
    "\u00c3\u00a2\u00e2\u201a\u00ac\u00c2\u00a2\tCarcillo, Fabrizio; Le Borgne, Yann-A\u00c3\u0192\u00c2\u00abl; Caelen, Olivier; Bontempi, Gianluca. Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics, 5,4,285-300,2018,Springer International Publishing"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/local-run-classification-credit-card-fraud/auto-ml-classification-credit-card-fraud-local.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.png)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning\n",
    "_**Classification of credit card fraudulent transactions on remote compute **_\n",
    "\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Train](#Train)\n",
    "1. [Results](#Results)\n",
    "1. [Test](#Test)\n",
    "1. [Acknowledgements](#Acknowledgements)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this example we use the associated credit card dataset to showcase how you can use AutoML for a simple classification problem. The goal is to predict if a credit card transaction is considered a fraudulent charge.\n",
    "\n",
    "This notebook is using remote compute to train the model.\n",
    "\n",
    "If you are using an Azure Machine Learning Compute Instance, you are all set. Otherwise, go through the [configuration](../../../configuration.ipynb) notebook first if you haven't already to establish your connection to the AzureML Workspace. \n",
    "\n",
    "In this notebook you will learn how to:\n",
    "1. Create an experiment using an existing workspace.\n",
    "2. Configure AutoML using `AutoMLConfig`.\n",
    "3. Train the model using remote compute.\n",
    "4. Explore the results.\n",
    "5. Test the fitted model."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "As part of the setup you have already created an Azure ML `Workspace` object. For Automated ML you will need to create an `Experiment` object, which is a named object in a `Workspace` used to run experiments."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.train.automl import AutoMLConfig"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample notebook may use features that are not available in previous versions of the Azure ML SDK."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This notebook was created using version 1.22.0 of the Azure ML SDK\")\n",
    "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# choose a name for experiment\n",
    "experiment_name = 'automl-classification-ccard-remote'\n",
    "\n",
    "experiment=Experiment(ws, experiment_name)\n",
    "\n",
    "output = {}\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Experiment Name'] = experiment.name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "outputDf = pd.DataFrame(data = output, index = [''])\n",
    "outputDf.T"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Attach existing AmlCompute\n",
    "A compute target is required to execute the Automated ML run. In this tutorial, you create AmlCompute as your training compute resource.\n",
    "#### Creation of AmlCompute takes approximately 5 minutes. \n",
    "If the AmlCompute with that name is already in your workspace this code will skip the creation process.\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"cpu-cluster-1\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS12_V2',\n",
    "                                                           max_nodes=6)\n",
    "    compute_target = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "Load the credit card dataset from a csv file containing both training features and labels. The features are inputs to the model, while the training labels represent the expected output of the model. Next, we'll split the data using random_split and extract the training data for the model."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/creditcard.csv\"\n",
    "dataset = Dataset.Tabular.from_delimited_files(data)\n",
    "training_data, validation_data = dataset.random_split(percentage=0.8, seed=223)\n",
    "label_column_name = 'Class'"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Instantiate a AutoMLConfig object. This defines the settings and data used to run the experiment.\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**task**|classification or regression|\n",
    "|**primary_metric**|This is the metric that you want to optimize. Classification supports the following primary metrics: <br><i>accuracy</i><br><i>AUC_weighted</i><br><i>average_precision_score_weighted</i><br><i>norm_macro_recall</i><br><i>precision_score_weighted</i>|\n",
    "|**enable_early_stopping**|Stop the run if the metric score is not showing improvement.|\n",
    "|**n_cross_validations**|Number of cross validation splits.|\n",
    "|**training_data**|Input dataset, containing both features and label column.|\n",
    "|**label_column_name**|The name of the label column.|\n",
    "\n",
    "**_You can find more information about primary metrics_** [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-auto-train#primary-metric)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    \"n_cross_validations\": 3,\n",
    "    \"primary_metric\": 'average_precision_score_weighted',\n",
    "    \"enable_early_stopping\": True,\n",
    "    \"max_concurrent_iterations\": 2, # This is a limit for testing purpose, please increase it as per cluster size\n",
    "    \"experiment_timeout_hours\": 0.25, # This is a time limit for testing purposes, remove it for real use cases, this will drastically limit ablity to find the best model possible\n",
    "    \"verbosity\": logging.INFO,\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'classification',\n",
    "                             debug_log = 'automl_errors.log',\n",
    "                             compute_target = compute_target,\n",
    "                             training_data = training_data,\n",
    "                             label_column_name = label_column_name,\n",
    "                             **automl_settings\n",
    "                            )"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the `submit` method on the experiment object and pass the run configuration. Depending on the data and the number of iterations this can run for a while. Validation errors and current status will be shown when setting `show_output=True` and the execution will be synchronous."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run = experiment.submit(automl_config, show_output = False)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to retrieve a run that already started, use the following code\n",
    "#from azureml.train.automl.run import AutoMLRun\n",
    "#remote_run = AutoMLRun(experiment = experiment, run_id = '<replace with your run id>')"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Widget for Monitoring Runs\n",
    "\n",
    "The widget will first report a \"loading\" status while running the first iteration. After completing the first iteration, an auto-updating graph and table will be shown. The widget will refresh once per minute, so you should see the graph update as child runs complete.\n",
    "\n",
    "**Note:** The widget displays a link at the bottom. Use this link to open a web interface to explore the individual run details"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "widget-rundetails-sample"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(remote_run).show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run.wait_for_completion(show_output=False)"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain model\n",
    "\n",
    "Automated ML models can be explained and visualized using the SDK Explainability library. "
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze results\n",
    "\n",
    "### Retrieve the Best Model\n",
    "\n",
    "Below we select the best pipeline from our iterations. The `get_output` method returns the best run and the fitted model.  Overloads on `get_output` allow you to retrieve the best run and fitted model for *any* logged metric or for a particular *iteration*."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = remote_run.get_output()\n",
    "fitted_model"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the properties of the model\n",
    "The fitted_model is a python object and you can read the different properties of the object.\n"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the fitted model\n",
    "\n",
    "Now that the model is trained, split the data in the same way the data was split for training (The difference here is the data is being split locally) and then run the test data through the trained model to get the predicted values."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the test data to dataframe\n",
    "X_test_df = validation_data.drop_columns(columns=[label_column_name]).to_pandas_dataframe()\n",
    "y_test_df = validation_data.keep_columns(columns=[label_column_name], validate=True).to_pandas_dataframe()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the predict functions on the model\n",
    "y_pred = fitted_model.predict(X_test_df)\n",
    "y_pred"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics for the prediction\n",
    "\n",
    "Now visualize the data on a scatter plot to show what our truth (actual) values are compared to the predicted values \n",
    "from the trained model that was returned."
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "cf =confusion_matrix(y_test_df.values,y_pred)\n",
    "plt.imshow(cf,cmap=plt.cm.Blues,interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "class_labels = ['False','True']\n",
    "tick_marks = np.arange(len(class_labels))\n",
    "plt.xticks(tick_marks,class_labels)\n",
    "plt.yticks([-0.5,0,1,1.5],['','False','True',''])\n",
    "# plotting text value inside cells\n",
    "thresh = cf.max() / 2.\n",
    "for i,j in itertools.product(range(cf.shape[0]),range(cf.shape[1])):\n",
    "    plt.text(j,i,format(cf[i,j],'d'),horizontalalignment='center',color='white' if cf[i,j] >thresh else 'black')\n",
    "plt.show()"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Credit Card fraud Detection dataset is made available under the Open Database License: http://opendatacommons.org/licenses/odbl/1.0/. Any rights in individual contents of the database are licensed under the Database Contents License: http://opendatacommons.org/licenses/dbcl/1.0/ and is available at: https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
    "\n",
    "The dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Universit\u00c3\u00a9 Libre de Bruxelles) on big data mining and fraud detection.\n",
    "More details on current and past projects on related topics are available on https://www.researchgate.net/project/Fraud-detection-5 and the page of the DefeatFraud project\n",
    "\n",
    "Please cite the following works:\n",
    "\n",
    "Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n",
    "\n",
    "Dal Pozzolo, Andrea; Caelen, Olivier; Le Borgne, Yann-Ael; Waterschoot, Serge; Bontempi, Gianluca. Learned lessons in credit card fraud detection from a practitioner perspective, Expert systems with applications,41,10,4915-4928,2014, Pergamon\n",
    "\n",
    "Dal Pozzolo, Andrea; Boracchi, Giacomo; Caelen, Olivier; Alippi, Cesare; Bontempi, Gianluca. Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE transactions on neural networks and learning systems,29,8,3784-3797,2018,IEEE\n",
    "\n",
    "Dal Pozzolo, Andrea Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n",
    "\n",
    "Carcillo, Fabrizio; Dal Pozzolo, Andrea; Le Borgne, Yann-A\u00c3\u00abl; Caelen, Olivier; Mazzer, Yannis; Bontempi, Gianluca. Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information fusion,41, 182-194,2018,Elsevier\n",
    "\n",
    "Carcillo, Fabrizio; Le Borgne, Yann-A\u00c3\u00abl; Caelen, Olivier; Bontempi, Gianluca. Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics, 5,4,285-300,2018,Springer International Publishing\n",
    "\n",
    "Bertrand Lebichot, Yann-A\u00c3\u00abl Le Borgne, Liyun He, Frederic Obl\u00c3\u00a9, Gianluca Bontempi Deep-Learning Domain Adaptation Techniques for Credit Cards Fraud Detection, INNSBDDL 2019: Recent Advances in Big Data and Deep Learning, pp 78-88, 2019\n",
    "\n",
    "Fabrizio Carcillo, Yann-A\u00c3\u00abl Le Borgne, Olivier Caelen, Frederic Obl\u00c3\u00a9, Gianluca Bontempi Combining Unsupervised and Supervised Learning in Credit Card Fraud Detection Information Sciences, 2019"
   ],
   "file_path": "/home/vsts/work/_temp/samples-python/results/how-to-use-azureml/automated-machine-learning/classification-credit-card-fraud/auto-ml-classification-credit-card-fraud.ipynb"
  }
 ]
}